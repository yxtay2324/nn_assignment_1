{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jMIlfFWkGB0"
   },
   "source": [
    "# Part B: Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ16K1uLTT7_"
   },
   "source": [
    "# Use [markdown](https://www.markdownguide.org/basic-syntax/) to label each (sub)question neatly.\n",
    "\n",
    "This notebook serves as your report. All your answers should be presented within it. \n",
    "\n",
    "You can submit multiple notebooks (e.g. 1 notebook per part / question).\n",
    "\n",
    "Before submission, remember to tidy up the notebook and retain only relevant parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dMfo-qGvShQe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# Setting the seed here is sufficient. \n",
    "# If you don't plan to use these starter code, make sure you add this cell.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mKSfgaY9S6J-"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Normalization, StringLookup, IntegerLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Qpyzb6UAYxX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UACP3RoZS6Hv"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./hdb_price_prediction.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBc022VZAE9P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXssXgxIAMlW",
    "outputId": "2e55ca01-4708-4af4-ffc3-0634c02f5b69"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "\n",
    "    from google.colab import drive\n",
    "    %cd '/content/drive/MyDrive/y4s1/cz4042 nn/assignment1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "g-XYjYh1AMrH",
    "outputId": "b4f032f3-dca3-4878-f9d2-c9aa212d67cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007264</td>\n",
       "      <td>7.006044</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271389</td>\n",
       "      <td>7.983837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.069743</td>\n",
       "      <td>9.090700</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.946890</td>\n",
       "      <td>7.519889</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.092551</td>\n",
       "      <td>9.130489</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133407</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>877 YISHUN STREET 81</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.475885</td>\n",
       "      <td>12.738721</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>145.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>810000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133408</th>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>785000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133409</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.916667</td>\n",
       "      <td>171.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>842000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133410</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>632 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.700595</td>\n",
       "      <td>13.222912</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>845000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133411</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>605 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.603845</td>\n",
       "      <td>13.592586</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>163.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>862000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133412 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "0           1  2017  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1           1  2017   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2           1  2017   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3           1  2017  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4           1  2017   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "...       ...   ...                       ...           ...   \n",
       "133407      6  2022      877 YISHUN STREET 81        Khatib   \n",
       "133408      1  2022      633 YISHUN STREET 61        Khatib   \n",
       "133409      2  2022      633 YISHUN STREET 61        Khatib   \n",
       "133410      2  2022      632 YISHUN STREET 61        Khatib   \n",
       "133411      5  2022      605 YISHUN STREET 61        Khatib   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0                  1.007264       7.006044           0.016807   \n",
       "1                  1.271389       7.983837           0.016807   \n",
       "2                  1.069743       9.090700           0.016807   \n",
       "3                  0.946890       7.519889           0.016807   \n",
       "4                  1.092551       9.130489           0.016807   \n",
       "...                     ...            ...                ...   \n",
       "133407             0.475885      12.738721           0.016807   \n",
       "133408             0.774113      13.229106           0.016807   \n",
       "133409             0.774113      13.229106           0.016807   \n",
       "133410             0.700595      13.222912           0.016807   \n",
       "133411             0.603845      13.592586           0.016807   \n",
       "\n",
       "        eigenvector_centrality                     flat_model_type  \\\n",
       "0                     0.006243                    2 ROOM, Improved   \n",
       "1                     0.006243              3 ROOM, New Generation   \n",
       "2                     0.002459              3 ROOM, New Generation   \n",
       "3                     0.006243              3 ROOM, New Generation   \n",
       "4                     0.002459              3 ROOM, New Generation   \n",
       "...                        ...                                 ...   \n",
       "133407                0.000968               EXECUTIVE, Maisonette   \n",
       "133408                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133409                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133410                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133411                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "\n",
       "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \n",
       "0                   61.333333            44.0     10 TO 12      232000.0  \n",
       "1                   60.583333            67.0     01 TO 03      250000.0  \n",
       "2                   62.416667            67.0     01 TO 03      262000.0  \n",
       "3                   62.083333            68.0     04 TO 06      265000.0  \n",
       "4                   62.416667            67.0     01 TO 03      265000.0  \n",
       "...                       ...             ...          ...           ...  \n",
       "133407              64.583333           145.0     07 TO 09      810000.0  \n",
       "133408              65.000000           164.0     04 TO 06      785000.0  \n",
       "133409              64.916667           171.0     04 TO 06      842000.0  \n",
       "133410              64.750000           164.0     10 TO 12      845000.0  \n",
       "133411              64.750000           163.0     04 TO 06      862000.0  \n",
       "\n",
       "[133412 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eNWYYVX1S6Fo"
   },
   "outputs": [],
   "source": [
    "# The functions in this cell are adapted from https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "# It is the same link as the one mentioned in the question paper (Q1b)\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"resale_price\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\") # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "imK7P_YkWqOb"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r2(y_true, y_pred): \n",
    "    '''\n",
    "    # Obtained from https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/\n",
    "    # TODO: you have to find out how to use it in your code\n",
    "    '''\n",
    "    SS_res = K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMntDP-4knPc"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one such example. To build models on such data, categorical features have to be encoded. Also, before applying neural networks, it is a good practice to try simpler machine learning algorithms first.\n",
    "\n",
    "For all models in Part B of the assignment, the following features should be used: \n",
    "*   Numeric features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "*   Categorical features: month, flat_model_type, storey_range\n",
    "\n",
    "One-hot encoding should be applied on categorical features. \n",
    "Standardisation should be performed on numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2TUDW0-oi91"
   },
   "source": [
    "### Part a\n",
    "\n",
    "Divide the dataset (‘HDB_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data (with the remaining data from year 2021 and 2022 used as test data).\n",
    "\n",
    "Why is this done instead of using random train/test splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IW5w4If2S6Df"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "train_dataframe = df[df['year'] <= 2020] \n",
    "val_dataframe = df[df['year'] > 2020] \n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "train_ds = train_ds.batch(256)\n",
    "val_ds = val_ds.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T9_62eaJojUS"
   },
   "outputs": [],
   "source": [
    "# why is this done instead of using random train/test splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQHliik4ojcN"
   },
   "source": [
    "### Part b\n",
    "\n",
    "A team of data scientists has implemented a linear regression model via Scikit-learn. They obtained a test R2 value of 0.627 and happily shared with you that their model only took a few seconds to train. They suggest you to try out an equivalent deep learning model to see if you get a similar result. Recall that a linear regression model is equivalent to a neural network with only 1 Dense layer (i.e. no hidden layer) with linear activation and 1 output node.\n",
    "\n",
    "However, modelling such a mix of feature types with neural networks requires some changes to the input layer. Implement this neural network by following this tutorial from the Keras documentation which guides you through the process of using the Functional API to do so. After encoding / standardisation, the features should be concatenated. Your architecture should resemble the figure shown in Appendix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77_f--_klnTg",
    "outputId": "ce4c2f2d-0c24-4938-c08e-d9a1ad2505b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['month', 'year', 'full_address', 'nearest_stn', 'dist_to_nearest_stn',\n",
       "       'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality',\n",
       "       'flat_model_type', 'remaining_lease_years', 'floor_area_sqm',\n",
       "       'storey_range', 'resale_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WE29x3dKojvg"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers\n",
    "month = keras.Input(shape=(1,), name=\"month\", dtype=\"int64\")\n",
    "month_encoded = encode_categorical_feature(month, \"month\", train_ds, False)\n",
    "\n",
    "# Categorical feature encoded as string\n",
    "flat_model_type = keras.Input(shape=(1,), name=\"flat_model_type\", dtype=\"string\")\n",
    "flat_model_type_encoded = encode_categorical_feature(flat_model_type, \"flat_model_type\", train_ds, True)\n",
    "\n",
    "storey_range = keras.Input(shape=(1,), name=\"storey_range\", dtype=\"string\")\n",
    "storey_range_encoded = encode_categorical_feature(storey_range, \"storey_range\", train_ds, True)\n",
    "\n",
    "# Numerical features\n",
    "floor_area_sqm = keras.Input(shape=(1,), name=\"floor_area_sqm\")\n",
    "floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, \"floor_area_sqm\", train_ds)\n",
    "\n",
    "remaining_lease_years = keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, \"remaining_lease_years\", train_ds)\n",
    "\n",
    "eigenvector_centrality = keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, \"eigenvector_centrality\", train_ds) \n",
    "\n",
    "degree_centrality = keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "degree_centrality_encoded = encode_numerical_feature(degree_centrality, \"degree_centrality\", train_ds)\n",
    "\n",
    "dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, \"dist_to_dhoby\", train_ds)\n",
    "\n",
    "dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, \"dist_to_nearest_stn\", train_ds)\n",
    "\n",
    "all_inputs = [\n",
    "    dist_to_nearest_stn,\n",
    "    dist_to_dhoby,\n",
    "    degree_centrality,\n",
    "    eigenvector_centrality,\n",
    "    remaining_lease_years,\n",
    "    floor_area_sqm,\n",
    "    month,\n",
    "    flat_model_type,\n",
    "    storey_range\n",
    "]\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        month_encoded,\n",
    "        flat_model_type_encoded,\n",
    "        floor_area_sqm_encoded,\n",
    "        storey_range_encoded,\n",
    "        remaining_lease_years_encoded,\n",
    "        eigenvector_centrality_encoded,\n",
    "        degree_centrality_encoded,\n",
    "        dist_to_dhoby_encoded,\n",
    "        dist_to_nearest_stn_encoded        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xe3dhzl5poTV"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "output = layers.Dense(1, activation=\"linear\")(all_features)\n",
    "model = keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "id": "yW9ZLmciqSbD",
    "outputId": "163c2e88-0503-4d23-f964-8fc32b477612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# plot the model\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlGdk77ioj3g"
   },
   "source": [
    "### Part c\n",
    "\n",
    "The team suggests you to train the model for 50 epochs using mini-batch gradient descent with batch size = 256, Adam optimiser (with a default learning rate of 𝛼 = 0.001) and mean square error as cost function. However, you find that your results are far off from their model. Change the optimiser to SGD (with default learning rate of 𝛼 = 0.01) and observe how the problem gets fixed. Report the test R2 value and explain why the change to SGD fixes the problem faced when using Adam optimiser. (Hint: Look carefully at how Adam is implemented and see how SGD is different.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VabvvSsgwfb2"
   },
   "outputs": [],
   "source": [
    "# adam optimizer\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "no_epochs = 50\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjvy82dKwfX4",
    "outputId": "eea67fb4-495e-4789-b7a9-26b134f1fbd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['year', 'full_address', 'nearest_stn'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 - 3s - loss: 219585888256.0000 - r2: -8.3474e+00 - val_loss: 301486768128.0000 - val_r2: -1.0083e+01 - 3s/epoch - 9ms/step\n",
      "Epoch 2/50\n",
      "342/342 - 2s - loss: 219584561152.0000 - r2: -8.3520e+00 - val_loss: 301485096960.0000 - val_r2: -1.0072e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 3/50\n",
      "342/342 - 2s - loss: 219583217664.0000 - r2: -8.3501e+00 - val_loss: 301483491328.0000 - val_r2: -1.0103e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 4/50\n",
      "342/342 - 2s - loss: 219581857792.0000 - r2: -8.3437e+00 - val_loss: 301481984000.0000 - val_r2: -1.0080e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 5/50\n",
      "342/342 - 2s - loss: 219580628992.0000 - r2: -8.3458e+00 - val_loss: 301480345600.0000 - val_r2: -1.0121e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 6/50\n",
      "342/342 - 2s - loss: 219579383808.0000 - r2: -8.3513e+00 - val_loss: 301478903808.0000 - val_r2: -1.0084e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 7/50\n",
      "342/342 - 2s - loss: 219578007552.0000 - r2: -8.3379e+00 - val_loss: 301477265408.0000 - val_r2: -1.0079e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 8/50\n",
      "342/342 - 2s - loss: 219576696832.0000 - r2: -8.3468e+00 - val_loss: 301475561472.0000 - val_r2: -1.0087e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 9/50\n",
      "342/342 - 2s - loss: 219575353344.0000 - r2: -8.3361e+00 - val_loss: 301474021376.0000 - val_r2: -1.0099e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 10/50\n",
      "342/342 - 2s - loss: 219573944320.0000 - r2: -8.3610e+00 - val_loss: 301472579584.0000 - val_r2: -1.0068e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 11/50\n",
      "342/342 - 2s - loss: 219572699136.0000 - r2: -8.3384e+00 - val_loss: 301470941184.0000 - val_r2: -1.0071e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 12/50\n",
      "342/342 - 2s - loss: 219571273728.0000 - r2: -8.3493e+00 - val_loss: 301469433856.0000 - val_r2: -1.0084e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 13/50\n",
      "342/342 - 2s - loss: 219570012160.0000 - r2: -8.3537e+00 - val_loss: 301467795456.0000 - val_r2: -1.0072e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 14/50\n",
      "342/342 - 2s - loss: 219568635904.0000 - r2: -8.3496e+00 - val_loss: 301466288128.0000 - val_r2: -1.0080e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 15/50\n",
      "342/342 - 3s - loss: 219567357952.0000 - r2: -8.3436e+00 - val_loss: 301464649728.0000 - val_r2: -1.0079e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "342/342 - 2s - loss: 219566014464.0000 - r2: -8.3404e+00 - val_loss: 301463076864.0000 - val_r2: -1.0078e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "342/342 - 2s - loss: 219564507136.0000 - r2: -8.3417e+00 - val_loss: 301461569536.0000 - val_r2: -1.0089e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "342/342 - 2s - loss: 219563360256.0000 - r2: -8.3549e+00 - val_loss: 301459800064.0000 - val_r2: -1.0065e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 19/50\n",
      "342/342 - 2s - loss: 219562033152.0000 - r2: -8.3613e+00 - val_loss: 301458423808.0000 - val_r2: -1.0082e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "342/342 - 2s - loss: 219560607744.0000 - r2: -8.3505e+00 - val_loss: 301456883712.0000 - val_r2: -1.0090e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "342/342 - 2s - loss: 219559280640.0000 - r2: -8.3440e+00 - val_loss: 301455245312.0000 - val_r2: -1.0089e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 22/50\n",
      "342/342 - 2s - loss: 219557904384.0000 - r2: -8.3422e+00 - val_loss: 301453639680.0000 - val_r2: -1.0080e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 23/50\n",
      "342/342 - 2s - loss: 219556626432.0000 - r2: -8.3484e+00 - val_loss: 301452132352.0000 - val_r2: -1.0074e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "342/342 - 2s - loss: 219555299328.0000 - r2: -8.3529e+00 - val_loss: 301450493952.0000 - val_r2: -1.0071e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "342/342 - 2s - loss: 219554037760.0000 - r2: -8.3670e+00 - val_loss: 301448986624.0000 - val_r2: -1.0071e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "342/342 - 2s - loss: 219552563200.0000 - r2: -8.3584e+00 - val_loss: 301447348224.0000 - val_r2: -1.0060e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 27/50\n",
      "342/342 - 2s - loss: 219551367168.0000 - r2: -8.3555e+00 - val_loss: 301445840896.0000 - val_r2: -1.0094e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 28/50\n",
      "342/342 - 2s - loss: 219549908992.0000 - r2: -8.3427e+00 - val_loss: 301444268032.0000 - val_r2: -1.0070e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "342/342 - 3s - loss: 219548729344.0000 - r2: -8.3483e+00 - val_loss: 301442662400.0000 - val_r2: -1.0078e+01 - 3s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "342/342 - 3s - loss: 219547369472.0000 - r2: -8.3525e+00 - val_loss: 301441089536.0000 - val_r2: -1.0089e+01 - 3s/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "342/342 - 2s - loss: 219545976832.0000 - r2: -8.3495e+00 - val_loss: 301439582208.0000 - val_r2: -1.0075e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "342/342 - 2s - loss: 219544698880.0000 - r2: -8.3439e+00 - val_loss: 301438009344.0000 - val_r2: -1.0070e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "342/342 - 2s - loss: 219543240704.0000 - r2: -8.3394e+00 - val_loss: 301436403712.0000 - val_r2: -1.0084e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "342/342 - 3s - loss: 219541995520.0000 - r2: -8.3379e+00 - val_loss: 301434863616.0000 - val_r2: -1.0082e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "342/342 - 3s - loss: 219540635648.0000 - r2: -8.3572e+00 - val_loss: 301433290752.0000 - val_r2: -1.0075e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "342/342 - 3s - loss: 219539308544.0000 - r2: -8.3376e+00 - val_loss: 301431652352.0000 - val_r2: -1.0094e+01 - 3s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "342/342 - 2s - loss: 219538079744.0000 - r2: -8.3422e+00 - val_loss: 301430079488.0000 - val_r2: -1.0100e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "342/342 - 2s - loss: 219536605184.0000 - r2: -8.3458e+00 - val_loss: 301428539392.0000 - val_r2: -1.0060e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "342/342 - 2s - loss: 219535310848.0000 - r2: -8.3552e+00 - val_loss: 301426999296.0000 - val_r2: -1.0088e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "342/342 - 2s - loss: 219533950976.0000 - r2: -8.3535e+00 - val_loss: 301425426432.0000 - val_r2: -1.0081e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "342/342 - 2s - loss: 219532525568.0000 - r2: -8.3537e+00 - val_loss: 301423788032.0000 - val_r2: -1.0083e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "342/342 - 2s - loss: 219531247616.0000 - r2: -8.3425e+00 - val_loss: 301422247936.0000 - val_r2: -1.0084e+01 - 2s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "342/342 - 2s - loss: 219529904128.0000 - r2: -8.3514e+00 - val_loss: 301420740608.0000 - val_r2: -1.0077e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "342/342 - 3s - loss: 219528691712.0000 - r2: -8.3413e+00 - val_loss: 301419069440.0000 - val_r2: -1.0068e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "342/342 - 3s - loss: 219527315456.0000 - r2: -8.3392e+00 - val_loss: 301417463808.0000 - val_r2: -1.0096e+01 - 3s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "342/342 - 3s - loss: 219526037504.0000 - r2: -8.3425e+00 - val_loss: 301416022016.0000 - val_r2: -1.0075e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "342/342 - 2s - loss: 219524743168.0000 - r2: -8.3485e+00 - val_loss: 301414416384.0000 - val_r2: -1.0061e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "342/342 - 2s - loss: 219523317760.0000 - r2: -8.3431e+00 - val_loss: 301412843520.0000 - val_r2: -1.0086e+01 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "342/342 - 3s - loss: 219522007040.0000 - r2: -8.3478e+00 - val_loss: 301411434496.0000 - val_r2: -1.0083e+01 - 3s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "342/342 - 3s - loss: 219520679936.0000 - r2: -8.3501e+00 - val_loss: 301409665024.0000 - val_r2: -1.0074e+01 - 3s/epoch - 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "output = layers.Dense(1, activation=\"linear\")(all_features)\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=adam_opt,\n",
    "              loss='mse', \n",
    "              metrics=[r2])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=no_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    verbose = 2,\n",
    "                    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uz7hysdb0-bT",
    "outputId": "3e2775c1-170a-40b1-947b-dbabb2b99c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s - loss: 301409697792.0000 - r2: -1.0089e+01 - 971ms/epoch - 5ms/step\n",
      "------- Evaluation of Adam optimiser -------\n",
      "Mean Squared Error: 301409697792.0\n",
      "R Squared Error: -10.088726043701172\n"
     ]
    }
   ],
   "source": [
    "evaluate_adam = model.evaluate(val_ds, verbose =2)\n",
    "\n",
    "print(\"------- Evaluation of Adam optimiser -------\")\n",
    "print(\"Mean Squared Error: \" + str(evaluate_adam[0]))\n",
    "print(\"R Squared Error: \" + str(evaluate_adam[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A2EU4QgTwff1"
   },
   "outputs": [],
   "source": [
    "# sgd optimizer\n",
    "sgd_opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "no_epochs = 50\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQFh-9jLwfjc",
    "outputId": "c5e2e55c-daf5-4692-d2ce-25ffc03e933f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 - 4s - loss: 105362341888.0000 - r2: -3.4847e+00 - val_loss: 79115247616.0000 - val_r2: -1.9028e+00 - 4s/epoch - 11ms/step\n",
      "Epoch 2/50\n",
      "342/342 - 3s - loss: 23966013440.0000 - r2: -1.3541e-02 - val_loss: 33178230784.0000 - val_r2: -2.1243e-01 - 3s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "342/342 - 3s - loss: 10475787264.0000 - r2: 0.5588 - val_loss: 21229615104.0000 - val_r2: 0.2245 - 3s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "342/342 - 3s - loss: 7811955712.0000 - r2: 0.6699 - val_loss: 17264797696.0000 - val_r2: 0.3698 - 3s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "342/342 - 3s - loss: 7028672512.0000 - r2: 0.7031 - val_loss: 15663082496.0000 - val_r2: 0.4281 - 3s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "342/342 - 3s - loss: 6648599552.0000 - r2: 0.7195 - val_loss: 14880990208.0000 - val_r2: 0.4574 - 3s/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "342/342 - 3s - loss: 6396240384.0000 - r2: 0.7292 - val_loss: 14443913216.0000 - val_r2: 0.4733 - 3s/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "342/342 - 3s - loss: 6205049344.0000 - r2: 0.7370 - val_loss: 14151780352.0000 - val_r2: 0.4830 - 3s/epoch - 9ms/step\n",
      "Epoch 9/50\n",
      "342/342 - 3s - loss: 6050588672.0000 - r2: 0.7442 - val_loss: 13935631360.0000 - val_r2: 0.4915 - 3s/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "342/342 - 3s - loss: 5921614848.0000 - r2: 0.7495 - val_loss: 13758050304.0000 - val_r2: 0.4978 - 3s/epoch - 10ms/step\n",
      "Epoch 11/50\n",
      "342/342 - 4s - loss: 5811039744.0000 - r2: 0.7537 - val_loss: 13619589120.0000 - val_r2: 0.5028 - 4s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "342/342 - 3s - loss: 5714327552.0000 - r2: 0.7582 - val_loss: 13488880640.0000 - val_r2: 0.5076 - 3s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "342/342 - 3s - loss: 5628794368.0000 - r2: 0.7615 - val_loss: 13381165056.0000 - val_r2: 0.5114 - 3s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "342/342 - 3s - loss: 5552240128.0000 - r2: 0.7648 - val_loss: 13285502976.0000 - val_r2: 0.5152 - 3s/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "342/342 - 3s - loss: 5483127296.0000 - r2: 0.7679 - val_loss: 13195166720.0000 - val_r2: 0.5183 - 3s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "342/342 - 3s - loss: 5420382720.0000 - r2: 0.7706 - val_loss: 13102456832.0000 - val_r2: 0.5211 - 3s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "342/342 - 3s - loss: 5362969600.0000 - r2: 0.7730 - val_loss: 13026676736.0000 - val_r2: 0.5239 - 3s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "342/342 - 3s - loss: 5309953536.0000 - r2: 0.7752 - val_loss: 12953442304.0000 - val_r2: 0.5274 - 3s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "342/342 - 3s - loss: 5261165056.0000 - r2: 0.7772 - val_loss: 12886857728.0000 - val_r2: 0.5291 - 3s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "342/342 - 3s - loss: 5215907840.0000 - r2: 0.7796 - val_loss: 12823716864.0000 - val_r2: 0.5319 - 3s/epoch - 9ms/step\n",
      "Epoch 21/50\n",
      "342/342 - 3s - loss: 5173784064.0000 - r2: 0.7808 - val_loss: 12760763392.0000 - val_r2: 0.5336 - 3s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "342/342 - 3s - loss: 5134445056.0000 - r2: 0.7825 - val_loss: 12713008128.0000 - val_r2: 0.5352 - 3s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "342/342 - 3s - loss: 5097603584.0000 - r2: 0.7845 - val_loss: 12660216832.0000 - val_r2: 0.5374 - 3s/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "342/342 - 3s - loss: 5062876672.0000 - r2: 0.7856 - val_loss: 12612407296.0000 - val_r2: 0.5388 - 3s/epoch - 10ms/step\n",
      "Epoch 25/50\n",
      "342/342 - 3s - loss: 5030133248.0000 - r2: 0.7872 - val_loss: 12577822720.0000 - val_r2: 0.5401 - 3s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "342/342 - 3s - loss: 4999297536.0000 - r2: 0.7882 - val_loss: 12544423936.0000 - val_r2: 0.5414 - 3s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "342/342 - 3s - loss: 4970029568.0000 - r2: 0.7900 - val_loss: 12502348800.0000 - val_r2: 0.5432 - 3s/epoch - 10ms/step\n",
      "Epoch 28/50\n",
      "342/342 - 3s - loss: 4942099456.0000 - r2: 0.7908 - val_loss: 12465828864.0000 - val_r2: 0.5453 - 3s/epoch - 10ms/step\n",
      "Epoch 29/50\n",
      "342/342 - 3s - loss: 4915597312.0000 - r2: 0.7920 - val_loss: 12432671744.0000 - val_r2: 0.5457 - 3s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "342/342 - 3s - loss: 4890245120.0000 - r2: 0.7932 - val_loss: 12392673280.0000 - val_r2: 0.5471 - 3s/epoch - 9ms/step\n",
      "Epoch 31/50\n",
      "342/342 - 3s - loss: 4866178560.0000 - r2: 0.7935 - val_loss: 12348358656.0000 - val_r2: 0.5488 - 3s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "342/342 - 3s - loss: 4843033088.0000 - r2: 0.7951 - val_loss: 12314103808.0000 - val_r2: 0.5491 - 3s/epoch - 10ms/step\n",
      "Epoch 33/50\n",
      "342/342 - 3s - loss: 4820815360.0000 - r2: 0.7954 - val_loss: 12298465280.0000 - val_r2: 0.5508 - 3s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "342/342 - 3s - loss: 4799574528.0000 - r2: 0.7972 - val_loss: 12268046336.0000 - val_r2: 0.5512 - 3s/epoch - 9ms/step\n",
      "Epoch 35/50\n",
      "342/342 - 3s - loss: 4779074560.0000 - r2: 0.7975 - val_loss: 12247566336.0000 - val_r2: 0.5520 - 3s/epoch - 9ms/step\n",
      "Epoch 36/50\n",
      "342/342 - 3s - loss: 4759351808.0000 - r2: 0.7984 - val_loss: 12211846144.0000 - val_r2: 0.5540 - 3s/epoch - 10ms/step\n",
      "Epoch 37/50\n",
      "342/342 - 3s - loss: 4740316672.0000 - r2: 0.7992 - val_loss: 12178167808.0000 - val_r2: 0.5548 - 3s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "342/342 - 4s - loss: 4721958912.0000 - r2: 0.8000 - val_loss: 12154326016.0000 - val_r2: 0.5560 - 4s/epoch - 10ms/step\n",
      "Epoch 39/50\n",
      "342/342 - 4s - loss: 4704320512.0000 - r2: 0.8009 - val_loss: 12128704512.0000 - val_r2: 0.5559 - 4s/epoch - 10ms/step\n",
      "Epoch 40/50\n",
      "342/342 - 3s - loss: 4687097856.0000 - r2: 0.8012 - val_loss: 12111930368.0000 - val_r2: 0.5575 - 3s/epoch - 10ms/step\n",
      "Epoch 41/50\n",
      "342/342 - 3s - loss: 4670609408.0000 - r2: 0.8023 - val_loss: 12090046464.0000 - val_r2: 0.5589 - 3s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "342/342 - 4s - loss: 4654521856.0000 - r2: 0.8028 - val_loss: 12063319040.0000 - val_r2: 0.5589 - 4s/epoch - 11ms/step\n",
      "Epoch 43/50\n",
      "342/342 - 4s - loss: 4638942208.0000 - r2: 0.8034 - val_loss: 12046049280.0000 - val_r2: 0.5592 - 4s/epoch - 11ms/step\n",
      "Epoch 44/50\n",
      "342/342 - 3s - loss: 4623856640.0000 - r2: 0.8042 - val_loss: 12020917248.0000 - val_r2: 0.5607 - 3s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "342/342 - 3s - loss: 4609193984.0000 - r2: 0.8048 - val_loss: 12001172480.0000 - val_r2: 0.5617 - 3s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "342/342 - 3s - loss: 4594955264.0000 - r2: 0.8053 - val_loss: 11975957504.0000 - val_r2: 0.5622 - 3s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "342/342 - 3s - loss: 4581113344.0000 - r2: 0.8059 - val_loss: 11950515200.0000 - val_r2: 0.5637 - 3s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "342/342 - 3s - loss: 4567669248.0000 - r2: 0.8065 - val_loss: 11940403200.0000 - val_r2: 0.5634 - 3s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "342/342 - 3s - loss: 4554631680.0000 - r2: 0.8071 - val_loss: 11929098240.0000 - val_r2: 0.5638 - 3s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "342/342 - 3s - loss: 4541816832.0000 - r2: 0.8074 - val_loss: 11907699712.0000 - val_r2: 0.5642 - 3s/epoch - 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "output = layers.Dense(1, activation=\"linear\")(all_features)\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=sgd_opt,\n",
    "              loss='mse', \n",
    "              metrics=[r2])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=no_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    verbose = 2,\n",
    "                    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOeAYUIm6YpI",
    "outputId": "9166f5b1-2cad-4456-8f98-588cfa810eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s - loss: 11907702784.0000 - r2: 0.5644 - 990ms/epoch - 5ms/step\n",
      "------- Evaluation of SGD optimiser -------\n",
      "Mean Squared Error: 11907702784.0\n",
      "R Squared Error: 0.564415454864502\n"
     ]
    }
   ],
   "source": [
    "evaluate_sgd = model.evaluate(val_ds, verbose =2)\n",
    "\n",
    "print(\"------- Evaluation of SGD optimiser -------\")\n",
    "print(\"Mean Squared Error: \" + str(evaluate_sgd[0]))\n",
    "print(\"R Squared Error: \" + str(evaluate_sgd[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvVhA6VD-Cok",
    "outputId": "0ed9de2c-12a0-43b2-dfde-89cdd3dc1443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error (Adam Optimizer): -10.088726043701172\n",
      "R Squared Error (SGD Optimizer): 0.564415454864502\n"
     ]
    }
   ],
   "source": [
    "# report test r^2 value\n",
    "print(\"R Squared Error (Adam Optimizer): \" + str(evaluate_adam[1]))\n",
    "print(\"R Squared Error (SGD Optimizer): \" + str(evaluate_sgd[1]))\n",
    "\n",
    "# explain why change to sgd fixes problem faced when using adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqxHs3mro-EF"
   },
   "source": [
    "### Part d\n",
    "\n",
    "Add 1 hidden layer (10 units) to the architecture in Q1c and train it with the same configuration as in Q1c (i.e. with Adam) except that the learning rate is increased to 0.08. Report the test R^2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jtBomGFxo-P4"
   },
   "outputs": [],
   "source": [
    "# adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.08)\n",
    "\n",
    "no_epochs = 50\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOmnwYZ_-1dh",
    "outputId": "3dca6fc8-4740-452c-df75-253ef96f3347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 - 4s - loss: 207019835392.0000 - r2: -7.8079e+00 - val_loss: 254408441856.0000 - val_r2: -8.3368e+00 - 4s/epoch - 11ms/step\n",
      "Epoch 2/50\n",
      "342/342 - 3s - loss: 136503885824.0000 - r2: -4.8137e+00 - val_loss: 145572216832.0000 - val_r2: -4.3419e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "342/342 - 3s - loss: 56194830336.0000 - r2: -1.3895e+00 - val_loss: 59504099328.0000 - val_r2: -1.1821e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "342/342 - 3s - loss: 15815080960.0000 - r2: 0.3302 - val_loss: 23683538944.0000 - val_r2: 0.1338 - 3s/epoch - 10ms/step\n",
      "Epoch 5/50\n",
      "342/342 - 4s - loss: 6442761728.0000 - r2: 0.7280 - val_loss: 14698857472.0000 - val_r2: 0.4635 - 4s/epoch - 10ms/step\n",
      "Epoch 6/50\n",
      "342/342 - 3s - loss: 5273347072.0000 - r2: 0.7771 - val_loss: 12822839296.0000 - val_r2: 0.5315 - 3s/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "342/342 - 3s - loss: 4954203136.0000 - r2: 0.7905 - val_loss: 12347603968.0000 - val_r2: 0.5493 - 3s/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "342/342 - 3s - loss: 4714177536.0000 - r2: 0.8003 - val_loss: 11978106880.0000 - val_r2: 0.5626 - 3s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "342/342 - 4s - loss: 4514967040.0000 - r2: 0.8087 - val_loss: 11776353280.0000 - val_r2: 0.5696 - 4s/epoch - 10ms/step\n",
      "Epoch 10/50\n",
      "342/342 - 3s - loss: 4351629824.0000 - r2: 0.8154 - val_loss: 11473598464.0000 - val_r2: 0.5809 - 3s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "342/342 - 3s - loss: 4218485248.0000 - r2: 0.8211 - val_loss: 11428574208.0000 - val_r2: 0.5814 - 3s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "342/342 - 3s - loss: 4111737088.0000 - r2: 0.8257 - val_loss: 11206619136.0000 - val_r2: 0.5902 - 3s/epoch - 9ms/step\n",
      "Epoch 13/50\n",
      "342/342 - 3s - loss: 4026204928.0000 - r2: 0.8293 - val_loss: 11196865536.0000 - val_r2: 0.5897 - 3s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "342/342 - 3s - loss: 3958505984.0000 - r2: 0.8318 - val_loss: 11067008000.0000 - val_r2: 0.5950 - 3s/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "342/342 - 3s - loss: 3902410496.0000 - r2: 0.8345 - val_loss: 11040367616.0000 - val_r2: 0.5964 - 3s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "342/342 - 3s - loss: 3852460032.0000 - r2: 0.8366 - val_loss: 10825159680.0000 - val_r2: 0.6040 - 3s/epoch - 10ms/step\n",
      "Epoch 17/50\n",
      "342/342 - 3s - loss: 3810726656.0000 - r2: 0.8383 - val_loss: 10792839168.0000 - val_r2: 0.6051 - 3s/epoch - 10ms/step\n",
      "Epoch 18/50\n",
      "342/342 - 3s - loss: 3780804608.0000 - r2: 0.8397 - val_loss: 10700705792.0000 - val_r2: 0.6080 - 3s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "342/342 - 3s - loss: 3756980736.0000 - r2: 0.8402 - val_loss: 10707752960.0000 - val_r2: 0.6076 - 3s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "342/342 - 3s - loss: 3737204480.0000 - r2: 0.8413 - val_loss: 10519977984.0000 - val_r2: 0.6149 - 3s/epoch - 9ms/step\n",
      "Epoch 21/50\n",
      "342/342 - 3s - loss: 3719201024.0000 - r2: 0.8417 - val_loss: 10634329088.0000 - val_r2: 0.6106 - 3s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "342/342 - 3s - loss: 3699665408.0000 - r2: 0.8427 - val_loss: 10390865920.0000 - val_r2: 0.6196 - 3s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "342/342 - 3s - loss: 3677697536.0000 - r2: 0.8439 - val_loss: 10517159936.0000 - val_r2: 0.6147 - 3s/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "342/342 - 3s - loss: 3647517696.0000 - r2: 0.8452 - val_loss: 10354115584.0000 - val_r2: 0.6204 - 3s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "342/342 - 3s - loss: 3615304448.0000 - r2: 0.8464 - val_loss: 10255617024.0000 - val_r2: 0.6241 - 3s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "342/342 - 3s - loss: 3576320256.0000 - r2: 0.8481 - val_loss: 10512381952.0000 - val_r2: 0.6140 - 3s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "342/342 - 2s - loss: 3521739520.0000 - r2: 0.8504 - val_loss: 10287671296.0000 - val_r2: 0.6223 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "342/342 - 3s - loss: 3450344192.0000 - r2: 0.8533 - val_loss: 10241894400.0000 - val_r2: 0.6244 - 3s/epoch - 10ms/step\n",
      "Epoch 29/50\n",
      "342/342 - 3s - loss: 3371510272.0000 - r2: 0.8565 - val_loss: 10077174784.0000 - val_r2: 0.6306 - 3s/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "342/342 - 3s - loss: 3295931136.0000 - r2: 0.8601 - val_loss: 10064217088.0000 - val_r2: 0.6301 - 3s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "342/342 - 3s - loss: 3231397376.0000 - r2: 0.8626 - val_loss: 9814301696.0000 - val_r2: 0.6393 - 3s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "342/342 - 3s - loss: 3176834304.0000 - r2: 0.8649 - val_loss: 9662038016.0000 - val_r2: 0.6455 - 3s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "342/342 - 3s - loss: 3124971520.0000 - r2: 0.8671 - val_loss: 9820868608.0000 - val_r2: 0.6388 - 3s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "342/342 - 3s - loss: 3076001024.0000 - r2: 0.8691 - val_loss: 9702294528.0000 - val_r2: 0.6438 - 3s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "342/342 - 3s - loss: 3032786944.0000 - r2: 0.8709 - val_loss: 9650851840.0000 - val_r2: 0.6454 - 3s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "342/342 - 3s - loss: 2998181888.0000 - r2: 0.8726 - val_loss: 9555829760.0000 - val_r2: 0.6484 - 3s/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "342/342 - 3s - loss: 2968280576.0000 - r2: 0.8737 - val_loss: 9802114048.0000 - val_r2: 0.6400 - 3s/epoch - 8ms/step\n",
      "Epoch 38/50\n",
      "342/342 - 3s - loss: 2944018944.0000 - r2: 0.8750 - val_loss: 9824362496.0000 - val_r2: 0.6389 - 3s/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "342/342 - 3s - loss: 2923226368.0000 - r2: 0.8756 - val_loss: 9404538880.0000 - val_r2: 0.6540 - 3s/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "342/342 - 3s - loss: 2905420544.0000 - r2: 0.8767 - val_loss: 9454282752.0000 - val_r2: 0.6526 - 3s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "342/342 - 3s - loss: 2889059328.0000 - r2: 0.8770 - val_loss: 9719764992.0000 - val_r2: 0.6433 - 3s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "342/342 - 3s - loss: 2874974208.0000 - r2: 0.8774 - val_loss: 9548214272.0000 - val_r2: 0.6487 - 3s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "342/342 - 3s - loss: 2860904448.0000 - r2: 0.8784 - val_loss: 9567109120.0000 - val_r2: 0.6488 - 3s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "342/342 - 3s - loss: 2848202240.0000 - r2: 0.8790 - val_loss: 9639632896.0000 - val_r2: 0.6452 - 3s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "342/342 - 3s - loss: 2835690240.0000 - r2: 0.8792 - val_loss: 9531211776.0000 - val_r2: 0.6503 - 3s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "342/342 - 3s - loss: 2825117696.0000 - r2: 0.8795 - val_loss: 9532017664.0000 - val_r2: 0.6500 - 3s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "342/342 - 3s - loss: 2815551232.0000 - r2: 0.8804 - val_loss: 9479763968.0000 - val_r2: 0.6524 - 3s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "342/342 - 3s - loss: 2805882368.0000 - r2: 0.8806 - val_loss: 9545123840.0000 - val_r2: 0.6495 - 3s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "342/342 - 3s - loss: 2796815872.0000 - r2: 0.8811 - val_loss: 9444556800.0000 - val_r2: 0.6530 - 3s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "342/342 - 3s - loss: 2788363008.0000 - r2: 0.8814 - val_loss: 9357885440.0000 - val_r2: 0.6560 - 3s/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "x = layers.Dense(10, activation=\"relu\")(all_features)\n",
    "output = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=opt,\n",
    "              loss='mse', \n",
    "              metrics=[r2])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=no_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    verbose = 2,\n",
    "                    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIIaIaYW-1jo",
    "outputId": "626643ff-9e7d-4e99-8bcd-f928a5947507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s - loss: 9357888512.0000 - r2: 0.6564 - 966ms/epoch - 5ms/step\n",
      "Mean Squared Error: 9357888512.0\n",
      "R Squared Error: 0.6564346551895142\n"
     ]
    }
   ],
   "source": [
    "evaluate_model = model.evaluate(val_ds, verbose =2)\n",
    "\n",
    "print(\"Mean Squared Error: \" + str(evaluate_model[0]))\n",
    "print(\"R Squared Error: \" + str(evaluate_model[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZfHjO2io-Yf"
   },
   "source": [
    "### Part e\n",
    "\n",
    "Compare the performance of the linear regression model to the Dense layer (Q1c) and the NN architecture (Q1d) and suggest reasons for the observations you made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6x4GfxRWo-h_",
    "outputId": "b3fe8624-f175-409f-cb46-8e83b73d106c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error (Adam Optimizer, Dense): -10.088726043701172\n",
      "R Squared Error (SGD Optimizer, Dense): 0.564415454864502\n",
      "R Squared Error (Adam Optimizer, NN): 0.6564346551895142\n"
     ]
    }
   ],
   "source": [
    "# adam optimizer, learning rate 0.001 (dense layer)\n",
    "print(\"R Squared Error (Adam Optimizer, Dense): \" + str(evaluate_adam[1]))\n",
    "\n",
    "# sgd optimizer, learning rate 0.001 (dense layer)\n",
    "print(\"R Squared Error (SGD Optimizer, Dense): \" + str(evaluate_sgd[1]))\n",
    "\n",
    "# adam optimizer, learing rate 0.08 (nn architecture)\n",
    "print(\"R Squared Error (Adam Optimizer, NN): \" + str(evaluate_model[1]))\n",
    "\n",
    "# Compare the performance\n",
    "\n",
    "# Suggest reasons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II3_yxh-pUZh"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Neural networks offer much more than fundamental machine learning algorithms. In this part of the assignment, we will investigate one of its advantages: the use of trainable embeddings. Also, we will learn how to set up a quick and convenient way of tuning your neural network models.\n",
    "\n",
    "Instead of using one-hot encoding, an alternative approach is to use embeddings to encode categorical variables. Such an approach utilises the ability of neural networks to learn richer representations1 of the data – an edge it has over traditional ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RdfWtftCi0w"
   },
   "source": [
    "### Part a\n",
    "\n",
    "Further split the data from year 2020 and before (i.e. those not in test set) by using data from year 2020 as validation set and the rest as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lXd05dUYCjPd"
   },
   "outputs": [],
   "source": [
    "train_dataframe = df[df['year'] < 2020]\n",
    "val_dataframe = df[df['year'] == 2020]\n",
    "test_dataframe = df[df['year'] > 2020]\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "test_ds = dataframe_to_dataset(test_dataframe)\n",
    "\n",
    "\n",
    "train_ds = train_ds.batch(256)\n",
    "val_ds = val_ds.batch(256)\n",
    "test_ds = test_ds.batch(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT8wmlHTCon8"
   },
   "source": [
    "### Part b\n",
    "\n",
    "For each categorical variable, replace the one-hot encoding with the layer tf.keras.layers.Embedding(). Set output_dim = floor(num_categories//divisor).\n",
    "‘num_categories’ refers to the number of categories in the categorical variable.\n",
    "‘divisor’ is a parameter which we will tune later (Hint: You will still need the\n",
    "lookup classes from Q1b. Read the documentation to find out what to change.)\n",
    "\n",
    "The Embedding layer produces a 2D output (3D, including batch), which cannot\n",
    "be concatenated with the other features. Add a Flatten layer to resolve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mjM3vx_FFMwO"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bI6EXGyACovt"
   },
   "outputs": [],
   "source": [
    "def embed_categorical_feature(feature, name, dataset, is_string, num_categories, divisor = 1):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"int\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    encoded_lookup = lookup(feature)\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = keras.layers.Embedding(input_dim = (num_categories+1), \n",
    "                                             output_dim = math.floor(num_categories/divisor))(encoded_lookup)\n",
    "    encoded_feature = keras.layers.Flatten()(encoded_feature)\n",
    "    \n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Pwqo2XI8FOwe"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers\n",
    "month_len = len(np.unique(train_dataframe['month']))\n",
    "month = keras.Input(shape=(1,), name=\"month\", dtype=\"int64\")\n",
    "month_embedded = embed_categorical_feature(month, \"month\", train_ds, False, month_len)\n",
    "\n",
    "# Categorical feature encoded as string\n",
    "flat_model_type_len = len(np.unique(train_dataframe['flat_model_type']))\n",
    "flat_model_type = keras.Input(shape=(1,), name=\"flat_model_type\", dtype=\"string\")\n",
    "flat_model_type_embedded = embed_categorical_feature(flat_model_type, \"flat_model_type\", train_ds, True, flat_model_type_len)\n",
    "\n",
    "storey_range_len = len(np.unique(train_dataframe['storey_range']))\n",
    "storey_range = keras.Input(shape=(1,), name=\"storey_range\", dtype=\"string\")\n",
    "storey_range_embedded = embed_categorical_feature(storey_range, \"storey_range\", train_ds, True, storey_range_len)\n",
    "\n",
    "# Numerical features\n",
    "floor_area_sqm = keras.Input(shape=(1,), name=\"floor_area_sqm\")\n",
    "floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, \"floor_area_sqm\", train_ds)\n",
    "\n",
    "remaining_lease_years = keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, \"remaining_lease_years\", train_ds)\n",
    "\n",
    "eigenvector_centrality = keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, \"eigenvector_centrality\", train_ds) \n",
    "\n",
    "degree_centrality = keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "degree_centrality_encoded = encode_numerical_feature(degree_centrality, \"degree_centrality\", train_ds)\n",
    "\n",
    "dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, \"dist_to_dhoby\", train_ds)\n",
    "\n",
    "dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, \"dist_to_nearest_stn\", train_ds)\n",
    "\n",
    "all_inputs = [\n",
    "    dist_to_nearest_stn,\n",
    "    dist_to_dhoby,\n",
    "    degree_centrality,\n",
    "    eigenvector_centrality,\n",
    "    remaining_lease_years,\n",
    "    floor_area_sqm,\n",
    "    month,\n",
    "    flat_model_type,\n",
    "    storey_range\n",
    "]\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        month_embedded,\n",
    "        flat_model_type_embedded,\n",
    "        floor_area_sqm_encoded,\n",
    "        storey_range_embedded,\n",
    "        remaining_lease_years_encoded,\n",
    "        eigenvector_centrality_encoded,\n",
    "        degree_centrality_encoded,\n",
    "        dist_to_dhoby_encoded,\n",
    "        dist_to_nearest_stn_encoded        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZoGL0foGHOAB"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "output = layers.Dense(1, activation=\"linear\")(all_features)\n",
    "model = keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "wlUPB-seHOEi",
    "outputId": "b0edf6a4-fede-43a8-a782-224df0bba297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# plot the model\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEXVLfnbCviP"
   },
   "source": [
    "### Part c\n",
    "\n",
    "Via a callback, introduce early stopping (based on val_loss, with patience of\n",
    "10 epochs) to the model.\n",
    "\n",
    "Using this as a reference, use KerasTuner (with the RandomSearch algorithm) to tune the model on the validation set, according to the following ranges:\n",
    "\n",
    "*   Number of neurons: min=4, max=32, step=4\n",
    "*   Learning rate: min=1e-4, max=2e-1, sampling=’log’\n",
    "*   Divisor: min=1, max=2, step=1\n",
    "\n",
    "Run 10 iterations of parameter search (i.e. max_trials=10), each for 50 epochs and report the best set of hyperparameters (based on validation accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "me9mTJ_hhgkf",
    "outputId": "e1a28596-620b-4914-a910-f31815db9fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (2.24.0)\n",
      "Requirement already satisfied: tensorboard in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (2.10.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (1.23.3)\n",
      "Requirement already satisfied: ipython in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (7.16.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.34.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.49.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.19.5)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (4.4.2)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (3.0.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.4.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.4.3)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (2.6.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->keras-tuner) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyoIDPGthdA8",
    "outputId": "c1632b6a-6ac2-4d73-a608-9587cd65eb3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-5fd8096cdee5>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bOWNl6Z-NvGa"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def model_builder(hp):\n",
    "    tune_divisor = hp.Int('divisor', min_value=1, max_value=2, step=1)\n",
    "\n",
    "    # Categorical features encoded as integers\n",
    "    month_len = len(np.unique(train_dataframe['month']))\n",
    "    month = keras.Input(shape=(1,), name=\"month\", dtype=\"int64\")\n",
    "    month_embedded = embed_categorical_feature(month, \"month\", train_ds, False, month_len, tune_divisor)\n",
    "\n",
    "    # Categorical feature encoded as string\n",
    "    flat_model_type_len = len(np.unique(train_dataframe['flat_model_type']))\n",
    "    flat_model_type = keras.Input(shape=(1,), name=\"flat_model_type\", dtype=\"string\")\n",
    "    flat_model_type_embedded = embed_categorical_feature(flat_model_type, \"flat_model_type\", train_ds, True, flat_model_type_len, tune_divisor)\n",
    "\n",
    "    storey_range_len = len(np.unique(train_dataframe['storey_range']))\n",
    "    storey_range = keras.Input(shape=(1,), name=\"storey_range\", dtype=\"string\")\n",
    "    storey_range_embedded = embed_categorical_feature(storey_range, \"storey_range\", train_ds, True, storey_range_len, tune_divisor)\n",
    "\n",
    "    # Numerical features\n",
    "    floor_area_sqm = keras.Input(shape=(1,), name=\"floor_area_sqm\")\n",
    "    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, \"floor_area_sqm\", train_ds)\n",
    "\n",
    "    remaining_lease_years = keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "    remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, \"remaining_lease_years\", train_ds)\n",
    "\n",
    "    eigenvector_centrality = keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, \"eigenvector_centrality\", train_ds) \n",
    "\n",
    "    degree_centrality = keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "    degree_centrality_encoded = encode_numerical_feature(degree_centrality, \"degree_centrality\", train_ds)\n",
    "\n",
    "    dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, \"dist_to_dhoby\", train_ds)\n",
    "\n",
    "    dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, \"dist_to_nearest_stn\", train_ds)\n",
    "\n",
    "    all_inputs = [\n",
    "      dist_to_nearest_stn,\n",
    "      dist_to_dhoby,\n",
    "      degree_centrality,\n",
    "      eigenvector_centrality,\n",
    "      remaining_lease_years,\n",
    "      floor_area_sqm,\n",
    "      month,\n",
    "      flat_model_type,\n",
    "      storey_range\n",
    "    ]\n",
    "\n",
    "    all_features = layers.concatenate(\n",
    "      [\n",
    "          month_embedded,\n",
    "          flat_model_type_embedded,\n",
    "          floor_area_sqm_encoded,\n",
    "          storey_range_embedded,\n",
    "          remaining_lease_years_encoded,\n",
    "          eigenvector_centrality_encoded,\n",
    "          degree_centrality_encoded,\n",
    "          dist_to_dhoby_encoded,\n",
    "          dist_to_nearest_stn_encoded        \n",
    "      ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # define the model\n",
    "    x = layers.Dense(hp.Int('neuron_no', min_value=4, max_value=32, step=4), \n",
    "                   activation=\"relu\")(all_features)\n",
    "    output = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = keras.Model(all_inputs, output)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 2e-1, sampling='log')),\n",
    "                loss='mse', \n",
    "                metrics=[r2, 'accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pDaMsxpWNvI2"
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYAFlZZfNvRX",
    "outputId": "996d0541-bf91-47a8-de50-0f3409b5a7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 57s]\n",
      "val_loss: 3052763904.0\n",
      "\n",
      "Best val_loss So Far: 3052763904.0\n",
      "Total elapsed time: 00h 21m 42s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds, \n",
    "             validation_data=val_ds,\n",
    "             epochs=50,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUQikD4XDdAq",
    "outputId": "73c27b1c-0e53-4778-f696-3edb26d550ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons:  12\n",
      "Divisor:  1\n",
      "Learning_rate: 0.06861805082794845\n"
     ]
    }
   ],
   "source": [
    "best_neuron = tuner.get_best_hyperparameters()[0].get('neuron_no')\n",
    "print('Number of neurons: ', best_neuron)\n",
    "\n",
    "best_divisor = tuner.get_best_hyperparameters()[0].get('divisor')\n",
    "print('Divisor: ', best_divisor)\n",
    "\n",
    "best_learning = tuner.get_best_hyperparameters()[0].get('learning_rate')\n",
    "print('Learning_rate:', best_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k8qZeEnDHRj"
   },
   "source": [
    "### Part d\n",
    "\n",
    "Using the best model configuration, train a model on the non-test split (i.e. year 2020 and before) for 50 epochs. Generate a plot to show how the train\n",
    "and test root mean square errors (RMSE) changes across epochs.\n",
    "\n",
    "(Tip: You can skip the first few epochs if the plot gets dominated by them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "M9kkcnnxhBBg"
   },
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PisJdk25TUjy",
    "outputId": "a7516030-f609-4550-d6b5-dd2f8048a40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "251/251 [==============================] - 4s 7ms/step - loss: 2742367744.0000 - r2: 0.8831 - accuracy: 0.0000e+00 - val_loss: 3076404736.0000 - val_r2: 0.8697 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2724821760.0000 - r2: 0.8839 - accuracy: 0.0000e+00 - val_loss: 3100934400.0000 - val_r2: 0.8697 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2714676992.0000 - r2: 0.8838 - accuracy: 0.0000e+00 - val_loss: 3203266304.0000 - val_r2: 0.8655 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2710545408.0000 - r2: 0.8843 - accuracy: 0.0000e+00 - val_loss: 3115184128.0000 - val_r2: 0.8672 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2701910784.0000 - r2: 0.8847 - accuracy: 0.0000e+00 - val_loss: 3032467456.0000 - val_r2: 0.8709 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2683100416.0000 - r2: 0.8856 - accuracy: 0.0000e+00 - val_loss: 2992321792.0000 - val_r2: 0.8729 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2681405184.0000 - r2: 0.8849 - accuracy: 0.0000e+00 - val_loss: 3031550976.0000 - val_r2: 0.8713 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2680560384.0000 - r2: 0.8853 - accuracy: 0.0000e+00 - val_loss: 2912726784.0000 - val_r2: 0.8775 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2682145280.0000 - r2: 0.8855 - accuracy: 0.0000e+00 - val_loss: 3066536448.0000 - val_r2: 0.8682 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2665596928.0000 - r2: 0.8859 - accuracy: 0.0000e+00 - val_loss: 3181792256.0000 - val_r2: 0.8622 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2668735232.0000 - r2: 0.8861 - accuracy: 0.0000e+00 - val_loss: 3002556928.0000 - val_r2: 0.8728 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2657313024.0000 - r2: 0.8862 - accuracy: 0.0000e+00 - val_loss: 3251215360.0000 - val_r2: 0.8623 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2659804160.0000 - r2: 0.8864 - accuracy: 0.0000e+00 - val_loss: 3018425088.0000 - val_r2: 0.8731 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2652711168.0000 - r2: 0.8868 - accuracy: 0.0000e+00 - val_loss: 3021974784.0000 - val_r2: 0.8720 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2646164480.0000 - r2: 0.8864 - accuracy: 0.0000e+00 - val_loss: 3169217280.0000 - val_r2: 0.8654 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2645075200.0000 - r2: 0.8871 - accuracy: 0.0000e+00 - val_loss: 3150728960.0000 - val_r2: 0.8664 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2644152320.0000 - r2: 0.8871 - accuracy: 0.0000e+00 - val_loss: 3030300416.0000 - val_r2: 0.8720 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2644647168.0000 - r2: 0.8871 - accuracy: 0.0000e+00 - val_loss: 2917341952.0000 - val_r2: 0.8760 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2658644480.0000 - r2: 0.8867 - accuracy: 0.0000e+00 - val_loss: 3114024192.0000 - val_r2: 0.8681 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2659143936.0000 - r2: 0.8859 - accuracy: 0.0000e+00 - val_loss: 2974658304.0000 - val_r2: 0.8730 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2637866752.0000 - r2: 0.8871 - accuracy: 0.0000e+00 - val_loss: 2900981760.0000 - val_r2: 0.8764 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2634922752.0000 - r2: 0.8871 - accuracy: 0.0000e+00 - val_loss: 3110894336.0000 - val_r2: 0.8671 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2636347136.0000 - r2: 0.8876 - accuracy: 0.0000e+00 - val_loss: 2892188160.0000 - val_r2: 0.8777 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2634379776.0000 - r2: 0.8873 - accuracy: 0.0000e+00 - val_loss: 3063166720.0000 - val_r2: 0.8704 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2631212032.0000 - r2: 0.8877 - accuracy: 0.0000e+00 - val_loss: 2917953536.0000 - val_r2: 0.8762 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2630325504.0000 - r2: 0.8876 - accuracy: 0.0000e+00 - val_loss: 3024696832.0000 - val_r2: 0.8719 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2624620032.0000 - r2: 0.8878 - accuracy: 0.0000e+00 - val_loss: 3079793664.0000 - val_r2: 0.8694 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2627687168.0000 - r2: 0.8880 - accuracy: 0.0000e+00 - val_loss: 3021399296.0000 - val_r2: 0.8719 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2626332928.0000 - r2: 0.8880 - accuracy: 0.0000e+00 - val_loss: 2927122944.0000 - val_r2: 0.8747 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2630862336.0000 - r2: 0.8876 - accuracy: 0.0000e+00 - val_loss: 3160326144.0000 - val_r2: 0.8671 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2615572224.0000 - r2: 0.8884 - accuracy: 0.0000e+00 - val_loss: 2960148224.0000 - val_r2: 0.8749 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2611386368.0000 - r2: 0.8888 - accuracy: 0.0000e+00 - val_loss: 3161859328.0000 - val_r2: 0.8653 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2617780480.0000 - r2: 0.8883 - accuracy: 0.0000e+00 - val_loss: 3190554624.0000 - val_r2: 0.8639 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2622971136.0000 - r2: 0.8875 - accuracy: 0.0000e+00 - val_loss: 2989939200.0000 - val_r2: 0.8742 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2619346944.0000 - r2: 0.8879 - accuracy: 0.0000e+00 - val_loss: 3084566016.0000 - val_r2: 0.8700 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2617741312.0000 - r2: 0.8883 - accuracy: 0.0000e+00 - val_loss: 2920293632.0000 - val_r2: 0.8759 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2612183296.0000 - r2: 0.8883 - accuracy: 0.0000e+00 - val_loss: 2856537856.0000 - val_r2: 0.8795 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2612422400.0000 - r2: 0.8880 - accuracy: 0.0000e+00 - val_loss: 3056682752.0000 - val_r2: 0.8682 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2613199104.0000 - r2: 0.8885 - accuracy: 0.0000e+00 - val_loss: 2857445888.0000 - val_r2: 0.8788 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2630849024.0000 - r2: 0.8876 - accuracy: 0.0000e+00 - val_loss: 2927284480.0000 - val_r2: 0.8764 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2609615616.0000 - r2: 0.8885 - accuracy: 0.0000e+00 - val_loss: 2911839232.0000 - val_r2: 0.8753 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2611265792.0000 - r2: 0.8885 - accuracy: 0.0000e+00 - val_loss: 3025272832.0000 - val_r2: 0.8698 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2609202176.0000 - r2: 0.8889 - accuracy: 0.0000e+00 - val_loss: 2879674880.0000 - val_r2: 0.8768 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2614489856.0000 - r2: 0.8880 - accuracy: 0.0000e+00 - val_loss: 3024155904.0000 - val_r2: 0.8726 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2608793344.0000 - r2: 0.8887 - accuracy: 0.0000e+00 - val_loss: 2932088832.0000 - val_r2: 0.8768 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2601906432.0000 - r2: 0.8893 - accuracy: 0.0000e+00 - val_loss: 2936886528.0000 - val_r2: 0.8738 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2613074432.0000 - r2: 0.8882 - accuracy: 0.0000e+00 - val_loss: 2989996032.0000 - val_r2: 0.8744 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 2612466944.0000 - r2: 0.8885 - accuracy: 0.0000e+00 - val_loss: 3016875520.0000 - val_r2: 0.8731 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2614091520.0000 - r2: 0.8883 - accuracy: 0.0000e+00 - val_loss: 2921374464.0000 - val_r2: 0.8754 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "251/251 [==============================] - 2s 6ms/step - loss: 2605824512.0000 - r2: 0.8883 - accuracy: 0.0000e+00 - val_loss: 3076871424.0000 - val_r2: 0.8688 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20afd2e5d90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rLL0dT35o9qC"
   },
   "outputs": [],
   "source": [
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "hzR9OHRqo9vo",
    "outputId": "ca3a5ddb-2f32-472d-80b9-36f3417d2fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20a85e9d220>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhdVZ3v//f3DDUPqapUKkNFwxBoA4REYohgtwwScEDytHKBi01suc2V9jq0bQvYjSCKgo+NfRGHH638Ag4MggytogQwIi1TogiGKVEiqcypylCVms/53j/2OlWnikpVqNSpUzn1eT3Pfvbea++1z1pFON+zhr23uTsiIiJjLZbvAoiISGFSgBERkZxQgBERkZxQgBERkZxQgBERkZxQgBERkZxQgBGZgMxsrZmdku9yjDUzm2NmbmaJfJdFck8BRg4JZrbBzDrMrM3MtprZCjOryDq+InxxvX9Qvv8I6R8O+0Vm9u9m1hSu9aqZfX0/n5NZbhq3igbufoy7rzqYa5jZ1Wb2gzEqksgbpgAjh5Kz3b0CWAAsBK4YdPwVYHlmJ/xKPhf4U9Y5VwCLgMVAJXAq8PuhPidr+T9jWw2RyUEBRg457r4V+CVRoMn2X8DJZlYT9s8CngO2Zp3zNuBed9/skQ3ufttoymFmi83sCTPbbWZbzOwmMyvKOr7UzF42sz1m9i0z+7WZ/a9w7Agze9TMms1sp5n90MymZOXdYGbvCttXm9ldZnabmbWG7rNFWedeZmabwrGXzex0MzsL+BxwXmiF/WE/dZhpZveY2Y7QmvtE1rGrzexuM7szXPt3ZnZ81vG3mNmqUP+12a1HMysNLcW/hPo/bmalWR99oZm9Fur+r4P+pqvNbK+ZbTOzG0bz30YmBgUYOeSYWSPwbmD9oEOdwAPA+WH/ImBw8HgS+LSZ/aOZHWdmdhBFSQH/BEwF3g6cDvxjKONU4G6iFlMd8DJwUnY1gK8AM4G3ALOBq4f5rPcDdwBTiOp4U/ico4H/A7zN3SuBM4EN7v4L4MvAnaEVdvzgC5pZjCgo/wGYFcr/KTM7M+u0c4AfA7XAj4D7zCxpZsmQ9yFgGvBx4IehPABfA04Ida4FPguks677DuDo8JmfN7O3hPT/C/xfd68CjgDuGuZvIhOcAowcSu4zs1ZgI7AduGqIc24DLjKzauCdwH2Djn8FuB64EFgNbDKz5YPOuS/8Ks8s/zBUYdx9jbs/6e697r4B+P/CZwK8B1jr7j9x917gRrJaUu6+3t1XunuXu+8AbsjKO5TH3f3n7p4Cvg9kAkYKKAbmmVkytMj+tN+rDPQ2oN7dr3H3bnf/M/Cf9AdogDXufre794QylgBLwlIBXBfyPgr8FLggBK6PAJ90903unnL337p7V9Z1v+DuHe7+B6IAl6lPD3CkmU119zZ3f/IA6yITkAKMHEqWhV/ppwB/RdRyGMDdHwfqgX8DfuruHYOOp9z9m+5+MlFr4Frglqxf0JnPmZK1/OdQhTGzo8zsp2HSwV6iFkOmTDOJAmHmcx1oyso7zczuCF1be4EfDFWfLNndfO1AiZkl3H098Cmi1s/2cM2Zw1wn25uBmdnBlKhbrSHrnOw6pEMdZmbqF9Iy/kLUEppKFIiGC3SD65OZsHExcBTwkpk9Y2bvO8C6yASkACOHHHf/NbCCqBtmKD8A/pnXd48Nvk6Hu38T2AXMG0VRvg28BMwNXTqfI+r6AtgCNGZODF1xjVl5vwI4MD/k/VBW3jfE3X/k7u8gChhO1EIjbA9nI/DqoGBa6e7vyTpndlYdYqEOm8MyO6RlvAnYBOwk6q48YhR1WefuFxB1u10P3G1m5W/0OjIxKMDIoeo/gDPMbPBAP0TdUWcAjw0+YGafMrNTwiB0InSPVfL6mWQHohLYC7SZ2V8Bl2Yd+xlwnJktC7PZPgZMH5S3DdhtZrOAfxnF52NmR5vZaWZWTPSl3kHUbQawDZgzKAhkexrYGyYJlJpZ3MyONbO3ZZ1zgpn9bajDp4AuonGsp4B9wGfDmMwpwNnAHaFVcwtwQ5hEEDezt4cyjlSfD5lZfbjG7pCcGi6PTFwKMHJICuMWtwFXDnGsxd0f8aFfdtQB/DtRF81Ooi/+D4Txh4z/soH3wdy7n2J8BvifQCvR2MWdWWXYSTRF+qtAM1ELaTXRFzTAF4C3AnuIgtFPDqjir1cMXBfqspXol//nwrEfh3Wzmf1ucMYwnnM20Wy8V8M1vgtUZ512P3AeUSvv74C/dfced+8mmnjw7pDvW8BF7v5SyPcZ4HngGaCFqDVyIN83ZwFrzayNaMD/fHfvPIB8MgGZXjgmknuhFdEEXOjuv8p3eQ6EmV0NHOnuH8p3WeTQpBaMSI6Y2ZlmNiV0DWXGZzQrSiYNBRiR3Hk70UyqnURdUcsGz2oTKWTqIhMRkZxQC0ZERHJCj8wOpk6d6nPmzMl3MUREDilr1qzZ6e71Qx1TgAnmzJnD6tWr810MEZFDipn9ZX/H1EUmIiI5oQAjIiI5oQAjIiI5oTEYESkoPT09NDU10dmpJ8yMpZKSEhobG0kmkwecRwFGRApKU1MTlZWVzJkzh4N7n5xkuDvNzc00NTVx2GGHHXA+dZGJSEHp7Oykrq5OwWUMmRl1dXVvuFWoACMiBUfBZeyN5m+a0wBjZhvM7Hkze9bMVoe0WjNbaWbrwrom6/wrzGy9mb2c/V5wMzshXGe9md2YeY+6mRWb2Z0h/Skzm5OVZ3n4jHVDvBJ37HTshlXXw6Y1OfsIEZFD0Xi0YE519wXuvijsXw484u5zgUfCPmY2j+hd4McQvRPiW2YWD3m+DVwCzA3LWSH9YmCXux8JfJ3wJj8zqyV6X/uJwGLgquxANuZWfRk2/HfOLi8ih47m5mYWLFjAggULmD59OrNmzerb7+7uHjbv6tWr+cQnPjFOJc29fAzyn0P0TnWAW4FVwGUh/Q537wJeNbP1wGIz2wBUufsTAGZ2G7AMeDDkuTpc627gptC6ORNY6e4tIc9KoqB0+5jXpqQaiiph76Yxv7SIHHrq6up49tlnAbj66qupqKjgM5/5TN/x3t5eEomhv3oXLVrEokWLhjx2MFKpFPF4fL/7B5rvjcp1C8aBh8xsjZldEtIa3H0LQFhPC+mziN4RntEU0maF7cHpA/K4ey/R2wHrhrnWAGZ2iZmtNrPVO3bsGF0NzaB6FuxpGvlcEZmUPvzhD/PpT3+aU089lcsuu4ynn36ak046iYULF3LSSSfx8ssvA7Bq1Sre9773AVFw+shHPsIpp5zC4Ycfzo033jjktR966CHe/va389a3vpVzzz2XtrY2IHr81TXXXMM73vEOfvzjH79u//bbb+e4447j2GOP5bLLLuu7XkVFBZ///Oc58cQTeeKJJw6q3rluwZzs7pvNbBqw0sxeGubcoUaQfJj00ebpT3C/GbgZYNGiRaN/b0F1I+zZOPJ5IjKuvvBfa3lh894xvea8mVVcdfYxbzjfK6+8wsMPP0w8Hmfv3r089thjJBIJHn74YT73uc9xzz33vC7PSy+9xK9+9StaW1s5+uijufTSSwfch7Jz506+9KUv8fDDD1NeXs7111/PDTfcwOc//3kgunfl8ccfB+Dyyy/v29+8eTNLlixhzZo11NTUsHTpUu677z6WLVvGvn37OPbYY7nmmmtG+Rfql9MA4+6bw3p7eK/5YmCbmc1w9y1mNgPYHk5vAmZnZW8ENof0xiHSs/M0mVmC6F3iLSH9lEF5Vo1dzQapboTNz+bs8iJy6Dv33HP7upv27NnD8uXLWbduHWZGT0/PkHne+973UlxcTHFxMdOmTWPbtm00NvZ/HT755JO88MILnHzyyQB0d3fz9re/ve/4eeedN+B6mf1nnnmGU045hfr66CHIF154IY899hjLli0jHo/zgQ98YEzqnLMAY2blQMzdW8P2UuAa4AFgOXBdWN8fsjwA/MjMbgBmEg3mP+3uKTNrNbMlwFPARcA3svIsB54APgg86u5uZr8Evpw1sL8UuCJXdaWqEdp3Qk8HJEtz9jEi8saMpqWRK+Xl5X3bV155Jaeeeir33nsvGzZs4JRTThkyT3Fxcd92PB6nt7d3wHF354wzzuD224ceXs7+zOz94V40WVJSclDjLtlyOQbTADxuZn8AngZ+5u6/IAosZ5jZOuCMsI+7rwXuAl4AfgF8zN1T4VqXAt8F1hO9gvbBkP49oC5MCPg0YUZaGNz/IvBMWK7JDPjnRHX4RbF38/DniYgQtWBmzYqGhVesWDHq6yxZsoT//u//Zv369QC0t7fzyiuvjJjvxBNP5Ne//jU7d+4klUpx++238853vnPU5difnLVg3P3PwPFDpDcDp+8nz7XAtUOkrwaOHSK9Ezh3P9e6BbjljZV6lKrD/IE9G6HuiHH5SBE5dH32s59l+fLl3HDDDZx22mmjvk59fT0rVqzgggsuoKurC4AvfelLHHXUUcPmmzFjBl/5ylc49dRTcXfe8573cM4554y6HPtjwzWVJpNFixb5qF841vJnuHEhnPMtWHjh2BZMRN6QF198kbe85S35LkZBGupva2Zrsu5zHECPihkLVZkWjKYqi4hkKMCMhUQxlE+DvQowIiIZCjBjpbpRLRgRkSwKMGNFd/OLiAygADNWqmfDnk2gSRMiIoACzNipmgU9+6BjV75LIiIyIeiVyWOl72bLTVBWm9+yiEjeNDc3c/rp0a1+W7duJR6P9z2S5emnn6aoqGjY/KtWraKoqIiTTjop52XNNQWYsZIJMHuaYPpx+S2LiOTNSI/rH8mqVauoqKgYdYAZ/DqA4V4PMFy+saAusrGSHWBERLKsWbOGd77znZxwwgmceeaZbNmyBYAbb7yRefPmMX/+fM4//3w2bNjAd77zHb7+9a+zYMECfvOb3wy4zr59+/jIRz7C2972NhYuXMj990ePclyxYgXnnnsuZ599NkuXLn3dfktLC8uWLWP+/PksWbKE5557DogC4CWXXMLSpUu56KKLxrzeasGMlfJpEEsqwIhMJA9eDlufH9trTj8O3n3dAZ/u7nz84x/n/vvvp76+njvvvJN//dd/5ZZbbuG6667j1Vdfpbi4mN27dzNlyhQ++tGP7rfVc+2113Laaadxyy23sHv3bhYvXsy73vUuAJ544gmee+45amtrWbFixYD9j3/84yxcuJD77ruPRx99lIsuuqivlbVmzRoef/xxSkvH/kG9CjBjJRaDqpkKMCIyQFdXF3/84x8544wzgOgtkTNmzABg/vz5XHjhhSxbtoxly5aNeK2HHnqIBx54gK997WsAdHZ28tprrwFwxhlnUFvbP/6bvf/444/3vW/mtNNOo7m5mT179gDw/ve/PyfBBRRgxlb1bL06WWQieQMtjVxxd4455pgh3w75s5/9jMcee4wHHniAL37xi6xdu3bEa91zzz0cffTRA9Kfeuqp/T6aP5NvsOjt8q9/pP9Y0hjMQdq6p5O//uqj3Pv7Jt1sKSKvU1xczI4dO/oCTE9PD2vXriWdTrNx40ZOPfVUvvrVr7J7927a2tqorKyktbV1yGudeeaZfOMb3+gLGL///e8PqAx/8zd/ww9/+EMgmkQwdepUqqqqxqB2w1OAOUh1FUVs3dPJy1vbooH+vZshnRo5o4hMCrFYjLvvvpvLLruM448/ngULFvDb3/6WVCrFhz70IY477jgWLlzIP/3TPzFlyhTOPvts7r333iEH+a+88kp6enqYP38+xx57LFdeeeUBleHqq69m9erVzJ8/n8svv5xbb701F1V9HT2uPziYx/Wf+fXHaKwp5XvHPAc/+zT80wv974gRkXGlx/Xnjh7XnwdHNlSwbntbNAYDGocREUEBZkwcNa2Sjbva6SyLZoawZ2N+CyQiMgEowIyBuQ0VuMOfe6ZECRroF8krdf2PvdH8TRVgxsBRDRUAvLTLoLgqeqqyiORFSUkJzc3NCjJjyN1pbm6mpKTkDeXTfTBj4M115STjFo3DVGmqskg+NTY20tTUxI4dO/JdlIJSUlJCY2PjG8qjADMGkvEYh00tZ9221jBVWQFGJF+SySSHHXZYvoshqItszMydVhlmkqkFIyICCjBjZm5DBa+1tNNTMQvam6GnI99FEhHJKwWYMTJ3WiXusM2mRgka6BeRSU4BZoxkZpK9mpmqrHEYEZnkFGDGyJvryknEjBf2VUcJGocRkUlOs8jGSFEimkn2+93JKEEBRkQmObVgxtDchgpe2tEFFQ0KMCIy6SnAjKG50yr5S0s76UpNVRYRUYAZQ5lnkrWWNOiJyiIy6SnAjKGjGioB2BGbFrVg9CwkEZnEFGDG0Jwwk+y13hroaYeOXfkukohI3ijAjKGiRIw5U8t5uUNTlUVEFGDG2NxpFTzXGt10qQAjIpNZzgOMmcXN7Pdm9tOwX2tmK81sXVjXZJ17hZmtN7OXzezMrPQTzOz5cOxGM7OQXmxmd4b0p8xsTlae5eEz1pnZ8lzXM2NuQyVrdpdHOxroF5FJbDxaMJ8EXszavxx4xN3nAo+EfcxsHnA+cAxwFvAtM4uHPN8GLgHmhuWskH4xsMvdjwS+DlwfrlULXAWcCCwGrsoOZLk0d1oFO7wKjyX16mQRmdRyGmDMrBF4L/DdrORzgFvD9q3Asqz0O9y9y91fBdYDi81sBlDl7k949Iq62wblyVzrbuD00Lo5E1jp7i3uvgtYSX9Qyqm5DRU4MdpLpuuBlyIyqeW6BfMfwGeBdFZag7tvAQjraSF9FpD9k78ppM0K24PTB+Rx915gD1A3zLUGMLNLzGy1ma0eq7ffHTa1nHjMaE5M0xiMiExqOQswZvY+YLu7rznQLEOk+TDpo83Tn+B+s7svcvdF9fX1B1jM4RUn4ry5roxN6VqNwYjIpJbLFszJwPvNbANwB3Camf0A2Ba6vQjr7eH8JmB2Vv5GYHNIbxwifUAeM0sA1UDLMNcaF0dNq+RPXVNg72ZI9Y7Xx4qITCg5CzDufoW7N7r7HKLB+0fd/UPAA0BmVtdy4P6w/QBwfpgZdhjRYP7ToRut1cyWhPGViwblyVzrg+EzHPglsNTMasLg/tKQNi7mNlTwYnsVeArato7Xx4qITCj5eFz/dcBdZnYx8BpwLoC7rzWzu4AXgF7gY+6eCnkuBVYApcCDYQH4HvB9M1tP1HI5P1yrxcy+CDwTzrvG3VtyXbGMuQ2V/MTrop09m6C6cfgMIiIFaFwCjLuvAlaF7Wbg9P2cdy1w7RDpq4Fjh0jvJASoIY7dAtwy2jIfjLnTKtjcF2A2Es2WFhGZXHQnfw4cXl/OVkKA0UC/iExSCjA5UJyIM7Wuno5YuaYqi8ikpQCTI3MbKqJWzN5xm7wmIjKhKMDkyNxplTT1VpNWgBGRSUoBJkfmNlSwNV1Das+WfBdFRCQvFGByZO60SrZRQ7x9O6RTI2cQESkwCjA5cnh9OTu8hpinYN/OfBdHRGTcKcDkSEkyTm/F9GinVeMwIjL5KMDkkFfMiDZa9bgYEZl8FGByKDllZrTRqoF+EZl8FGByqLRuBmk3fI+6yERk8lGAyaHp1RXspJqu3QowIjL5KMDk0PTqErZ6DT279DwyEZl8FGByqKGqhG1eozEYEZmUFGByaEZ1Kdu8hmT79pFPFhEpMAowOTS1oojt1FDSswt6u/JdHBGRcaUAk0OJeIz24oZoR/fCiMgkowCTY70VmQCjcRgRmVwUYHIsXpm5m18BRkQmFwWYHCuqbYw21EUmIpOMAkyOVddOo8uTdOteGBGZZBRgcmx6dSnbfApdLQowIjK5KMDkWENVCduoIb1XAUZEJhcFmBybUR3dzR9r25bvooiIjCsFmBybXl3CNq+luFN384vI5KIAk2MlyTh7k3UUpdqhc2++iyMiMm4UYMZBV6nu5heRyUcBZjz03Wyp98KIyOShADMOEtWZAKMWjIhMHgow46Ak3M3fq1cni8gkogAzDqbW1rLXS+lobsp3UURExo0CzDhoCFOVe3frZksRmTwUYMZBdLPlFD1RWUQmFQWYcTA9PC4m0a67+UVk8lCAGQfVpUmarY6yrp2QTue7OCIi4yJnAcbMSszsaTP7g5mtNbMvhPRaM1tpZuvCuiYrzxVmtt7MXjazM7PSTzCz58OxG83MQnqxmd0Z0p8yszlZeZaHz1hnZstzVc8DYWZ0lNQT915ob85nUURExs2wAcbMTsvaPmzQsb8d4dpdwGnufjywADjLzJYAlwOPuPtc4JGwj5nNA84HjgHOAr5lZvFwrW8DlwBzw3JWSL8Y2OXuRwJfB64P16oFrgJOBBYDV2UHsnzoLZ8ebWgcRkQmiZFaMF/L2r5n0LF/Gy6jR9rCbjIsDpwD3BrSbwWWhe1zgDvcvcvdXwXWA4vNbAZQ5e5PuLsDtw3Kk7nW3cDpoXVzJrDS3VvcfRewkv6glBexKr06WUQml5ECjO1ne6j912c2i5vZs8B2oi/8p4AGd98CENbTwumzgI1Z2ZtC2qywPTh9QB537wX2AHXDXGtw+S4xs9VmtnrHjh0jVeegFNVEH5/eqwAjIpPDSAHG97M91P7rM7un3H0B0EjUGjl2mNOHClg+TPpo82SX72Z3X+Tui+rr64cp2sGrqIsCTEeLbrYUkckhMcLxw83sAaIv7Mw2Yf+w/WcbyN13m9kqom6qbWY2w923hO6vzItSmoDZWdkagc0hvXGI9Ow8TWaWAKqBlpB+yqA8qw60vLnQUFPJDq8i3tJEeT4LIiIyTkZqwZwD/DvRWExmO7O/bJh8mFm9mU0J26XAu4CXgAeAzKyu5cD9YfsB4PwwM+wwosH8p0M3WquZLQnjKxcNypO51geBR8M4zS+BpWZWEwb3l4a0vJleXco2ryW9R11kIjI5DNuCcfdfZ++bWRI4Ftjk7iO9onEGcGuYCRYD7nL3n5rZE8BdZnYx8BpwbvistWZ2F/AC0At8zN1T4VqXAiuAUuDBsAB8D/i+ma0narmcH67VYmZfBJ4J513j7i0jlDenpleVsNZrmNmmJyqLyOQwbIAxs+8A3whf/tXAE0AKqDWzz7j77fvL6+7PAQuHSG8GTt9PnmuBa4dIX00U2AandxIC1BDHbgFu2V/5xlt9ZTE7qKG48y/5LoqIyLgYqYvsr919bdj+e+AVdz8OOAH4bE5LVmDiMaO1qJ7ynhbo7c53cUREcm6kAJP9TXgGcB+Au6ufZxT6Xp3cpmeSiUjhGynA7Daz95nZQuBk4BcAYcZWaa4LV3AqMnfzKz6LSOEbaZry/wZuBKYDn8pquZwO/CyXBStEiSkzYSvQqjdbikjhG2kW2SsM8YgVd/8leZ72eygqqYtu8+natYniPJdFRCTXRppFduNwx939E2NbnMI2pa6Bbo/TvrNJAUZECt5IXWQfBf4I3EV09/yIzx+T/Zs+pYzt1FCkVyeLyCQwUoCZQXSfyXlENz/eCdwTnlAsb9D0qhK2eQ2NGuQXkUlg2Flk7t7s7t9x91OBDwNTgLVm9nfjUbhCM706CjDJdgUYESl8I7VgADCztwIXEN0L8yCwJpeFKlQlyTi743WUda4d+WQRkUPcSIP8XwDeB7wI3AFcEd67IqPUUTKNks590NUGxRX5Lo6ISM6M1IK5EvgzcHxYvhw90Bgjemnl/NwWr/Ckyhugk+hmy+Ij810cEZGcGSnAHPA7X+TAWNVMaCZ6dfJUBRgRKVwj3Wg55KN/wyP4zwf0aOA3qKhmFrwKvbs3HdgAmIjIIWrYWWRmVmVmV5jZTWa21CIfJ+o2+x/jU8TCUj41upu/rVmvThaRwjbSj+jvA7uI3gPzv4B/AYqAc9z92RyXrSBNraujzUvoalGAEZHCNlKAOTy8/wUz+y6wE3iTu7fmvGQFKnMvTKlenSwiBW6kx/X3ZDbC64tfVXA5ODOqStnmNcT06mQRKXAjtWCON7O9YduA0rCfmaZcldPSFaCq0gTbbSrHtr+Y76KIiOTUSLPI4uNVkMnCzNhe/Caqeh6DrlYorsx3kUREcmKkLjLJgdaKcHvRznX5LYiISA4pwORBT024wXLnK/ktiIhIDinA5EHJtLn0eJzU9pfzXRQRkZxRgMmDmXVVvObT6Nr6Ur6LIiKSMwowedBYW8qffKa6yESkoCnA5MHsmjLW+yyK926AVM+I54uIHIoUYPJgRnUJrzKTuPfCLj0vVEQKkwJMHiTiMfaUZ6Yqa6BfRAqTAkyeaKqyiBQ6BZg8mVpXzw5qdLOliBQsBZg8mV1bxiupmaS3a6qyiBQmBZg8aayJpir7znXgnu/iiIiMOQWYPJldW8affCbx7r3Qtj3fxRERGXMKMHkS3QszM9rRTDIRKUA5CzBmNtvMfmVmL5rZWjP7ZEivNbOVZrYurGuy8lxhZuvN7GUzOzMr/QQzez4cu9HMLKQXm9mdIf0pM5uTlWd5+Ix1ZrY8V/UcrWmVxbxmjdGOZpKJSAHKZQumF/hnd38LsAT4mJnNAy4HHnH3ucAjYZ9w7HzgGOAs4FtmlnkfzbeBS4C5YTkrpF8M7HL3I4GvA9eHa9UCVwEnAouBq7ID2UQQixmJKbPotFLNJBORgpSzAOPuW9z9d2G7FXgRmAWcA9waTrsVWBa2zwHucPcud38VWA8sNrMZQJW7P+HuDtw2KE/mWncDp4fWzZnASndvcfddwEr6g9KE0VhbxsZ4I+xQF5mIFJ5xGYMJXVcLgaeABnffAlEQAqaF02YBG7OyNYW0WWF7cPqAPO7eC+wB6oa51uByXWJmq81s9Y4dO0ZfwVFqrCnjldQMtWBEpCDlPMCYWQVwD/Apd9873KlDpPkw6aPN05/gfrO7L3L3RfX19cMULTdm15byQvd02NsEXW3j/vkiIrmU0wBjZkmi4PJDd/9JSN4Wur0I68wc3SZgdlb2RmBzSG8cIn1AHjNLANVAyzDXmlAas2eSNasVIyKFJZezyAz4HvCiu9+QdegBIDOrazlwf1b6+WFm2GFEg/lPh260VjNbEq550aA8mWt9EHg0jNP8ElhqZjVhcH9pSJtQZoebLQF1k4lIwUnk8NonA38HPG9mz4a0zwHXAXeZ2cXAa/E096EAABVnSURBVMC5AO6+1szuAl4gmoH2MXdPhXyXAiuAUuDBsEAUwL5vZuuJWi7nh2u1mNkXgWfCede4e0uuKjpas2vL+ItPJ21xYpqqLCIFJmcBxt0fZ+ixEIDT95PnWuDaIdJXA8cOkd5JCFBDHLsFuOVAy5sPdeVFJJLF7CqaRZ1mkolIgdGd/HlkZjTWlEZTldVFJiIFRgEmz2bXlrEuNQNa/gSp3nwXR0RkzCjA5FljTSl/6GqAVDfs1uuTRaRwKMDk2eyaMtZ2NUQ7GugXkQKiAJNnjQOmKivAiEjhUIDJs9m1ZeylnM6SetihACMihUMBJs9m15QB0FLyZrVgRKSgKMDkWVVpgsriBE2J2VGA0euTRaRAKMDkmZnRWFvG+vRM6NwN+8b/qc4iIrmgADMBzK4p5fnO8NYCdZOJSIFQgJkAGmvKeLJ1arSjACMiBUIBZgKYXVvKhp5qPFmmmWQiUjAUYCaAxpoynBgdVYerBSMiBUMBZgKYXVsKQEvZYXropYgUDAWYCaAx3AuzKTEb9rwG+3bmuUQiIgdPAWYCqChOUFOW5Imit0cJa/7//BZIRGQMKMBMELNry1jT3gBHnAZPfxd6u/NdJBGRg6IAM0HMrimjaVcHnHgptG2FF+7Pd5FERA6KAswE0VhTyqZdHaSPOB3qjoSnvp3vIomIHBQFmAmisbaM7lSa7W09cOJHYdMa2PhMvoslIjJqCjATxOyaaKpy0652OP4CKK6GJ7+V51KJiIyeAswEkZmqvHFXOxRXwFv/LhqH2bMpzyUTERkdBZgJojG0YDa2dEQJiy8BHJ75bv4KJSJyEBRgJoiSZJz6yuKoiwyg5s1w9HtgzQrobs9r2URERkMBZgKZXVPa34IBWPKP0NECz9+Vv0KJiIySAswEMru2LBqDyXjzSTD9OHjyO3rTpYgcchRgJpDGmlK27OmkJ5WOEsyiGy93vAiv/jq/hRMReYMUYCaQY2dWk0o7qzfsykr8AJRNjVoxIiKHEAWYCeRvjqqnKBHjoRe29icmS+BtF8Mrv4Dn7lJXmYgcMhRgJpDy4gR/feRUHlq7Dc8OJIv/N8yYDz/5B/jeGbDx6fwVUkTkACnATDBnzGtg0+4OXtzS2p9YXgf/8Cs455uwe2MUZH7897DrL/krqIjICBRgJpjT39KAGQO7yQBicVj4Ifj4GnjnZfDyg3DT2+Dhq2HrH3WvjIhMOObq0wdg0aJFvnr16nwXA4APfvu3tHen+Pkn/3r/J+1pgkeugefu7E+raoS6I2Dq3OiJzDVzoHo2VDdCSXU0K01EZAyZ2Rp3XzTUscR4F0ZGtvSYBr7885fY2NLO7NqyoU+qboS/vTlqzWz5AzT/CZrXR8vzP4bOPQPPL6qM8lQ3QuV0KJ0SBZ2SKVBa07+ubIDyaZAoyn1FRaSgKcBMQGfMm86Xf/4SK1/YxkfecdjwJ9cdES3Z3KG9ORqj2bMxau1kr7c+D527obdz/9ctq4PKGVDREC2J4qibLpaIFotF6+JKKKuF0tqwrom2k6WhxWQD1xaPrqXWlEjBy1mAMbNbgPcB29392JBWC9wJzAE2AP/D3XeFY1cAFwMp4BPu/suQfgKwAigFfg580t3dzIqB24ATgGbgPHffEPIsB/4tFOVL7n5rruqZC4dNLeeohgoeemHryAFmKGZQPjVaGk/Y/3k9nVFLp3N3tG5vhrZt0Lo1Wtq2QesW2PEypLoh3QvpFHgq2k71RNtvuHwxSJZDURkUlUfbydIogFls4BJLQElVf+AqrekPZImSrDzx/u14MjqWKB64jhdlTfP2/m2zKK+IjKlctmBWADcRBYGMy4FH3P06M7s87F9mZvOA84FjgJnAw2Z2lLungG8DlwBPEgWYs4AHiYLRLnc/0szOB64HzgtB7CpgEeDAGjN7IBPIDhVL503nW6vWs2tfNzXlOequSpZES2XD6K/R3R49L629JVp37Iq2ezvDF7j3ryEKSj3tUb6efWEdFk9H56ZT4D3RfroXWv4cXbdjV/91xlqyPOoyzO46LKmKPr+nI5Qxa50ojWb3lddHN8JmthOl/fXq3te/ne6BogooropafSVhXVwZ5UmWZK1DUOxqhbYdsG87tG2HfTuiBYtamGW1YR22i6ui4BpLDGxtxpIQV2eFjL+c/atz98fMbM6g5HOAU8L2rcAq4LKQfoe7dwGvmtl6YLGZbQCq3P0JADO7DVhGFGDOAa4O17obuMnMDDgTWOnuLSHPSqKgdPtY1zGXzpjXwE2/Ws+jL23nAyc05rs4+1dUFi3V41DGdDpqbWWCTW9nCEKZVlU62k91R0tvZ1i6wrq7v7sO+rc9FX2Zd+zub83tbYLte6Iv52RZ1MIqKou+zBMl0fX27YRdq6OWX9feocscL47yxZLQ3RYFqINRVBmVe3+ftz+J0hDUqvrXxRVRABrQjRmLtouroGJaaAlP698uq4sC5UgtPvdQ344oWCbLcxPk0uG/XebHiaf7F4tFQT+mybL5Mt4/axrcfQuAu28xs2khfRZRCyWjKaT1hO3B6Zk8G8O1es1sD1CXnT5EngHM7BKi1hFvetObRl+rHDhuVjXTq0p46IWtEzvAjKdYLPxqr813SV6vpzMKNL2dUUAqKhv6SzXVGwWHrtb+pae9Pxj2dEJvRxQUiyujL/fyeqioj9bJ6L1B9HaHlmNz/9K5NwTa0IWZWXq7w2fujc7pao22W7dE52a3MjNfzp17o2C7P0UVoUUWWmHxJHS19X9Gd/jSzxYvCsE6/H3iRf2trHgyCsKxeH8LLlkabSdLo/1Ub9Sa27cjtOx2QPvO139OtkRpGKc8MlqmzoXaI6LrpXui+qd6wt+qJ/qR0bYd2rZG69aw7u0Iwbl64FJU0f/3yw5uno6um+oKP3bCj55UT/RvIvOjpW9dGoL9UCzrbzTob5Xq6f9BldlO9/T/rTN/v8F/y0TJwO7jZGnUXT3GJkq7eagRXx8mfbR5Bia63wzcDNE05ZGLOX5iMeOMeQ38eM1GOrpTlBZpjGBCS5ZA9ZC/YwaKJ8YmSCaKotmAldMP7jrD6e3u75bbtyP6ou3cnRUcswJlqjsKgIO7AJNlUbDsyXQZtvd3i/aN6/X2f8n3dESfkR1oM9uxRAi206L3JTUuirZLpvRPPDHrH79L90LLq9C8DrY+By/+14GPGcaLwgSXadF0/2RJCLp7orHJzj3R0tcitYFjh2bRNeJF0Zd4PBm1ZuPJUM9Ml2tHf/dwPs06Af7h0TG/7HgHmG1mNiO0XmYA20N6EzA767xGYHNIbxwiPTtPk5klgGqgJaSfMijPqrGtxvhYekwD33/yLzy+fidnzDuIcRKR0UgURUHzQALnoaC3G3a9Go3ppXsHjk9ltkuqosBSWnNgMx3dD35GpHsUSPcX/DJjkanQyuprcaVC4AqBLLMdS/R3Efe09wfons6sbuPsruOuqL45MN4B5gFgOXBdWN+flf4jM7uBaJB/LvC0u6fMrNXMlgBPARcB3xh0rSeADwKPhtllvwS+bGaZv9hS4IrcV23snXhYHZUlCR5au1UBRuRgJYqg/uhoGStjMd3eLGohjaVEUTTGlme5nKZ8O1FLYqqZNRHN7LoOuMvMLgZeA84FcPe1ZnYX8ALQC3wszCADuJT+acoPhgXge8D3w4SAFqJZaLh7i5l9EXgmnHdNZsD/UFOUiHHaX03j4Re30ZtKk4hrsFJEDh16VEwwkR4Vk+1nz23hYz/6HXdesoQTD6/Ld3FERAYY7lEx+kk8wb3z6HqK4jEeemFbvosiIvKGKMBMcBXFCU46so6VLwx6R4yIyASnAHMIWDpvOq+1tHPzY3+muzfP0xlFRA6QAswh4P0LZvKOI6fylQdf4vQbVnH/s5tIp9WaEZGJTQHmEFBRnOD7Fy/m1o8spqI4ySfveJazb3qc36zbke+iiYjsl2aRBRN1Ftlg6bTzwB8287WHXqZpVwfvOHIq750/gyPqKziivpza8iJMj8IXkXGiF44VkFjMWLZwFu8+bjo/fPI1vvmr9Ty+fmff8SllSY6sr+CI+greVFdGY00pjTWlzJpSxrTKYmIxBR8RGR9qwQSHSgtmsHTa2bS7gz/taONPO/axfntbtL29jeZ93QPOTcaNGdWlzJxSwvSqEhqylunVxUyrLGFqRbGeeyYiB0wtmAIWixmza8uYXVvGKYOegLGvq5dNuzvYtKuDprDetLuDzbs7WP2XXWzf20V36vWz0sqK4tRVFFFXXszUimLqyouYUp6kpqyIKaVJppQlmVJWxJSyJFUlSSpLEpQXJdQ6EpEBFGAKWHlxgqMaKjmqoXLI4+7O7vYetu7tZNveTrbv7aJ5Xzc727pobou2N+3u4Lmm3exu7xkyGGWYRZMRMgGnojhBWXGC8qI4ZUUJyovjlBcnKEvGKS2KU5yMUxqWkmSMkrAuTsQpToR1MkZxIkZR2I8rgIkcUhRgJjEzo6a8iJryIt4yo2rYc92djp4Uu9t72NXezZ72Hna199Da2UNrZy+tnT3s7ezt297X3cvejh627ulgX1eK9u5e9nWnDuo+nnjMKIpHAacoEevbTsaNZDxGMh6lJeJGIh4jEbNoiRuJWLQfC2mxmBE3Ix7rX2JmxGMQt+h4zAamxywrrS9/9HeMZ6UnsvJm8iVioVyxrLJl9vuORetkZh2P6qZJG3KoUoCRA2JmlBUlKCtKMHNK6aiv05tK09mbpqM7RWdPtHT0pOjoTtHVmw5Liq6eNJ1h3Z1K090bLV29UZCK0pyeVLpv6U45PeG89u4Uvek0vSmnN+2k0tG56bST8mg/lY6OZdLSafqOTSTZATQZj4UWXmjZJaMWX0kyTkkiRmlRnLKiOKXJRLQuiketyOKoG7OsOB6ti6IWZeZYWTKuLk4ZcwowMq4S8RgV8RgVxRP7n15f0MkKPNF2CE7Z6enoWKpvDb3p9ICAFQWzKOBlgl0q7fSknd5Umt60h2CYpifVn9bdGwXPzHZ3Kk1PbxSku3pSfeu9HT1s70nR3t0fsNu7e3kjsbI0Gae8OApKZckEJUXxvi7NqCvz9d2Zmf1MkCvOCnqZ4/3dnv1dn0XxmALaJDCx/y8XyZNYzIgN+XLUQ4e70xVai/u6e2nvTrGvK1q3dfX2bQ91vCMrUO1o7erbzrQ6O3vTB93SS8YHdXn2dXvGs1pt/a23RCzaznQnJuMDuz/jocsxntmPWd86ZtG5fV2XNlR36dDdoDEjq8s0as3HMumZ7dC1an3dquFY1rZln591zf0dL4SuUQUYkQJlZqG1EaemvGjMr5/p7swEna7eNF09UTdmZ9a6OxW1sgZ3gWZ3fWZaZ129KXpS2V2fTltXb9SKC+mZ1l5mO9P12Zvu7w4tFJmgY/QHoaHTBgYqyzqeCVaD8/alAfNmVvONCxaOefkVYERkVCZqd6d71viaZ42zZXVvptJRF2dvOt3XrZnp4szu7kx7SE9H25njHtJT7rj3H0ul+49lzkv70Hmzj6fCtg84F8g+Bn15Mudl8rt73/FMug/6/Mw5fXnD+Ti8qXb046rDmVj/MkREDpKF7rCE7hfOOz3sUkREckIBRkREckIBRkREckIBRkREckIBRkREckIBRkREckIBRkREckIBRkREckJvtAzMbAfwl4O4xFRg54hnFR7Ve3JRvSeXA6n3m929fqgDCjBjxMxW7++1oYVM9Z5cVO/J5WDrrS4yERHJCQUYERHJCQWYsXNzvguQJ6r35KJ6Ty4HVW+NwYiISE6oBSMiIjmhACMiIjmhAHOQzOwsM3vZzNab2eX5Lk8umdktZrbdzP6YlVZrZivNbF1Y1+SzjGPNzGab2a/M7EUzW2tmnwzphV7vEjN72sz+EOr9hZBe0PXOMLO4mf3ezH4a9idLvTeY2fNm9qyZrQ5po667AsxBMLM48E3g3cA84AIzm5ffUuXUCuCsQWmXA4+4+1zgkbBfSHqBf3b3twBLgI+F/8aFXu8u4DR3Px5YAJxlZkso/HpnfBJ4MWt/stQb4FR3X5B1/8uo664Ac3AWA+vd/c/u3g3cAZyT5zLljLs/BrQMSj4HuDVs3wosG9dC5Zi7b3H334XtVqIvnVkUfr3d3dvCbjIsToHXG8DMGoH3At/NSi74eg9j1HVXgDk4s4CNWftNIW0yaXD3LRB9GQPT8lyenDGzOcBC4CkmQb1DN9GzwHZgpbtPinoD/wF8FkhnpU2GekP0I+IhM1tjZpeEtFHXPZGDAk4mNkSa5n0XIDOrAO4BPuXue82G+k9fWNw9BSwwsynAvWZ2bL7LlGtm9j5gu7uvMbNT8l2ePDjZ3Teb2TRgpZm9dDAXUwvm4DQBs7P2G4HNeSpLvmwzsxkAYb09z+UZc2aWJAouP3T3n4Tkgq93hrvvBlYRjb8Ver1PBt5vZhuIurxPM7MfUPj1BsDdN4f1duBeomGAUdddAebgPAPMNbPDzKwIOB94IM9lGm8PAMvD9nLg/jyWZcxZ1FT5HvCiu9+QdajQ610fWi6YWSnwLuAlCrze7n6Fuze6+xyi/58fdfcPUeD1BjCzcjOrzGwDS4E/chB11538B8nM3kPUZxsHbnH3a/NcpJwxs9uBU4ge4b0NuAq4D7gLeBPwGnCuuw+eCHDIMrN3AL8Bnqe/T/5zROMwhVzv+UQDunGiH6J3ufs1ZlZHAdc7W+gi+4y7v28y1NvMDidqtUA0fPIjd7/2YOquACMiIjmhLjIREckJBRgREckJBRgREckJBRgREckJBRgREckJBRiRcWRmqfCk2swyZg9NNLM52U+6Fsk3PSpGZHx1uPuCfBdCZDyoBSMyAYT3cFwf3sHytJkdGdLfbGaPmNlzYf2mkN5gZveG97X8wcxOCpeKm9l/hne4PBTuwhfJCwUYkfFVOqiL7LysY3vdfTFwE9HTIQjbt7n7fOCHwI0h/Ubg1+F9LW8F1ob0ucA33f0YYDfwgRzXR2S/dCe/yDgyszZ3rxgifQPRC77+HB6uudXd68xsJzDD3XtC+hZ3n2pmO4BGd+/KusYcosfqzw37lwFJd/9S7msm8npqwYhMHL6f7f2dM5SurO0UGmeVPFKAEZk4zstaPxG2f0v0VF+AC4HHw/YjwKXQ92KwqvEqpMiB0q8bkfFVGt4SmfELd89MVS42s6eIfvhdENI+AdxiZv8C7AD+PqR/ErjZzC4maqlcCmzJeelF3gCNwYhMAGEMZpG778x3WUTGirrIREQkJ9SCERGRnFALRkREckIBRkREckIBRkREckIBRkREckIBRkREcuL/Adc2b1h+oVgsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sqrt(history.history['loss']))\n",
    "plt.plot(np.sqrt(history.history['val_loss']))\n",
    "plt.title('RMSE against epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train error', 'Test error'], loc='upper right')\n",
    "\n",
    "#plt.savefig('Part 1b Q2(d) rsme.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBARkj0gDPLX"
   },
   "source": [
    "### Part e\n",
    "\n",
    "Using the model from the best epoch, report the test R^2 value and show the top 30 test samples with the largest errors. List down any trends you find in\n",
    "these samples and suggest ways to reduce these errors.\n",
    "\n",
    "(Tip: Add the prediction error as a column in the DataFrame and sort by it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huGXlxE5DPbx",
    "outputId": "8c1c858f-de68-4e3e-f88b-c937577365b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 1s 4ms/step - loss: 10487806976.0000 - r2: 0.6153 - accuracy: 0.0000e+00\n",
      "R Squared Error: 0.6152568459510803\n"
     ]
    }
   ],
   "source": [
    "find_r2 = best_model.evaluate(test_ds)\n",
    "\n",
    "print(\"R Squared Error: \" + str(find_r2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTIhnwst1laf",
    "outputId": "db407b95-ae03-44f3-b2ff-d5c27bf33085"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['year', 'full_address', 'nearest_stn'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 1s 4ms/step\n",
      "[[392939.84]\n",
      " [543758.06]\n",
      " [323566.44]\n",
      " ...\n",
      " [353213.4 ]\n",
      " [222754.78]\n",
      " [489824.94]]\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(test_ds)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpZ6VtUwyy_R",
    "outputId": "45040416-8725-456c-a853-c3e72bec754c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-7ffd4fd45c45>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dataframe[\"predicted\"] = predictions\n"
     ]
    }
   ],
   "source": [
    "# append to dataframe\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "test_dataframe[\"predicted\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6zO7tn5yzHZ",
    "outputId": "869dd395-15e3-422b-a710-7f686b03b4d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-987e31f29704>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dataframe[\"error\"] = abs(test_dataframe[\"predicted\"] - test_dataframe[\"resale_price\"])\n",
      "<ipython-input-45-987e31f29704>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dataframe.sort_values(by=['error'], inplace=True, ascending=False)\n"
     ]
    }
   ],
   "source": [
    "# get error \n",
    "test_dataframe[\"error\"] = abs(test_dataframe[\"predicted\"] - test_dataframe[\"resale_price\"])\n",
    "\n",
    "# sort by error\n",
    "test_dataframe.sort_values(by=['error'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8zWtRGdyzNL",
    "outputId": "e8ac6092-4d5d-4940-fc8c-042c504c3427"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "      <th>predicted</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127225</th>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>92 DAWSON ROAD</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>0.585</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5 ROOM, Premium Apartment Loft</td>\n",
       "      <td>92.833</td>\n",
       "      <td>122.000</td>\n",
       "      <td>37 TO 39</td>\n",
       "      <td>1418000.000</td>\n",
       "      <td>334620.219</td>\n",
       "      <td>1083379.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106700</th>\n",
       "      <td>9</td>\n",
       "      <td>2021</td>\n",
       "      <td>273A BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.767</td>\n",
       "      <td>6.328</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>89.000</td>\n",
       "      <td>120.000</td>\n",
       "      <td>37 TO 39</td>\n",
       "      <td>1295000.000</td>\n",
       "      <td>222444.297</td>\n",
       "      <td>1072555.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120285</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>1B CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>5 ROOM, Type S2</td>\n",
       "      <td>87.917</td>\n",
       "      <td>107.000</td>\n",
       "      <td>46 TO 48</td>\n",
       "      <td>1308000.000</td>\n",
       "      <td>270878.312</td>\n",
       "      <td>1037121.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115521</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>92 DAWSON ROAD</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>0.585</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5 ROOM, Premium Apartment Loft</td>\n",
       "      <td>93.333</td>\n",
       "      <td>122.000</td>\n",
       "      <td>40 TO 42</td>\n",
       "      <td>1328000.000</td>\n",
       "      <td>292705.906</td>\n",
       "      <td>1035294.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103978</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>275A BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.828</td>\n",
       "      <td>6.370</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.917</td>\n",
       "      <td>120.000</td>\n",
       "      <td>25 TO 27</td>\n",
       "      <td>1280000.000</td>\n",
       "      <td>271589.156</td>\n",
       "      <td>1008410.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131020</th>\n",
       "      <td>8</td>\n",
       "      <td>2022</td>\n",
       "      <td>139B LORONG 1A TOA PAYOH</td>\n",
       "      <td>Caldecott</td>\n",
       "      <td>0.484</td>\n",
       "      <td>4.157</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.750</td>\n",
       "      <td>114.000</td>\n",
       "      <td>31 TO 33</td>\n",
       "      <td>1265000.000</td>\n",
       "      <td>265653.844</td>\n",
       "      <td>999346.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118249</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>273B BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.776</td>\n",
       "      <td>6.297</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.250</td>\n",
       "      <td>120.000</td>\n",
       "      <td>25 TO 27</td>\n",
       "      <td>1270000.000</td>\n",
       "      <td>281356.281</td>\n",
       "      <td>988643.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118240</th>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>273A BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.767</td>\n",
       "      <td>6.328</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.667</td>\n",
       "      <td>120.000</td>\n",
       "      <td>34 TO 36</td>\n",
       "      <td>1338888.000</td>\n",
       "      <td>355337.031</td>\n",
       "      <td>983550.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118246</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>275A BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.828</td>\n",
       "      <td>6.370</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.333</td>\n",
       "      <td>120.000</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1310000.000</td>\n",
       "      <td>329211.594</td>\n",
       "      <td>980788.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119399</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>96A HENDERSON ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.587</td>\n",
       "      <td>2.933</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>96.083</td>\n",
       "      <td>113.000</td>\n",
       "      <td>40 TO 42</td>\n",
       "      <td>1400000.000</td>\n",
       "      <td>419716.594</td>\n",
       "      <td>980283.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120241</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>1F CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.516</td>\n",
       "      <td>2.614</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>4 ROOM, Type S1</td>\n",
       "      <td>87.917</td>\n",
       "      <td>95.000</td>\n",
       "      <td>37 TO 39</td>\n",
       "      <td>1205000.000</td>\n",
       "      <td>231051.234</td>\n",
       "      <td>973948.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120247</th>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1G CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.567</td>\n",
       "      <td>2.662</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>4 ROOM, Type S1</td>\n",
       "      <td>87.833</td>\n",
       "      <td>95.000</td>\n",
       "      <td>40 TO 42</td>\n",
       "      <td>1220000.000</td>\n",
       "      <td>278125.688</td>\n",
       "      <td>941874.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131009</th>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>139A LORONG 1A TOA PAYOH</td>\n",
       "      <td>Caldecott</td>\n",
       "      <td>0.515</td>\n",
       "      <td>4.108</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>89.167</td>\n",
       "      <td>114.000</td>\n",
       "      <td>31 TO 33</td>\n",
       "      <td>1180000.000</td>\n",
       "      <td>242216.672</td>\n",
       "      <td>937783.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98078</th>\n",
       "      <td>5</td>\n",
       "      <td>2021</td>\n",
       "      <td>18C HOLLAND DRIVE</td>\n",
       "      <td>Buona Vista</td>\n",
       "      <td>0.379</td>\n",
       "      <td>6.141</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.007</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>90.417</td>\n",
       "      <td>117.000</td>\n",
       "      <td>34 TO 36</td>\n",
       "      <td>1210000.000</td>\n",
       "      <td>274072.438</td>\n",
       "      <td>935927.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106849</th>\n",
       "      <td>9</td>\n",
       "      <td>2021</td>\n",
       "      <td>96A HENDERSON ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.587</td>\n",
       "      <td>2.933</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>96.750</td>\n",
       "      <td>113.000</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1220000.000</td>\n",
       "      <td>284447.844</td>\n",
       "      <td>935552.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120291</th>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1G CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.567</td>\n",
       "      <td>2.662</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>5 ROOM, Type S2</td>\n",
       "      <td>87.750</td>\n",
       "      <td>107.000</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1228000.000</td>\n",
       "      <td>292932.594</td>\n",
       "      <td>935067.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124906</th>\n",
       "      <td>8</td>\n",
       "      <td>2022</td>\n",
       "      <td>8B UPPER BOON KENG ROAD</td>\n",
       "      <td>Kallang</td>\n",
       "      <td>0.187</td>\n",
       "      <td>3.052</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5 ROOM, Premium Apartment</td>\n",
       "      <td>94.000</td>\n",
       "      <td>116.000</td>\n",
       "      <td>25 TO 27</td>\n",
       "      <td>1230000.000</td>\n",
       "      <td>295343.125</td>\n",
       "      <td>934656.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124828</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>9 BOON KENG ROAD</td>\n",
       "      <td>Bendemeer</td>\n",
       "      <td>0.336</td>\n",
       "      <td>2.536</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1240888.000</td>\n",
       "      <td>306454.156</td>\n",
       "      <td>934433.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114530</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>1B CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>5 ROOM, Type S2</td>\n",
       "      <td>88.083</td>\n",
       "      <td>106.000</td>\n",
       "      <td>43 TO 45</td>\n",
       "      <td>1254000.000</td>\n",
       "      <td>321867.625</td>\n",
       "      <td>932132.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106970</th>\n",
       "      <td>9</td>\n",
       "      <td>2021</td>\n",
       "      <td>1D CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.438</td>\n",
       "      <td>2.507</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>4 ROOM, Type S1</td>\n",
       "      <td>88.333</td>\n",
       "      <td>93.000</td>\n",
       "      <td>46 TO 48</td>\n",
       "      <td>1200000.000</td>\n",
       "      <td>269767.531</td>\n",
       "      <td>930232.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113057</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>50 COMMONWEALTH DRIVE</td>\n",
       "      <td>Commonwealth</td>\n",
       "      <td>0.197</td>\n",
       "      <td>5.422</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>92.333</td>\n",
       "      <td>117.000</td>\n",
       "      <td>34 TO 36</td>\n",
       "      <td>1230000.000</td>\n",
       "      <td>304103.531</td>\n",
       "      <td>925896.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131016</th>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>139A LORONG 1A TOA PAYOH</td>\n",
       "      <td>Caldecott</td>\n",
       "      <td>0.515</td>\n",
       "      <td>4.108</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.917</td>\n",
       "      <td>113.000</td>\n",
       "      <td>16 TO 18</td>\n",
       "      <td>1180000.000</td>\n",
       "      <td>259745.609</td>\n",
       "      <td>920254.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100263</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>150 MEI LING STREET</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>0.245</td>\n",
       "      <td>4.709</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>EXECUTIVE, Apartment</td>\n",
       "      <td>73.417</td>\n",
       "      <td>148.000</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>1235000.000</td>\n",
       "      <td>314968.594</td>\n",
       "      <td>920031.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101491</th>\n",
       "      <td>7</td>\n",
       "      <td>2021</td>\n",
       "      <td>9A BOON TIONG ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.192</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>93.500</td>\n",
       "      <td>112.000</td>\n",
       "      <td>25 TO 27</td>\n",
       "      <td>1200000.000</td>\n",
       "      <td>286458.219</td>\n",
       "      <td>913541.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118250</th>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>273B BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.776</td>\n",
       "      <td>6.297</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.167</td>\n",
       "      <td>120.000</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>1238000.000</td>\n",
       "      <td>326656.312</td>\n",
       "      <td>911343.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110121</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>7 BOON KENG ROAD</td>\n",
       "      <td>Boon Keng</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.638</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.053</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.333</td>\n",
       "      <td>119.000</td>\n",
       "      <td>22 TO 24</td>\n",
       "      <td>1200000.000</td>\n",
       "      <td>297728.250</td>\n",
       "      <td>902271.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120253</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>1B CANTONMENT ROAD</td>\n",
       "      <td>Outram Park</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>4 ROOM, Type S1</td>\n",
       "      <td>87.667</td>\n",
       "      <td>94.000</td>\n",
       "      <td>49 TO 51</td>\n",
       "      <td>1228000.000</td>\n",
       "      <td>327787.250</td>\n",
       "      <td>900212.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118243</th>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>273B BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.776</td>\n",
       "      <td>6.297</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.500</td>\n",
       "      <td>120.000</td>\n",
       "      <td>31 TO 33</td>\n",
       "      <td>1270000.000</td>\n",
       "      <td>375659.531</td>\n",
       "      <td>894340.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102286</th>\n",
       "      <td>7</td>\n",
       "      <td>2021</td>\n",
       "      <td>50 JALAN BAHAGIA</td>\n",
       "      <td>Boon Keng</td>\n",
       "      <td>1.034</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.053</td>\n",
       "      <td>3 ROOM, Terrace</td>\n",
       "      <td>50.083</td>\n",
       "      <td>174.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>1140000.000</td>\n",
       "      <td>246697.734</td>\n",
       "      <td>893302.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113653</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>139B LORONG 1A TOA PAYOH</td>\n",
       "      <td>Caldecott</td>\n",
       "      <td>0.484</td>\n",
       "      <td>4.157</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>89.500</td>\n",
       "      <td>117.000</td>\n",
       "      <td>37 TO 39</td>\n",
       "      <td>1180000.000</td>\n",
       "      <td>289278.406</td>\n",
       "      <td>890721.594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "127225      7  2022            92 DAWSON ROAD    Queenstown   \n",
       "106700      9  2021     273A BISHAN STREET 24        Bishan   \n",
       "120285      2  2022        1B CANTONMENT ROAD   Outram Park   \n",
       "115521     12  2021            92 DAWSON ROAD    Queenstown   \n",
       "103978      8  2021     275A BISHAN STREET 24        Bishan   \n",
       "131020      8  2022  139B LORONG 1A TOA PAYOH     Caldecott   \n",
       "118249      6  2022     273B BISHAN STREET 24        Bishan   \n",
       "118240      1  2022     273A BISHAN STREET 24        Bishan   \n",
       "118246      6  2022     275A BISHAN STREET 24        Bishan   \n",
       "119399      5  2022        96A HENDERSON ROAD   Tiong Bahru   \n",
       "120241      2  2022        1F CANTONMENT ROAD   Outram Park   \n",
       "120247      4  2022        1G CANTONMENT ROAD   Outram Park   \n",
       "131009      3  2022  139A LORONG 1A TOA PAYOH     Caldecott   \n",
       "98078       5  2021         18C HOLLAND DRIVE   Buona Vista   \n",
       "106849      9  2021        96A HENDERSON ROAD   Tiong Bahru   \n",
       "120291      4  2022        1G CANTONMENT ROAD   Outram Park   \n",
       "124906      8  2022   8B UPPER BOON KENG ROAD       Kallang   \n",
       "124828      2  2022          9 BOON KENG ROAD     Bendemeer   \n",
       "114530     12  2021        1B CANTONMENT ROAD   Outram Park   \n",
       "106970      9  2021        1D CANTONMENT ROAD   Outram Park   \n",
       "113057     11  2021     50 COMMONWEALTH DRIVE  Commonwealth   \n",
       "131016      7  2022  139A LORONG 1A TOA PAYOH     Caldecott   \n",
       "100263      6  2021       150 MEI LING STREET    Queenstown   \n",
       "101491      7  2021        9A BOON TIONG ROAD   Tiong Bahru   \n",
       "118250      7  2022     273B BISHAN STREET 24        Bishan   \n",
       "110121     10  2021          7 BOON KENG ROAD     Boon Keng   \n",
       "120253      5  2022        1B CANTONMENT ROAD   Outram Park   \n",
       "118243      3  2022     273B BISHAN STREET 24        Bishan   \n",
       "102286      7  2021          50 JALAN BAHAGIA     Boon Keng   \n",
       "113653     11  2021  139B LORONG 1A TOA PAYOH     Caldecott   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "127225                0.585          3.882              0.017   \n",
       "106700                0.767          6.328              0.034   \n",
       "120285                0.353          2.413              0.034   \n",
       "115521                0.585          3.882              0.017   \n",
       "103978                0.828          6.370              0.034   \n",
       "131020                0.484          4.157              0.017   \n",
       "118249                0.776          6.297              0.034   \n",
       "118240                0.767          6.328              0.034   \n",
       "118246                0.828          6.370              0.034   \n",
       "119399                0.587          2.933              0.017   \n",
       "120241                0.516          2.614              0.034   \n",
       "120247                0.567          2.662              0.034   \n",
       "131009                0.515          4.108              0.017   \n",
       "98078                 0.379          6.141              0.034   \n",
       "106849                0.587          2.933              0.017   \n",
       "120291                0.567          2.662              0.034   \n",
       "124906                0.187          3.052              0.017   \n",
       "124828                0.336          2.536              0.017   \n",
       "114530                0.353          2.413              0.034   \n",
       "106970                0.438          2.507              0.034   \n",
       "113057                0.197          5.422              0.017   \n",
       "131016                0.515          4.108              0.017   \n",
       "100263                0.245          4.709              0.017   \n",
       "101491                0.192          2.345              0.017   \n",
       "118250                0.776          6.297              0.034   \n",
       "110121                0.365          2.638              0.017   \n",
       "120253                0.353          2.413              0.034   \n",
       "118243                0.776          6.297              0.034   \n",
       "102286                1.034          3.360              0.017   \n",
       "113653                0.484          4.157              0.017   \n",
       "\n",
       "        eigenvector_centrality                 flat_model_type  \\\n",
       "127225                   0.008  5 ROOM, Premium Apartment Loft   \n",
       "106700                   0.016                    5 ROOM, DBSS   \n",
       "120285                   0.121                 5 ROOM, Type S2   \n",
       "115521                   0.008  5 ROOM, Premium Apartment Loft   \n",
       "103978                   0.016                    5 ROOM, DBSS   \n",
       "131020                   0.024                    5 ROOM, DBSS   \n",
       "118249                   0.016                    5 ROOM, DBSS   \n",
       "118240                   0.016                    5 ROOM, DBSS   \n",
       "118246                   0.016                    5 ROOM, DBSS   \n",
       "119399                   0.048                5 ROOM, Improved   \n",
       "120241                   0.121                 4 ROOM, Type S1   \n",
       "120247                   0.121                 4 ROOM, Type S1   \n",
       "131009                   0.024                    5 ROOM, DBSS   \n",
       "98078                    0.007                5 ROOM, Improved   \n",
       "106849                   0.048                5 ROOM, Improved   \n",
       "120291                   0.121                 5 ROOM, Type S2   \n",
       "124906                   0.048       5 ROOM, Premium Apartment   \n",
       "124828                   0.004                    5 ROOM, DBSS   \n",
       "114530                   0.121                 5 ROOM, Type S2   \n",
       "106970                   0.121                 4 ROOM, Type S1   \n",
       "113057                   0.005                5 ROOM, Improved   \n",
       "131016                   0.024                    5 ROOM, DBSS   \n",
       "100263                   0.008            EXECUTIVE, Apartment   \n",
       "101491                   0.048                5 ROOM, Improved   \n",
       "118250                   0.016                    5 ROOM, DBSS   \n",
       "110121                   0.053                    5 ROOM, DBSS   \n",
       "120253                   0.121                 4 ROOM, Type S1   \n",
       "118243                   0.016                    5 ROOM, DBSS   \n",
       "102286                   0.053                 3 ROOM, Terrace   \n",
       "113653                   0.024                    5 ROOM, DBSS   \n",
       "\n",
       "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \\\n",
       "127225                 92.833         122.000     37 TO 39   1418000.000   \n",
       "106700                 89.000         120.000     37 TO 39   1295000.000   \n",
       "120285                 87.917         107.000     46 TO 48   1308000.000   \n",
       "115521                 93.333         122.000     40 TO 42   1328000.000   \n",
       "103978                 88.917         120.000     25 TO 27   1280000.000   \n",
       "131020                 88.750         114.000     31 TO 33   1265000.000   \n",
       "118249                 88.250         120.000     25 TO 27   1270000.000   \n",
       "118240                 88.667         120.000     34 TO 36   1338888.000   \n",
       "118246                 88.333         120.000     28 TO 30   1310000.000   \n",
       "119399                 96.083         113.000     40 TO 42   1400000.000   \n",
       "120241                 87.917          95.000     37 TO 39   1205000.000   \n",
       "120247                 87.833          95.000     40 TO 42   1220000.000   \n",
       "131009                 89.167         114.000     31 TO 33   1180000.000   \n",
       "98078                  90.417         117.000     34 TO 36   1210000.000   \n",
       "106849                 96.750         113.000     28 TO 30   1220000.000   \n",
       "120291                 87.750         107.000     28 TO 30   1228000.000   \n",
       "124906                 94.000         116.000     25 TO 27   1230000.000   \n",
       "124828                 88.000         117.000     28 TO 30   1240888.000   \n",
       "114530                 88.083         106.000     43 TO 45   1254000.000   \n",
       "106970                 88.333          93.000     46 TO 48   1200000.000   \n",
       "113057                 92.333         117.000     34 TO 36   1230000.000   \n",
       "131016                 88.917         113.000     16 TO 18   1180000.000   \n",
       "100263                 73.417         148.000     10 TO 12   1235000.000   \n",
       "101491                 93.500         112.000     25 TO 27   1200000.000   \n",
       "118250                 88.167         120.000     07 TO 09   1238000.000   \n",
       "110121                 88.333         119.000     22 TO 24   1200000.000   \n",
       "120253                 87.667          94.000     49 TO 51   1228000.000   \n",
       "118243                 88.500         120.000     31 TO 33   1270000.000   \n",
       "102286                 50.083         174.000     01 TO 03   1140000.000   \n",
       "113653                 89.500         117.000     37 TO 39   1180000.000   \n",
       "\n",
       "        predicted       error  \n",
       "127225 334620.219 1083379.781  \n",
       "106700 222444.297 1072555.703  \n",
       "120285 270878.312 1037121.688  \n",
       "115521 292705.906 1035294.094  \n",
       "103978 271589.156 1008410.844  \n",
       "131020 265653.844  999346.156  \n",
       "118249 281356.281  988643.719  \n",
       "118240 355337.031  983550.969  \n",
       "118246 329211.594  980788.406  \n",
       "119399 419716.594  980283.406  \n",
       "120241 231051.234  973948.766  \n",
       "120247 278125.688  941874.312  \n",
       "131009 242216.672  937783.328  \n",
       "98078  274072.438  935927.562  \n",
       "106849 284447.844  935552.156  \n",
       "120291 292932.594  935067.406  \n",
       "124906 295343.125  934656.875  \n",
       "124828 306454.156  934433.844  \n",
       "114530 321867.625  932132.375  \n",
       "106970 269767.531  930232.469  \n",
       "113057 304103.531  925896.469  \n",
       "131016 259745.609  920254.391  \n",
       "100263 314968.594  920031.406  \n",
       "101491 286458.219  913541.781  \n",
       "118250 326656.312  911343.688  \n",
       "110121 297728.250  902271.750  \n",
       "120253 327787.250  900212.750  \n",
       "118243 375659.531  894340.469  \n",
       "102286 246697.734  893302.266  \n",
       "113653 289278.406  890721.594  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Xy1UD4Jm5OwM"
   },
   "outputs": [],
   "source": [
    "# trends in these samples\n",
    "\n",
    "# ways to reduce these errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1asGaFv5UIp"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Model degradation is a common issue faced when deploying neural network models in the real world. In typical coursework settings, you learn the ropes by experimenting on toy datasets, which only offers a static snapshot of the situation. Real life problems, such as the analysis of factors influencing HDB prices, have new data points coming in daily that might exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. In such situations, models trained on older data points that differ greatly from the new data could perform poorly. In the last part of this assignment, we will investigate whether this has happened.\n",
    "\n",
    "There are 2 datasets to work with: ‘HDB_price_prediction.csv’ and ‘HDB_price_prediction_old.csv’. The latter is a subset of the former: both start from the same date but the latter ends on August 2021 while the former has data until August 2022. Both have the same set of training data (2020 and before) but the test data for the latter (i.e. ‘old test set’) is up till August 2021, while the test set from the former has complete data from 2021, along with data till August 2022 (‘new test set’)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fXNnCpN5cb5"
   },
   "source": [
    "### Part a\n",
    "\n",
    "Apply your model from Q2d on the ‘old test set’. On the ‘new test set’, split it into 2021 and 2022. For all 3 test sets, report the test R^2 value you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "fkFRCQ8DPmD8"
   },
   "outputs": [],
   "source": [
    "# # old test set\n",
    "# old_df = pd.read_csv('./hdb_price_prediction_old.csv')\n",
    "# old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "cfuPi8Ol5cmg",
    "outputId": "6e59e88d-1541-40d8-dd69-8f74d50c9d1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007</td>\n",
       "      <td>7.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333</td>\n",
       "      <td>44.000</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271</td>\n",
       "      <td>7.984</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.070</td>\n",
       "      <td>9.091</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.417</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.947</td>\n",
       "      <td>7.520</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083</td>\n",
       "      <td>68.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.093</td>\n",
       "      <td>9.130</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.417</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104089</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>710 YISHUN AVENUE 5</td>\n",
       "      <td>Yishun</td>\n",
       "      <td>0.826</td>\n",
       "      <td>14.410</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4 ROOM, New Generation</td>\n",
       "      <td>61.750</td>\n",
       "      <td>93.000</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>390000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104090</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>117 YISHUN RING ROAD</td>\n",
       "      <td>Yishun</td>\n",
       "      <td>1.045</td>\n",
       "      <td>15.215</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4 ROOM, Model A</td>\n",
       "      <td>60.917</td>\n",
       "      <td>104.000</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>380000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104091</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>453 YISHUN STREET 41</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>1.425</td>\n",
       "      <td>13.351</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4 ROOM, Model A</td>\n",
       "      <td>91.083</td>\n",
       "      <td>93.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>433000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104092</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>505D YISHUN STREET 51</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>1.260</td>\n",
       "      <td>13.233</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4 ROOM, Model A</td>\n",
       "      <td>93.667</td>\n",
       "      <td>93.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>460000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104093</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>161 YISHUN STREET 11</td>\n",
       "      <td>Yishun</td>\n",
       "      <td>0.405</td>\n",
       "      <td>14.842</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>62.917</td>\n",
       "      <td>126.000</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>550000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104094 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "0           1  2017  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1           1  2017   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2           1  2017   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3           1  2017  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4           1  2017   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "...       ...   ...                       ...           ...   \n",
       "104089      8  2021       710 YISHUN AVENUE 5        Yishun   \n",
       "104090      8  2021      117 YISHUN RING ROAD        Yishun   \n",
       "104091      8  2021      453 YISHUN STREET 41        Khatib   \n",
       "104092      8  2021     505D YISHUN STREET 51        Khatib   \n",
       "104093      8  2021      161 YISHUN STREET 11        Yishun   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0                     1.007          7.006              0.017   \n",
       "1                     1.271          7.984              0.017   \n",
       "2                     1.070          9.091              0.017   \n",
       "3                     0.947          7.520              0.017   \n",
       "4                     1.093          9.130              0.017   \n",
       "...                     ...            ...                ...   \n",
       "104089                0.826         14.410              0.017   \n",
       "104090                1.045         15.215              0.017   \n",
       "104091                1.425         13.351              0.017   \n",
       "104092                1.260         13.233              0.017   \n",
       "104093                0.405         14.842              0.017   \n",
       "\n",
       "        eigenvector_centrality         flat_model_type  remaining_lease_years  \\\n",
       "0                        0.006        2 ROOM, Improved                 61.333   \n",
       "1                        0.006  3 ROOM, New Generation                 60.583   \n",
       "2                        0.002  3 ROOM, New Generation                 62.417   \n",
       "3                        0.006  3 ROOM, New Generation                 62.083   \n",
       "4                        0.002  3 ROOM, New Generation                 62.417   \n",
       "...                        ...                     ...                    ...   \n",
       "104089                   0.000  4 ROOM, New Generation                 61.750   \n",
       "104090                   0.000         4 ROOM, Model A                 60.917   \n",
       "104091                   0.001         4 ROOM, Model A                 91.083   \n",
       "104092                   0.001         4 ROOM, Model A                 93.667   \n",
       "104093                   0.000        5 ROOM, Improved                 62.917   \n",
       "\n",
       "        floor_area_sqm storey_range  resale_price  \n",
       "0               44.000     10 TO 12    232000.000  \n",
       "1               67.000     01 TO 03    250000.000  \n",
       "2               67.000     01 TO 03    262000.000  \n",
       "3               68.000     04 TO 06    265000.000  \n",
       "4               67.000     01 TO 03    265000.000  \n",
       "...                ...          ...           ...  \n",
       "104089          93.000     07 TO 09    390000.000  \n",
       "104090         104.000     07 TO 09    380000.000  \n",
       "104091          93.000     04 TO 06    433000.000  \n",
       "104092          93.000     01 TO 03    460000.000  \n",
       "104093         126.000     10 TO 12    550000.000  \n",
       "\n",
       "[104094 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old data set\n",
    "old_df = pd.read_csv('hdb_price_prediction_old.csv')\n",
    "old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Gx72VJtD2PO5"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "old_train_df = old_df[old_df['year'] <= 2020]\n",
    "old_test_df = old_df[old_df['year'] > 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Bt9tJAn2OFJ9"
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "old_train_ds = dataframe_to_dataset(old_train_df)\n",
    "old_test_ds = dataframe_to_dataset(old_test_df)\n",
    "\n",
    "old_train_ds = old_train_ds.batch(256)\n",
    "old_test_ds = old_test_ds.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "kvpKwL53PnpK"
   },
   "outputs": [],
   "source": [
    "# # new test set\n",
    "# new_df = pd.read_csv('./hdb_price_prediction.csv')\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "SdYjF60OOFQ0",
    "outputId": "599ba1aa-8ed1-4d93-8df6-7aab124880fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007</td>\n",
       "      <td>7.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333</td>\n",
       "      <td>44.000</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271</td>\n",
       "      <td>7.984</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.070</td>\n",
       "      <td>9.091</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.417</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.947</td>\n",
       "      <td>7.520</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083</td>\n",
       "      <td>68.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.093</td>\n",
       "      <td>9.130</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.417</td>\n",
       "      <td>67.000</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133407</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>877 YISHUN STREET 81</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.476</td>\n",
       "      <td>12.739</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>64.583</td>\n",
       "      <td>145.000</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>810000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133408</th>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774</td>\n",
       "      <td>13.229</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>65.000</td>\n",
       "      <td>164.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>785000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133409</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774</td>\n",
       "      <td>13.229</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.917</td>\n",
       "      <td>171.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>842000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133410</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>632 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.701</td>\n",
       "      <td>13.223</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750</td>\n",
       "      <td>164.000</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>845000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133411</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>605 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.604</td>\n",
       "      <td>13.593</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750</td>\n",
       "      <td>163.000</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>862000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133412 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "0           1  2017  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1           1  2017   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2           1  2017   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3           1  2017  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4           1  2017   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "...       ...   ...                       ...           ...   \n",
       "133407      6  2022      877 YISHUN STREET 81        Khatib   \n",
       "133408      1  2022      633 YISHUN STREET 61        Khatib   \n",
       "133409      2  2022      633 YISHUN STREET 61        Khatib   \n",
       "133410      2  2022      632 YISHUN STREET 61        Khatib   \n",
       "133411      5  2022      605 YISHUN STREET 61        Khatib   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0                     1.007          7.006              0.017   \n",
       "1                     1.271          7.984              0.017   \n",
       "2                     1.070          9.091              0.017   \n",
       "3                     0.947          7.520              0.017   \n",
       "4                     1.093          9.130              0.017   \n",
       "...                     ...            ...                ...   \n",
       "133407                0.476         12.739              0.017   \n",
       "133408                0.774         13.229              0.017   \n",
       "133409                0.774         13.229              0.017   \n",
       "133410                0.701         13.223              0.017   \n",
       "133411                0.604         13.593              0.017   \n",
       "\n",
       "        eigenvector_centrality                     flat_model_type  \\\n",
       "0                        0.006                    2 ROOM, Improved   \n",
       "1                        0.006              3 ROOM, New Generation   \n",
       "2                        0.002              3 ROOM, New Generation   \n",
       "3                        0.006              3 ROOM, New Generation   \n",
       "4                        0.002              3 ROOM, New Generation   \n",
       "...                        ...                                 ...   \n",
       "133407                   0.001               EXECUTIVE, Maisonette   \n",
       "133408                   0.001  MULTI-GENERATION, Multi Generation   \n",
       "133409                   0.001  MULTI-GENERATION, Multi Generation   \n",
       "133410                   0.001  MULTI-GENERATION, Multi Generation   \n",
       "133411                   0.001  MULTI-GENERATION, Multi Generation   \n",
       "\n",
       "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \n",
       "0                      61.333          44.000     10 TO 12    232000.000  \n",
       "1                      60.583          67.000     01 TO 03    250000.000  \n",
       "2                      62.417          67.000     01 TO 03    262000.000  \n",
       "3                      62.083          68.000     04 TO 06    265000.000  \n",
       "4                      62.417          67.000     01 TO 03    265000.000  \n",
       "...                       ...             ...          ...           ...  \n",
       "133407                 64.583         145.000     07 TO 09    810000.000  \n",
       "133408                 65.000         164.000     04 TO 06    785000.000  \n",
       "133409                 64.917         171.000     04 TO 06    842000.000  \n",
       "133410                 64.750         164.000     10 TO 12    845000.000  \n",
       "133411                 64.750         163.000     04 TO 06    862000.000  \n",
       "\n",
       "[133412 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new test set\n",
    "new_df = pd.read_csv('hdb_price_prediction.csv')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "D7Haih0JOFX9"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "new_2021_df = df[df['year'] == 2021] \n",
    "new_2022_df = df[df['year'] == 2022] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "VB9kB7KBRIY8"
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "new_2021_ds = dataframe_to_dataset(new_2021_df)\n",
    "new_2022_ds = dataframe_to_dataset(new_2022_df)\n",
    "\n",
    "new_2021_ds = new_2021_ds.batch(256)\n",
    "new_2022_ds = new_2022_ds.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avD_KOMW3hrf",
    "outputId": "a70cb585-bb31-4cd2-a4bf-a394d68a127d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 4ms/step - loss: 5846237696.0000 - r2: 0.7735 - accuracy: 0.0000e+00\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 7473773056.0000 - r2: 0.7145 - accuracy: 0.0000e+00\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 15644053504.0000 - r2: 0.4498 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "r2_old = best_model.evaluate(old_test_ds)\n",
    "r2_new_2021 = best_model.evaluate(new_2021_ds)\n",
    "r2_new_2022 = best_model.evaluate(new_2022_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohJfcnpv2520",
    "outputId": "e65f0d34-748a-476e-c7ec-ff4e70062060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error (old): 0.7735299468040466\n",
      "R Squared Error (new 2021): 0.7145421504974365\n",
      "R Squared Error (new 2022): 0.4497981071472168\n"
     ]
    }
   ],
   "source": [
    "print(\"R Squared Error (old): \" + str(r2_old[1]))\n",
    "print(\"R Squared Error (new 2021): \" + str(r2_new_2021[1]))\n",
    "print(\"R Squared Error (new 2022): \" + str(r2_new_2022[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3VGsaYt5cuI"
   },
   "source": [
    "### Part b\n",
    "\n",
    "The team that produced the linear regression model shared with you their results (test R^2 values): 0.76 on the old test set, 0.715 when only using 2021\n",
    "data as test set and 0.464 when only using 2022 data as test set.\n",
    "\n",
    "In light of this (along with their result in Q1b and your results from Q3a), compare the extent to which model degradation has impacted your model to that of the team’s linear regression model and explain why this has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "esb6hgCs5c2t"
   },
   "outputs": [],
   "source": [
    "# compare the extent to which model degradation impacted model to that of team's linear regression model\n",
    "\n",
    "# explain why this occurred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nW620NY5c9O"
   },
   "source": [
    "### Part c\n",
    "\n",
    "Model degradation could be caused by various data distribution shifts: covariate shift (features), label shift and/or concept drift (altered relationship between features and labels). Recall that machine learning models generally need the test data distribution to be similar to the training data distribution.\n",
    "\n",
    "With appropriate plots, visualise the distributions of all the features and labels used by the model. Which variable(s) showed the largest covariate/label shift that might have led to the drop in model performance as seen in Q3b? With these insights, suggest a way to address the problem of model degradation. (Note: Only include plots relevant to your answer. Do not include all plots.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "U0F5OAcI5dD0",
    "outputId": "46fb3f18-5eca-44d5-8b1d-de551ab82cbe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xV5X3v8c9XJOCNQoBYZLCgiOEiDM6IVILxEpVgThVPTDGJtyTSKOSksUmVnqYx8fCSNNYL3hISL5CKhNQYiFUjXlCpFzJY5CoBC9FRjihKRKNU8Nc/1pphM+yZvQdm32a+79drv2bvZz1r7d8M7PnNc1nPo4jAzMysJfuVOgAzMyt/ThZmZpaTk4WZmeXkZGFmZjk5WZiZWU77lzqAQunVq1f079+/1GGYmVWUpUuXvhkRvZuWt9tk0b9/f+rq6kodhplZRZH0h2zl7oYyM7OcnCzMzCwnJwszM8up3Y5ZmFnl+/DDD6mvr+eDDz4odSjtTteuXamqqqJz58551XeyMLOyVV9fzyGHHEL//v2RVOpw2o2IYMuWLdTX1zNgwIC8znE3lJmVrQ8++ICePXs6UbQxSfTs2bNVLTYnCzMra04UhdHan6uThZmZ5eQxCzOrGNcv/H2bXu9bpw3KWWfr1q3MmTOHyy67rFXXHj9+PHPmzKF79+57G15ZKXjLQlInSf8p6f709cclLZS0Lv3aI6PuVEnrJa2VdEZGeY2kFemxGXK7tCiuX/j7xodZR7V161ZuvfXWPcp37tzZ4nkPPPBAu0kUUJxuqG8CazJeXwk8GhFHAY+mr5E0BJgIDAXGAbdK6pSecxswCTgqfYwrQtxmZlx55ZW89NJLVFdXc9xxx3HyySfzxS9+kWOOOQaAs88+m5qaGoYOHcrMmTMbz+vfvz9vvvkmGzduZPDgwVxyySUMHTqU008/nffff79U385eK2iykFQFnAn8LKP4LGBW+nwWcHZG+dyI2B4RG4D1wChJfYBuEfFMJHvAzs44x8ysoKZPn86RRx7JsmXL+NGPfsSSJUuYNm0aq1evBuCOO+5g6dKl1NXVMWPGDLZs2bLHNdatW8fkyZNZtWoV3bt359577y32t7HPCt2yuAH4e+CjjLJDI2ITQPr1E2l5X+CVjHr1aVnf9HnTcjOzohs1atRu9ybMmDGDESNGMHr0aF555RXWrVu3xzkDBgyguroagJqaGjZu3FiscNtMwZKFpM8BmyNiab6nZCmLFsqzveckSXWS6t54440839bMLH8HHXRQ4/NFixbxyCOP8Mwzz/DCCy8wcuTIrPcudOnSpfF5p06d2LFjR1FibUuFbFmMAf5K0kZgLnCKpH8FXk+7lki/bk7r1wP9Ms6vAl5Ly6uylO8hImZGRG1E1Pbuvcdy7GZmrXbIIYewbdu2rMf++Mc/0qNHDw488EBefPFFnn322SJHVzwFmzobEVOBqQCSTgK+HRFflvQj4EJgevp1fnrKAmCOpOuAw0gGspdExE5J2ySNBp4DLgBuKlTcZla+8pnq2tZ69uzJmDFjGDZsGAcccACHHnpo47Fx48bx4x//mOHDh3P00UczevToosdXLKW4z2I6ME/SV4GXgXMBImKVpHnAamAHMDkiGuamXQrcBRwAPJg+zMyKYs6cOVnLu3TpwoMPZv911DAu0atXL1auXNlY/u1vf7vN4yuGoiSLiFgELEqfbwFObabeNGBalvI6YFjhIjQzs5Z4uQ8zM8vJycLMzHJysjAzs5ycLMzMLCcnCzMzy8lLlJtZ5Xj8mra93slT9/rUgw8+mHfffXeP8osuuojPfe5zfP7zn9+XyMqOWxZmZpaTk4WZWQ7XXXcdw4YNY9iwYdxwww27HYsIpkyZwpAhQzjzzDPZvHlzM1epbO6GMjNrwdKlS7nzzjt57rnniAiOP/54Pv3pTzcev++++1i7di0rVqzg9ddfZ8iQIXzlK18pYcSF4WRhZtaCxYsXM2HChMbVZs855xyeeuqpxuNPPvkk5513Hp06deKwww7jlFNOKVWoBeVuKDOzFiR7rrWsI+z07GRR4bxHtllhnXjiifz617/mT3/6E++99x733XcfY8eO3e343Llz2blzJ5s2beLxxx8vYbSF424oM6sc+zDVdW8de+yxXHTRRYwaNQqAr33ta4wcObLx+IQJE3jsscc45phjGDRo0G7jGe2Jk0UH1NASKcXeAGaV6PLLL+fyyy/frazhHgtJ3HzzzaUIq6jcDWVmZjk5WZiZWU5OFmZmllPBkoWkrpKWSHpB0ipJ30/Lr5L0qqRl6WN8xjlTJa2XtFbSGRnlNZJWpMdmqCPMUzMzKyOFHODeDpwSEe9K6gwsltSwWe31EXFtZmVJQ4CJwFDgMOARSYPSfbhvAyYBzwIPAOPwPtxmZkVTsJZFJBqWZOycPlq6u+UsYG5EbI+IDcB6YJSkPkC3iHgmkrtjZgNnFypuMzPbU0GnzkrqBCwFBgK3RMRzkj4LTJF0AVAH/F1EvA30JWk5NKhPyz5Mnzctz/Z+k0haIBx++OFt/N2YWanduuzWNr3eZdWXten12rOCDnBHxM6IqAaqSFoJw0i6lI4EqoFNwL+k1bONQ0QL5dneb2ZE1EZEbe/evfc5fjOzYrj77rsZPnw4w4cP54QTTuCFF15oPPbQQw9x9NFHM3DgQKZPn95Y/p3vfIdPfvKTDB8+nAkTJrB161YAtmzZwsknn8zBBx/MlClT2izGosyGioitwCJgXES8niaRj4CfAqPSavVAv4zTqoDX0vKqLOVmZu3CgAEDeOKJJ1i+fDnf/e53mTRpEgA7d+5k8uTJPPjgg6xevZp77rmH1atXA3DaaaexcuVKli9fzqBBg7jmmmRjqK5du3L11Vdz7bXXNvt+e6OQs6F6S+qePj8A+AzwYjoG0WACsDJ9vgCYKKmLpAHAUcCSiNgEbJM0Op0FdQEwv1Bxm5ll2rhxI4MHD+aSSy5h6NChnH766bz//vu89NJLjBs3jpqaGsaOHcuLL77Izp07OeKII4gItm7dyn777ceTTz4JwNixY1m/fn3W9zjhhBPo0aMHAKNHj6a+Pul5X7JkCQMHDuSII47gYx/7GBMnTmT+/OTX3+mnn87++++/xzkHHXQQn/rUp+jatWub/hwK2bLoAzwuaTnwO2BhRNwP/HM6DXY5cDLwLYCIWAXMA1YDDwGT05lQAJcCPyMZ9H4Jz4QysyJat24dkydPZtWqVXTv3p17772XSZMmcdNNN7F06VKuvfZaLrvsMjp16sSgQYNYvXo1ixcvpqamhqeeeort27dTX1/PwIEDc77X7bffzmc/+1kAXn31Vfr129XhUlVVxauvvrrHOXfccUfjOYVSsAHuiFgOjMxSfn4L50wDpmUprwOGtWmAZmZ5GjBgANXV1QDU1NSwceNGnn76ac4999zGOtu3bweSFsSTTz7Jhg0bmDp1Kj/96U/59Kc/zXHHHZfzfR5//HFuv/12Fi9eDGRfHr3pbWbTpk1j//3350tf+tJef3/58B3cZmY5dOnSpfF5p06deOutt+jevTvLli1rfKxZswZIksVTTz3FkiVLGD9+PFu3bmXRokWceOKJLb7H8uXL+drXvsb8+fPp2bMnkLQkXnnllcY69fX1HHbYYY2vZ82axf3338/dd99d8D01vOqsmVWMcpnq2q1bNwYMGMAvf/lLzj33XCKC5cuXM2LECI4//nguuOACjjjiCLp27Up1dTU/+clPuP/++5u93ssvv8w555zDz3/+cwYN2rUa9HHHHce6devYsGEDffv2Ze7cucyZMwdIZkn98Ic/5IknnuDAAw8s+PfsloWZ2V64++67uf322xkxYgRDhw5tHHju0qUL/fr1Y/To0UDS0ti2bRvHHHNMs9f6wQ9+wJYtW7jsssuorq6mtrYWgP3335+bb76ZM844g8GDB/OFL3yBoUOHAjBlyhS2bdvGaaedRnV1NV//+tcbr9e/f38uv/xy7rrrLqqqqhpnUO0L5bNlYCWqra2Nurq6UodRcHuzN0W+52TuwOe9L6wU1qxZw+DBg0sdRruV7ecraWlE1Dat65aFmZnl5DELM7MiufPOO7nxxht3KxszZgy33HJLiSLKn5OFAe5yMiuGiy++mIsvvrjUYewVd0OZmVlOThZmZpaTk4WZmeXkMQszqxhv3HRzm16v9zfabgnv9s4tCzOzEmvL/SwWLlxITU0NxxxzDDU1NTz22GNtEqOThZlZibXlfha9evXiN7/5DStWrGDWrFmcf36za7e2ipOFmVkLKm0/i5EjRzYuNjh06FA++OCDxhVx94WThZlZDpW6n8W9997LyJEjd1s1d295gNvMLIdK3M9i1apVXHHFFTz88MOt+2abUchtVbtKWiLpBUmrJH0/Lf+4pIWS1qVfe2ScM1XSeklrJZ2RUV6T7q63XtIMFXrhdjOzDJW2n0V9fT0TJkxg9uzZHHnkkW3yMyhky2I7cEpEvCupM7BY0oPAOcCjETFd0pXAlcAVkoYAE4GhwGHAI5IGpVur3gZMAp4FHgDG4a1VzTqccpnqWs77WWzdupUzzzyTa665hjFjxrTZ91ywlkUk3k1fdk4fAZwFzErLZwFnp8/PAuZGxPaI2ECy3/YoSX2AbhHxTCRtstkZ55iZlUS57mdx8803s379eq6++mqqq6uprq5m8+bN+/z9FnQ/C0mdgKXAQOCWiLhC0taI6J5R5+2I6CHpZuDZiPjXtPx2ktbDRmB6RHwmLR8LXBERn8vyfpNIWiAcfvjhNX/4wx8K9r2Vi7bazyLbQoJeXNBKzftZFFZr9rMo6AB32oVULak7cJ+kYS1UzzYOES2UZ3u/mcBMSDY/amW4lspMEmZmUKTZUBGxVdIikrGG1yX1iYhNaRdTQ/uoHuiXcVoV8FpaXpWl3Mysong/iywk9QY+TBPFAcBngB8CC4ALgenp1/npKQuAOZKuIxngPgpYEhE7JW2TNBp4DrgAuKlQcVcC/+VvHUlE7DFdtFKV034WrR2CKGTLog8wKx232A+YFxH3S3oGmCfpq8DLwLkAEbFK0jxgNbADmJx2YwFcCtwFHEAyjuGZUGYdQNeuXdmyZQs9e/ZsNwmjHEQEW7ZsoWvXrnmfU7BkERHLgZFZyrcApzZzzjRgWpbyOqCl8Q4za4eqqqqor6/njTfeKHUo7U7Xrl2pqqrKXTHlO7jNrGx17tyZAQMGlDoMw8miXfKUVzNra15I0MzMcnKyMDOznNwN1YF5Cq6Z5cvJooNwYjCzfeFuKDMzy8kti3bOLQozawtuWZiZWU5OFmZmlpOThZmZ5eRkYWZmOTlZmJlZTk4WZmaWk5OFtdr1C3/vKblmHYyThZmZ5eRkYWZmORUsWUjqJ+lxSWskrZL0zbT8KkmvSlqWPsZnnDNV0npJayWdkVFeI2lFemyG2vn+ig3dPO7qMbNyUcjlPnYAfxcRz0s6BFgqaWF67PqIuDazsqQhwERgKHAY8IikQek+3LcBk4BngQeAcXgf7t0UOrE4cZl1bAVrWUTEpoh4Pn2+DVgD9G3hlLOAuRGxPSI2AOuBUZL6AN0i4pmICGA2cHah4jYzsz0VZcxCUn9gJPBcWjRF0nJJd0jqkZb1BV7JOK0+LeubPm9anu19Jkmqk1TnDd7NzNpOwZOFpIOBe4G/jYh3SLqUjgSqgU3AvzRUzXJ6tFC+Z2HEzIiojYja3r1773PsZmaWKGiykNSZJFHcHRG/AoiI1yNiZ0R8BPwUGJVWrwf6ZZxeBbyWlldlKTczsyIp5GwoAbcDayLiuozyPhnVJgAr0+cLgImSukgaABwFLImITcA2SaPTa14AzC9U3GZmtqdCzoYaA5wPrJC0LC37B+A8SdUkXUkbgb8BiIhVkuYBq0lmUk1OZ0IBXArcBRxAMgvKM6HMzIqoYMkiIhaTfbzhgRbOmQZMy1JeBwxru+isrWVOrf3WaYNKGImZFYLv4DYzs5zyShaSHs2nzMzM2qcWu6EkdQUOBHql90M0dCt1I7nL2szMOoBcYxZ/A/wtSWJYyq5k8Q5wSwHjsia83IaZlVKLySIibgRulPSNiLipSDGZmVmZyWs2VETcJOkEoH/mORExu0BxmZlZGckrWUj6OckSHcuAhnsfGhb1MzOzdi7f+yxqgSHpqq9mZtbB5HufxUrgzwsZiJmZla98Wxa9gNWSlgDbGwoj4q8KEpWZmZWVfJPFVYUMwszMylu+s6GeKHQgHZ3vozCzcpbvbKht7Npw6GNAZ+C9iOhWqMDMzKx85NuyOCTztaSz2bVpkZmZtXN7tepsRPwaOKWNYzEzszKVbzfUORkv9yO578L3XJiZdRD5tiz+V8bjDGAbcFZLJ0jqJ+lxSWskrZL0zbT845IWSlqXfu2Rcc5USeslrZV0RkZ5jaQV6bEZ6faqZmZWJPmOWVy8F9feAfxdRDwv6RBgqaSFwEXAoxExXdKVwJXAFZKGABOBoSSr3D4iaVC6teptwCTgWZKd9sbhrVXNzIom326oKuAmkn21A1gMfDMi6ps7JyI2AZvS59skrQH6krRITkqrzQIWAVek5XMjYjuwQdJ6YJSkjUC3iHgmjWU2cDZOFiXn6b5mHUe+3VB3AgtI/uLvC/wmLcuLpP7ASOA54NA0kTQklE+k1foCr2ScVp+W9U2fNy3P9j6TJNVJqnvjjTfyDc/MzHLIN1n0jog7I2JH+rgL6J3PiZIOBu4F/jYi3mmpapayaKF8z8KImRFRGxG1vXvnFZ6ZmeUh32TxpqQvS+qUPr4MbMl1kqTOJIni7oj4VVr8uqQ+6fE+wOa0vB7ol3F6FfBaWl6Vpdwq1PULf+8uLLMKk2+y+ArwBeD/k4xDfB5ocdA7nbF0O7AmIq7LOLQAuDB9fiEwP6N8oqQukgYARwFL0q6qbZJGp9e8IOMcMzMrgnwXErwauDAi3oZk+itwLUkSac4Y4HxghaRladk/ANOBeZK+CrwMnAsQEaskzQNWk8ykmpzOhAK4FLgLOIBkYNuD22ZmRZRvshjekCgAIuItSSNbOiEiFpN9vAHg1GbOmQZMy1JeBwzLM1YzM2tj+SaL/ST1aNKyyPdca0Guvnv37ZtZOcj3F/6/AE9L+jeSmUhfIEsLwMzM2qd87+CeLamOZPFAAedExOqCRmZmZmUj766kNDk4QZiZdUB7tUS5mZl1LE4WZmaWk5OFtTnfoW3W/jhZmJlZTk4WZmaWk5OFmZnl5GRhZmY5OVmYmVlOThZmZpaTFwO0ovBUWrPK5paFmZnl5GRhZmY5FSxZSLpD0mZJKzPKrpL0qqRl6WN8xrGpktZLWivpjIzyGkkr0mMz0q1VzcysiArZsrgLGJel/PqIqE4fDwBIGgJMBIam59wqqVNa/zZgEsme3Ec1c00zMyuggiWLiHgSeCvP6mcBcyNie0RsANYDoyT1AbpFxDMREcBs4OzCRGxmZs0pxZjFFEnL026qHmlZX+CVjDr1aVnf9HnTcjMzK6JiJ4vbgCOBamATyXatkOy+11S0UJ6VpEmS6iTVvfHGG/saq5mZpYqaLCLi9YjYGREfAT8FRqWH6oF+GVWrgNfS8qos5c1df2ZE1EZEbe/evds2eDOzDqyoySIdg2gwAWiYKbUAmCipi6QBJAPZSyJiE7BN0uh0FtQFwPxixmxmZgW8g1vSPcBJQC9J9cD3gJMkVZN0JW0E/gYgIlZJmkeyx/cOYHJE7EwvdSnJzKoDgAfTh5mZFVHBkkVEnJel+PYW6k8DpmUprwOGtWFoJeelL8ys0nhtqCJxgjCzSublPszMLCcnCzMzy8ndUFYwubreMo9/67RBe5RnlplZaTlZWFnwmI5ZeXOysIrQXCvEzIrDYxZmZpaTk4WZmeXkZGFmZjl5zMJa5fl3ftH4/Nhuf13CSMysmNyyMDOznNyysDbn1odZ++NkYUXhBGJW2dwNZWZmOTlZmJlZTk4WZmaWk5OFmZnlVLBkIekOSZslrcwo+7ikhZLWpV97ZBybKmm9pLWSzsgor5G0Ij02I92L28zMiqiQLYu7gHFNyq4EHo2Io4BH09dIGgJMBIam59wqqVN6zm3AJOCo9NH0mlYiz7/zi8aHmbVvBUsWEfEk8FaT4rOAWenzWcDZGeVzI2J7RGwA1gOjJPUBukXEMxERwOyMc8yyun7h773kuVkbK/Z9FodGxCaAiNgk6RNpeV/g2Yx69WnZh+nzpuVZSZpE0grh8MMPb8OwrRT8C9+sfJTLTXnZxiGihfKsImImMBOgtra22XrWOvl0M7kryqx9K/ZsqNfTriXSr5vT8nqgX0a9KuC1tLwqS7l1YO5mMiu+YieLBcCF6fMLgfkZ5RMldZE0gGQge0naZbVN0uh0FtQFGeeYmVmRFKwbStI9wElAL0n1wPeA6cA8SV8FXgbOBYiIVZLmAauBHcDkiNiZXupSkplVBwAPpg8zMyuigiWLiDivmUOnNlN/GjAtS3kdMKwNQ7Mi8liGWftQLgPcVob8i97MGjhZ2G6cIMwsGyeLDqQ97ynh2VFmheVkYRXLCcKseLzqrJmZ5eRkYWZmObkbyoquPY+dmLVXThZWNjOgipVEGsY6vnXaoIK9h1l742Rhzap6Z2nj8/puNQV5j4YEMfbR5YwFnjp1eEHex4PhZvvGyaIClWs3zthHlzc+L8Qv/dZ+322ZIDKv5RaJdUROFh1UuXQ9mVllcLIosGJ2f5Rri8PMKp+TRQfgVkR27loyy5+ThTVrxOI3G5/Xj2+5btU7S+m2/U3e6dKnxXqFHtcws8JwsqhwpWg1tNUv/MzrlJpbGWYt8x3cZmaWU0laFpI2AtuAncCOiKiV9HHgF0B/YCPwhYh4O60/FfhqWv//RMRvSxC2NdFt+yaq3vmw2eP7cp9GQ4upLQbq93bKrVsYZruUshvq5Ih4M+P1lcCjETFd0pXp6yskDQEmAkOBw4BHJA3K2Ha1wyiXgepu2zeVOoRmtWWSMbNdymnM4iySPbsBZgGLgCvS8rkRsR3YIGk9MAp4pgQxVoy2TiyZrYSWNCSSllocrVEuCdKsoytVsgjgYUkB/CQiZgKHRsQmgIjYJOkTad2+wLMZ59anZXuQNAmYBHD44YcXKvacvLTE7kYsfpMBXVo/mL0vA+ltkWSa+3d0N5V1RKVKFmMi4rU0ISyU9GILdZWlLLJVTJPOTIDa2tqsdSy7XOMLmdNoC6mcZkiZ2S4lSRYR8Vr6dbOk+0i6lV6X1CdtVfQBNqfV64F+GadXAa8VNeB2oFCLAjaXRAqdXMrhbnVPt7WOpOjJQtJBwH4RsS19fjrwA2ABcCEwPf06Pz1lATBH0nUkA9xHAUuKHbftm4axjMyb9rKV7Y1yGNdw4rD2rhQti0OB+yQ1vP+ciHhI0u+AeZK+CrwMnAsQEaskzQNWAzuAye19JtTe/tWcbx9/rlZGvoPZlagcWiRmlajoySIi/gsYkaV8C3BqM+dMA6YVOLSylPnLrTVdSa0dHM7sNnrhU71aE2JJlePyIdkGxnO1NtwysXJXTlNnrQTKuRXRVt1UZrbvnCw6gLGPLqfb9qTlUI6thsyb/PJJDPsyYyrz3Pcm5N8N5e4r6+icLNq5crvbem/jKfSU2mLe+e37NKwSOVm0kWLeiDdi8ZuMIFkeqxCzi4p1T0UhleNYhlklc7KocNn+Um/pr/f2kAhaq7lWSTlMuTWrFE4WVlZydVO11HrafbC+8z7H0pbJxEvAWKVzsigTzf1iKufZSu1Rc/8Ou3dr7SrPHONoq3GPvZl6a1ZoThZlyklil1xdbQ2tjOa62Aq51Wu25NLczCnPqLJK5mRh7V5zXVeVtGihWxtWak4WJdT0r1K3JvZOW00P7rZ9E2c+sCuxeBaV2S5OFhWiI85iamvZkkpLXVTNtTyaK89MLg11+nT5iPXjz8tav6G10Nb3eLgVYoXgZLGPWjvLxdM1O5ZN21fu1b95PuMbXk/KisnJwjq05rqwWpqi29rlSbJpTQJpbeIwKwQnizLQdKyiUleAbY9yJYbM45njHc1p6J7K1mUFy3OOk3hGlZWKk0WJ5DuY7bGK8pHvQHrTBJMs5LhnWWbdhgSTz3hIg+ff+cUe9ft0GQbQ7DhJPrx2lWXjZFEkz7/zC6reWUpVqQOxomrtcizN1Rv7aH7vt2n7SgAOuu//NpY9depwzr93V51sLRInBsvFyaJMuAXRsbVm+u/ezupqqYuroTUx8IF7GNhQeNr39jgOTiwdlSKi1DHkRdI44EagE/CziJjeUv3a2tqoq6srSCytHUwc/fJMFuy3fo9yJwhrKw3JIp/B94auqoZWSHOaJpeWxkicQNoPSUsjorZpeUW0LCR1Am4BTgPqgd9JWhARq0sbWfMyE8SC/XaVO0FYIeTb3fVOlz6NSaKl1kz2u913jY80TTgPLDqQ8dNm7XaOWyPtS0W0LCT9JXBVRJyRvp4KEBHXNHdOW7cs8m1NbK7/+z3KnCCsI/j38Wc0Pj/zgd/mdU6+U48n3fpLAG5ddutu5f1/+dweddeelIzXfOu0QbvVP/epjxqf9/7GlMbrNVzjuD8/rrEc4I2bbt6jfkfQXMuiUpLF54FxEfG19PX5wPERMaVJvUnApPTl0cDaogYKvYBKzgyVHH8lxw6Ov5QqOXZo+/j/IiJ6Ny2siG4oQFnK9shyETETmFn4cLKTVJctI1eKSo6/kmMHx19KlRw7FC/+/XJXKQv1QL+M11XAayWKxcysw6mUZPE74ChJAyR9DJgILChxTGZmHUZFdENFxA5JU4DfkkydvSMiVpU4rGxK1gXWRio5/kqOHRx/KVVy7FCk+CtigNvMzEqrUrqhzMyshJwszMwsJyeLvSBpnKS1ktZLujLL8S9JWp4+npY0ohRxZpMr9ox6x0namd7jUjbyiV/SSZKWSVol6Ylix9iSPP7v/Jmk30h6IY3/4lLEmY2kOyRtlpR1nRAlZqTf23JJxxY7xubkEXvZfmYhd/wZ9Qr3uY0IP1rxIBlgfwk4AvgY8AIwpEmdE4Ae6fPPAs+VOu58Y8+o9xjwAPD5Usfdyp99d2A1cHj6+hOljrQpLVwAAAZASURBVLuV8f8D8MP0eW/gLeBjpY49jedE4FhgZTPHxwMPktwXNbpc/t/nGXtZfmbzjT/j/1fBPrduWbTeKGB9RPxXRPw3MBc4K7NCRDwdEW+nL5+FslmZPGfsqW8A9wKbixlcHvKJ/4vAryLiZYCIKKfvIZ/4AzhEkoCDSZLFjuKGmV1EPEkST3POAmZH4lmgu6S920qwjeWKvYw/s0BeP3so8OfWyaL1+gKvZLyuT8ua81WSv7bKQc7YJfUFJgA/LmJc+crnZz8I6CFpkaSlki4oWnS55RP/zcBgkptOVwDfjIiPqAyt/WyUq3L6zOalGJ/birjPoszktfQIgKSTSf7jfaqgEeUvn9hvAK6IiJ3JH7dlJZ/49wdqgFOBA4BnJD0bEeWwSXU+8Z8BLANOAY4EFkp6KiLeKXRwbSDvz0a5KsPPbL4K/rl1smi9vJYekTQc+Bnw2YjYUqTYcskn9lpgbvofrhcwXtKOiPh1cUJsUT7x1wNvRsR7wHuSngRGAOWQLPKJ/2JgeiSd0OslbQA+CSwpToj7pKKX5SnTz2y+Cv65dTdU6+VcekTS4cCvgPPL5C/aBjljj4gBEdE/IvoD/wZcViaJAvJb9mU+MFbS/pIOBI4H1hQ5zubkE//LJK0iJB1KsnryfxU1yr23ALggnRU1GvhjROS/BWAJlfFnNi/F+Ny6ZdFK0czSI5K+nh7/MfBPQE/g1jTT74gyWNUyz9jLVj7xR8QaSQ+R7NTzEcmuii1vCVckef78rwbukrSCpFvniogoi+WzJd0DnAT0klQPfA/oDI2xP0AyI2o98CeSVlJZyCP2svzMNsgj/sLHkE65MjMza5a7oczMLCcnCzMzy8nJwszMcnKyMDOznJwszMzagXwXG8yo/wVJq9MFK+fkrO/ZUGZmlU/SicC7JOtzDctR9yhgHnBKRLwt6RO51lFzy8KsjUm6q5BLu0uqlTSjUNe3ypRtsUFJR0p6KF0n7SlJn0wPXQLc0rB4Yj4LbvqmPLNUutKrynnhPkn7R0QdUFfqWKwizAS+HhHrJB0P3Eqy7tggAEn/QXKD6FUR8VBLF3LLwjo0Sf0lrZF0K/A88F1Jv0s3wfl+WucgSf+ebki0UtJfp+X/lNZdKWmmsqzgJqlG0hPpX3a/bWnJ7nSl3BvSzXdWShqVll+VXv9hYLaSzZ3uT48dLOlOSSvSmP93Wn66pGckPS/pl5IObvMfnpW19N/8BOCXkpYBPwEa/v/tDxxFclf4ecDPJHVv6XpOFmbJ+kuzgStIltQeBVQDNWk/8DjgtYgYkfYFN/wFdnNEHJeWHQB8LvOikjoDN5FsRFMD3AFMyxHLQRFxAnBZWr9BDXBWRHyxSf3vkqzBdExEDAcek9QL+EfgMxFxLEkr5PJ8fxjWbuwHbI2I6ozH4PRYPTA/Ij6MiA3AWpLk0eLFzDq6P6Sb9ZyePv6TpJXxSZIP0ArgM5J+KGlsRPwxPe9kSc+l6zidAgxtct2jgWEky4wvI/kFnmtTnXugsf+5W8Zfewsi4v0s9T8D3NLwIu2DHg0MAf4jfd8Lgb/I9UOw9iVd1n6DpHOhcdvbhu1ifw2cnJb3IumWanHBSo9ZmMF76VcB10TET5pWkFRDskjeNWl30D+T9P/WRsQrkq4CujY9DVgVEX/ZiliaTk9seP1e04oZ79H0HAELI+K8VryvVbhmFhv8EnCbpH8kWXhwLsl2vr8FTpe0GtgJfCfXsuxuWZjt8lvgKw39+5L6SvqEpMOAP0XEvwLXkuyF3JAY3kzrZ5v9tBboLekv0+t1ltS09dFUw3jIp0i6l/6Yo/7DwJSGF5J6kGwLOkbSwLTsQEmDclzHKlxEnBcRfSKic0RURcTtEbEhIsalXahDIuIHad2IiMvTsmMiYm6u67tlYZaKiIclDSbZXQ+SOetfBgYCP5L0EfAhcGlEbJX0U5Iuqo0ke1U0vd5/p1NoZ0j6M5LP2w3AqhbCeFvS00A34Ct5hP3/gFvSG7F2At+PiF9Jugi4R1KXtN4/Uh4bQFmF8k15ZmVC0iLg2+nUWLOy4m4oMzPLyd1QZkUm6RZgTJPiGyPipBKEY5YXd0OZmVlO7oYyM7OcnCzMzCwnJwszM8vJycLMzHL6H6Ht7Suxx2JhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution for train and test sets\n",
    "old_train_df[\"resale_price\"].plot.hist(bins=100, alpha=0.5)\n",
    "old_test_df[\"resale_price\"].plot.hist(bins=100, alpha=0.5)\n",
    "new_2021_df[\"resale_price\"].plot.hist(bins=100, alpha=0.5)\n",
    "new_2022_df[\"resale_price\"].plot.hist(bins=100, alpha=0.5)\n",
    "\n",
    "plt.legend([\"train\", \"old\", \"new_2021\", \"new_2022\"])\n",
    "plt.xlabel(\"resale_price\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "#show plot\n",
    "plt.ticklabel_format()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "2V7kjk4z4xg-"
   },
   "outputs": [],
   "source": [
    "# comment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "CCyGgmkmBun4",
    "outputId": "2ad01124-4484-422b-9dc7-7d1e2540a508"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution for flat_model_type\n",
    "old_train_df[\"flat_model_type\"].value_counts().plot.bar(color=\"orange\")\n",
    "old_test_df[\"flat_model_type\"].value_counts().plot.bar(color=\"black\")\n",
    "new_2021_df[\"flat_model_type\"].value_counts().plot.bar(color=\"pink\")\n",
    "new_2022_df[\"flat_model_type\"].value_counts().plot.bar(color=\"cyan\")\n",
    "\n",
    "plt.legend([\"train\", \"old\", \"new_2021\", \"new_2022\"])\n",
    "plt.xlabel(\"flat_model_type\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "XwhVD0gmBuxK"
   },
   "outputs": [],
   "source": [
    "# comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "BJ8FFYJvBu3h",
    "outputId": "78d86ca9-8a1f-40e3-8963-d2b8724c6f89"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution for nearest_stn\n",
    "old_train_df[\"nearest_stn\"].value_counts().plot.bar(color=\"orange\")\n",
    "old_test_df[\"nearest_stn\"].value_counts().plot.bar(color=\"black\")\n",
    "new_2021_df[\"nearest_stn\"].value_counts().plot.bar(color=\"pink\")\n",
    "new_2022_df[\"nearest_stn\"].value_counts().plot.bar(color=\"cyan\")\n",
    "\n",
    "plt.legend([\"train\", \"old\", \"new_2021\", \"new_2022\"])\n",
    "plt.xlabel(\"nearest_stn\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "rjr1ATIv4xnt"
   },
   "outputs": [],
   "source": [
    "# comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "-66lV4G8bTtr"
   },
   "outputs": [],
   "source": [
    "# which variable showed largest covariate/label shift that might have led to drop in model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "5Nezk7vSbUzs"
   },
   "outputs": [],
   "source": [
    "# suggest a way to address problem of model degradation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tLzJ-2E5da6"
   },
   "source": [
    "### Part d\n",
    "\n",
    "The team passed you a script (‘RFE.py’) that recursively removes features from a neural network, so as to find the best feature subset. Run this piece of\n",
    "code with your model from Q2d and report the best feature subset obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "YcDaTx6F5di-"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "num_features = 9\n",
    "\n",
    "vec = [1 for i in range(num_features)]\n",
    "best_loss = 1e15\n",
    "new_best_loss = 1e14\n",
    "which_iter = ''\n",
    "\n",
    "all_losses = [] # should be len 9,8,7,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Uwrt8LhgGriF"
   },
   "outputs": [],
   "source": [
    "def train_model(feature_mask, best_neuron, best_divisor, best_learning):\n",
    "    \"\"\"\n",
    "    Given a boolean mask (feature_mask), select the features accordingly, train the model and return the validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_mask_string = ''.join([str(i) for i in feature_mask])\n",
    "    \n",
    "    # TODO: define the input layer here (your code from Q2)\n",
    "    # Categorical features encoded as integers\n",
    "    month_len = len(np.unique(train_dataframe['month']))\n",
    "    month = keras.Input(shape=(1,), name=\"month\", dtype=\"int64\")\n",
    "    month_embedded = embed_categorical_feature(month, \"month\", train_ds, False, month_len, best_divisor)\n",
    "\n",
    "    # Categorical feature encoded as string\n",
    "    flat_model_type_len = len(np.unique(train_dataframe['flat_model_type']))\n",
    "    flat_model_type = keras.Input(shape=(1,), name=\"flat_model_type\", dtype=\"string\")\n",
    "    flat_model_type_embedded = embed_categorical_feature(flat_model_type, \"flat_model_type\", train_ds, True, flat_model_type_len, best_divisor)\n",
    "\n",
    "    storey_range_len = len(np.unique(train_dataframe['storey_range']))\n",
    "    storey_range = keras.Input(shape=(1,), name=\"storey_range\", dtype=\"string\")\n",
    "    storey_range_embedded = embed_categorical_feature(storey_range, \"storey_range\", train_ds, True, storey_range_len, best_divisor)\n",
    "\n",
    "    # Numerical features\n",
    "    floor_area_sqm = keras.Input(shape=(1,), name=\"floor_area_sqm\")\n",
    "    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm, \"floor_area_sqm\", train_ds)\n",
    "\n",
    "    remaining_lease_years = keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "    remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years, \"remaining_lease_years\", train_ds)\n",
    "\n",
    "    eigenvector_centrality = keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, \"eigenvector_centrality\", train_ds) \n",
    "\n",
    "    degree_centrality = keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "    degree_centrality_encoded = encode_numerical_feature(degree_centrality, \"degree_centrality\", train_ds)\n",
    "\n",
    "    dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, \"dist_to_dhoby\", train_ds)\n",
    "\n",
    "    dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, \"dist_to_nearest_stn\", train_ds)\n",
    "\n",
    "    all_inputs = [\n",
    "        dist_to_nearest_stn,\n",
    "        dist_to_dhoby,\n",
    "        degree_centrality,\n",
    "        eigenvector_centrality,\n",
    "        remaining_lease_years,\n",
    "        floor_area_sqm,\n",
    "        month,\n",
    "        flat_model_type,\n",
    "        storey_range\n",
    "    ]\n",
    "\n",
    "    all_features_input = [\n",
    "            month_embedded,\n",
    "            flat_model_type_embedded,\n",
    "            floor_area_sqm_encoded,\n",
    "            storey_range_embedded,\n",
    "            remaining_lease_years_encoded,\n",
    "            eigenvector_centrality_encoded,\n",
    "            degree_centrality_encoded,\n",
    "            dist_to_dhoby_encoded,\n",
    "            dist_to_nearest_stn_encoded        \n",
    "      ]\n",
    "\n",
    "\n",
    "    selected_inputs = []\n",
    "    print('going through feature_mask', feature_mask)\n",
    "    for i,j in zip(all_features_input, feature_mask):\n",
    "        if j == 1:\n",
    "            selected_inputs.append(i)\n",
    "            print(i)\n",
    "        else:\n",
    "            print('Skipping', i)\n",
    "\n",
    "    all_features = layers.concatenate(selected_inputs)\n",
    "    \n",
    "    # TODO: Complete the rest of the architecture + training code and retrieve the training history\n",
    "    # setup parameters\n",
    "    opt_ = tf.keras.optimizers.Adam(learning_rate=best_learning)\n",
    "    epochs_ = 50\n",
    "    batch_ = 256\n",
    "\n",
    "    # define model\n",
    "    x_ = layers.Dense(best_neuron, activation=\"relu\")(all_features)\n",
    "    output_ = layers.Dense(1, activation=\"linear\")(x_)\n",
    "    model_ = keras.Model(all_inputs, output_)\n",
    "\n",
    "    # compile model\n",
    "    model_.compile(optimizer=opt_, loss='mse', metrics=[r2])\n",
    "\n",
    "    # fit model\n",
    "    history_ = model_.fit(train_ds, epochs=epochs_, validation_data=val_ds, batch_size=batch_, verbose=2)\n",
    "\n",
    "    val_loss_hx = history_.history['val_loss'] # NOTE: You can use RMSE if you find it easier to interpret.\n",
    "    val_loss_min = min(val_loss_hx)\n",
    "    \n",
    "    return val_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MATBo2xhGoII",
    "outputId": "bac4a919-2bcc-4f9a-895e-73aea71a95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ix 0 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten_3/Reshape:0', description=\"created by layer 'flatten_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_4/Reshape:0', description=\"created by layer 'flatten_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_6/truediv:0', description=\"created by layer 'normalization_6'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_5/Reshape:0', description=\"created by layer 'flatten_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_7/truediv:0', description=\"created by layer 'normalization_7'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_8/truediv:0', description=\"created by layer 'normalization_8'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_9/truediv:0', description=\"created by layer 'normalization_9'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_10/truediv:0', description=\"created by layer 'normalization_10'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_11/truediv:0', description=\"created by layer 'normalization_11'\")\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:637: UserWarning: Input dict contained keys ['year', 'full_address', 'nearest_stn'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 - 4s - loss: 75463311360.0000 - r2: -2.1944e+00 - val_loss: 10334036992.0000 - val_r2: 0.5634 - 4s/epoch - 15ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9070600192.0000 - r2: 0.6152 - val_loss: 8755932160.0000 - val_r2: 0.6311 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8159265792.0000 - r2: 0.6524 - val_loss: 8272871424.0000 - val_r2: 0.6506 - 2s/epoch - 9ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7729560576.0000 - r2: 0.6711 - val_loss: 7987307008.0000 - val_r2: 0.6598 - 2s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 7374686208.0000 - r2: 0.6864 - val_loss: 7645968896.0000 - val_r2: 0.6749 - 2s/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7023542784.0000 - r2: 0.7012 - val_loss: 7267692544.0000 - val_r2: 0.6931 - 2s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 6650627072.0000 - r2: 0.7170 - val_loss: 6995680768.0000 - val_r2: 0.7063 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 6278251008.0000 - r2: 0.7322 - val_loss: 6353241088.0000 - val_r2: 0.7333 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 5906667520.0000 - r2: 0.7484 - val_loss: 6109035520.0000 - val_r2: 0.7415 - 2s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5580034048.0000 - r2: 0.7623 - val_loss: 5737296896.0000 - val_r2: 0.7559 - 2s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 5312730112.0000 - r2: 0.7736 - val_loss: 5876413440.0000 - val_r2: 0.7534 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5092592128.0000 - r2: 0.7828 - val_loss: 5417819648.0000 - val_r2: 0.7726 - 2s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 4893444608.0000 - r2: 0.7917 - val_loss: 5371516416.0000 - val_r2: 0.7736 - 2s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 4729188864.0000 - r2: 0.7987 - val_loss: 5117652992.0000 - val_r2: 0.7837 - 2s/epoch - 6ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4588042752.0000 - r2: 0.8041 - val_loss: 5004311040.0000 - val_r2: 0.7895 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4442542592.0000 - r2: 0.8105 - val_loss: 4786113536.0000 - val_r2: 0.7952 - 2s/epoch - 6ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4312296448.0000 - r2: 0.8158 - val_loss: 4770763776.0000 - val_r2: 0.7987 - 2s/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4203961856.0000 - r2: 0.8208 - val_loss: 4628122112.0000 - val_r2: 0.8048 - 2s/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4109423360.0000 - r2: 0.8249 - val_loss: 4480871424.0000 - val_r2: 0.8104 - 2s/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4030290176.0000 - r2: 0.8284 - val_loss: 4576323072.0000 - val_r2: 0.8065 - 2s/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 3963833600.0000 - r2: 0.8304 - val_loss: 4439292416.0000 - val_r2: 0.8125 - 2s/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 3904503808.0000 - r2: 0.8334 - val_loss: 4152207360.0000 - val_r2: 0.8230 - 2s/epoch - 6ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 3870601472.0000 - r2: 0.8339 - val_loss: 4293648640.0000 - val_r2: 0.8183 - 2s/epoch - 6ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 3827263744.0000 - r2: 0.8369 - val_loss: 4193698304.0000 - val_r2: 0.8241 - 2s/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 3793830656.0000 - r2: 0.8382 - val_loss: 4326181888.0000 - val_r2: 0.8173 - 2s/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 3771567104.0000 - r2: 0.8394 - val_loss: 4095108864.0000 - val_r2: 0.8211 - 2s/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 3752579584.0000 - r2: 0.8399 - val_loss: 4078737408.0000 - val_r2: 0.8270 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 3736858880.0000 - r2: 0.8409 - val_loss: 3997658624.0000 - val_r2: 0.8317 - 2s/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 3730664704.0000 - r2: 0.8409 - val_loss: 4156032256.0000 - val_r2: 0.8247 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 3716412160.0000 - r2: 0.8415 - val_loss: 3964270080.0000 - val_r2: 0.8319 - 2s/epoch - 6ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 3714803712.0000 - r2: 0.8417 - val_loss: 4204416768.0000 - val_r2: 0.8222 - 2s/epoch - 6ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 3700104960.0000 - r2: 0.8425 - val_loss: 4421387264.0000 - val_r2: 0.8119 - 2s/epoch - 6ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 3698078720.0000 - r2: 0.8424 - val_loss: 4189641472.0000 - val_r2: 0.8240 - 2s/epoch - 6ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 3691220736.0000 - r2: 0.8426 - val_loss: 4074628608.0000 - val_r2: 0.8286 - 2s/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 3683673600.0000 - r2: 0.8425 - val_loss: 4186754560.0000 - val_r2: 0.8238 - 2s/epoch - 6ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 3675333120.0000 - r2: 0.8433 - val_loss: 4112437248.0000 - val_r2: 0.8267 - 2s/epoch - 6ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 3668487168.0000 - r2: 0.8434 - val_loss: 4060265728.0000 - val_r2: 0.8284 - 2s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 3674457088.0000 - r2: 0.8431 - val_loss: 4007028480.0000 - val_r2: 0.8309 - 2s/epoch - 6ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 1s - loss: 3661414656.0000 - r2: 0.8438 - val_loss: 4244839424.0000 - val_r2: 0.8210 - 1s/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 3665957376.0000 - r2: 0.8438 - val_loss: 4088042496.0000 - val_r2: 0.8274 - 2s/epoch - 6ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 3663949568.0000 - r2: 0.8439 - val_loss: 3955570944.0000 - val_r2: 0.8319 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 3654087936.0000 - r2: 0.8442 - val_loss: 4014439168.0000 - val_r2: 0.8316 - 2s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 3656279296.0000 - r2: 0.8440 - val_loss: 4125615616.0000 - val_r2: 0.8254 - 2s/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 3655608320.0000 - r2: 0.8438 - val_loss: 3972235008.0000 - val_r2: 0.8328 - 2s/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 3649097472.0000 - r2: 0.8438 - val_loss: 4132308992.0000 - val_r2: 0.8244 - 2s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 3650128896.0000 - r2: 0.8443 - val_loss: 3831674112.0000 - val_r2: 0.8379 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 3640115968.0000 - r2: 0.8448 - val_loss: 4022355712.0000 - val_r2: 0.8303 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 3641947904.0000 - r2: 0.8441 - val_loss: 3885324800.0000 - val_r2: 0.8354 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 3638812160.0000 - r2: 0.8451 - val_loss: 4071485952.0000 - val_r2: 0.8259 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 3645053440.0000 - r2: 0.8442 - val_loss: 3946919168.0000 - val_r2: 0.8327 - 2s/epoch - 8ms/step\n",
      "new min loss: len 9, ix 0\n",
      "session cleared!\n",
      "\n",
      "ix 1 i 1\n",
      "updated temp_vec [1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 3s - loss: 89166086144.0000 - r2: -2.8227e+00 - val_loss: 21216251904.0000 - val_r2: 0.1082 - 3s/epoch - 14ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 19296342016.0000 - r2: 0.1810 - val_loss: 19314212864.0000 - val_r2: 0.1857 - 2s/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 18001471488.0000 - r2: 0.2357 - val_loss: 18383065088.0000 - val_r2: 0.2256 - 2s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 17033849856.0000 - r2: 0.2764 - val_loss: 17317877760.0000 - val_r2: 0.2709 - 2s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 15999849472.0000 - r2: 0.3203 - val_loss: 16330056704.0000 - val_r2: 0.3108 - 2s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 14788582400.0000 - r2: 0.3717 - val_loss: 14880686080.0000 - val_r2: 0.3764 - 2s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 13399473152.0000 - r2: 0.4306 - val_loss: 13439207424.0000 - val_r2: 0.4330 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 11863803904.0000 - r2: 0.4955 - val_loss: 11760003072.0000 - val_r2: 0.5066 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 10270941184.0000 - r2: 0.5628 - val_loss: 10222793728.0000 - val_r2: 0.5678 - 2s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 8742329344.0000 - r2: 0.6272 - val_loss: 8673256448.0000 - val_r2: 0.6347 - 2s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 7399312384.0000 - r2: 0.6862 - val_loss: 7394539520.0000 - val_r2: 0.6877 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6315615232.0000 - r2: 0.7319 - val_loss: 6331680256.0000 - val_r2: 0.7321 - 2s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5509211648.0000 - r2: 0.7658 - val_loss: 5682442240.0000 - val_r2: 0.7602 - 2s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 4958358528.0000 - r2: 0.7892 - val_loss: 5101124608.0000 - val_r2: 0.7862 - 2s/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4606236160.0000 - r2: 0.8040 - val_loss: 4700491776.0000 - val_r2: 0.8005 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4399482368.0000 - r2: 0.8128 - val_loss: 4566002176.0000 - val_r2: 0.8082 - 2s/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4301370880.0000 - r2: 0.8165 - val_loss: 4638437888.0000 - val_r2: 0.8009 - 2s/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4259932160.0000 - r2: 0.8186 - val_loss: 4493819904.0000 - val_r2: 0.8107 - 2s/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4236351744.0000 - r2: 0.8197 - val_loss: 4546053632.0000 - val_r2: 0.8087 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4230475776.0000 - r2: 0.8190 - val_loss: 4446683136.0000 - val_r2: 0.8134 - 2s/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4035106304.0000 - r2: 0.8282 - val_loss: 4437403648.0000 - val_r2: 0.8127 - 2s/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 3903581952.0000 - r2: 0.8333 - val_loss: 4484990464.0000 - val_r2: 0.8112 - 2s/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 3854283520.0000 - r2: 0.8356 - val_loss: 4377927680.0000 - val_r2: 0.8141 - 2s/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 3801816832.0000 - r2: 0.8376 - val_loss: 4173672448.0000 - val_r2: 0.8232 - 2s/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 3756134400.0000 - r2: 0.8403 - val_loss: 4239614720.0000 - val_r2: 0.8214 - 2s/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 3696168960.0000 - r2: 0.8421 - val_loss: 4257810944.0000 - val_r2: 0.8204 - 2s/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 3648943104.0000 - r2: 0.8447 - val_loss: 4096230656.0000 - val_r2: 0.8274 - 2s/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 3585213696.0000 - r2: 0.8469 - val_loss: 4108608768.0000 - val_r2: 0.8268 - 2s/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 3514965760.0000 - r2: 0.8501 - val_loss: 3817528064.0000 - val_r2: 0.8378 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 3357359104.0000 - r2: 0.8567 - val_loss: 3846785792.0000 - val_r2: 0.8376 - 2s/epoch - 6ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 3287196416.0000 - r2: 0.8600 - val_loss: 3787695616.0000 - val_r2: 0.8402 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 3256996864.0000 - r2: 0.8608 - val_loss: 3850897152.0000 - val_r2: 0.8384 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 3228338688.0000 - r2: 0.8623 - val_loss: 3782355712.0000 - val_r2: 0.8392 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 3202942976.0000 - r2: 0.8636 - val_loss: 3798066432.0000 - val_r2: 0.8402 - 2s/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 3189159680.0000 - r2: 0.8642 - val_loss: 3685769728.0000 - val_r2: 0.8427 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 3175116032.0000 - r2: 0.8643 - val_loss: 3986602752.0000 - val_r2: 0.8309 - 2s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 3152268544.0000 - r2: 0.8656 - val_loss: 3565502720.0000 - val_r2: 0.8470 - 2s/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 3144375040.0000 - r2: 0.8660 - val_loss: 3492169984.0000 - val_r2: 0.8519 - 2s/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 3135915008.0000 - r2: 0.8662 - val_loss: 3622310144.0000 - val_r2: 0.8481 - 2s/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 3119981312.0000 - r2: 0.8666 - val_loss: 3617745920.0000 - val_r2: 0.8469 - 2s/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 3116148736.0000 - r2: 0.8670 - val_loss: 3948839680.0000 - val_r2: 0.8329 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 3115807744.0000 - r2: 0.8666 - val_loss: 3517970432.0000 - val_r2: 0.8493 - 2s/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 3101297408.0000 - r2: 0.8676 - val_loss: 3711460864.0000 - val_r2: 0.8433 - 2s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 3089864960.0000 - r2: 0.8684 - val_loss: 3636969472.0000 - val_r2: 0.8460 - 2s/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 3078426880.0000 - r2: 0.8691 - val_loss: 3558271744.0000 - val_r2: 0.8495 - 2s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 3064829184.0000 - r2: 0.8692 - val_loss: 3606263296.0000 - val_r2: 0.8484 - 2s/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 3064510720.0000 - r2: 0.8691 - val_loss: 3533261568.0000 - val_r2: 0.8506 - 2s/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 3045867008.0000 - r2: 0.8700 - val_loss: 3640302592.0000 - val_r2: 0.8461 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 3035826176.0000 - r2: 0.8706 - val_loss: 3804218368.0000 - val_r2: 0.8395 - 2s/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 3033533696.0000 - r2: 0.8702 - val_loss: 3606071296.0000 - val_r2: 0.8477 - 2s/epoch - 7ms/step\n",
      "new min loss: len 9, ix 1\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 3s - loss: 76492972032.0000 - r2: -2.2329e+00 - val_loss: 10383537152.0000 - val_r2: 0.5671 - 3s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9243958272.0000 - r2: 0.6065 - val_loss: 8896281600.0000 - val_r2: 0.6231 - 2s/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8333103104.0000 - r2: 0.6452 - val_loss: 8359255040.0000 - val_r2: 0.6468 - 2s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7935688192.0000 - r2: 0.6628 - val_loss: 8115923968.0000 - val_r2: 0.6585 - 2s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 7629105664.0000 - r2: 0.6753 - val_loss: 7717856256.0000 - val_r2: 0.6762 - 2s/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7325635584.0000 - r2: 0.6887 - val_loss: 7576411648.0000 - val_r2: 0.6816 - 2s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7003813888.0000 - r2: 0.7018 - val_loss: 7218499072.0000 - val_r2: 0.6944 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 6664573952.0000 - r2: 0.7160 - val_loss: 6940291072.0000 - val_r2: 0.7076 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 6303515136.0000 - r2: 0.7316 - val_loss: 6412074496.0000 - val_r2: 0.7311 - 2s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5964711936.0000 - r2: 0.7463 - val_loss: 6181252096.0000 - val_r2: 0.7384 - 2s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 5659015168.0000 - r2: 0.7591 - val_loss: 5830332928.0000 - val_r2: 0.7522 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5403531264.0000 - r2: 0.7698 - val_loss: 5714614784.0000 - val_r2: 0.7600 - 2s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5205410304.0000 - r2: 0.7783 - val_loss: 5332272640.0000 - val_r2: 0.7749 - 2s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 5027437056.0000 - r2: 0.7860 - val_loss: 5263833088.0000 - val_r2: 0.7699 - 2s/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4882336256.0000 - r2: 0.7918 - val_loss: 5137687552.0000 - val_r2: 0.7836 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4764039680.0000 - r2: 0.7969 - val_loss: 5110885888.0000 - val_r2: 0.7840 - 2s/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4650032640.0000 - r2: 0.8019 - val_loss: 5067086848.0000 - val_r2: 0.7872 - 2s/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 1s - loss: 4545418752.0000 - r2: 0.8064 - val_loss: 4885575680.0000 - val_r2: 0.7938 - 1s/epoch - 6ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4454481408.0000 - r2: 0.8101 - val_loss: 4926272000.0000 - val_r2: 0.7930 - 2s/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4368681472.0000 - r2: 0.8139 - val_loss: 4549384704.0000 - val_r2: 0.8074 - 2s/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4309683712.0000 - r2: 0.8159 - val_loss: 4665627648.0000 - val_r2: 0.8021 - 2s/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4248708096.0000 - r2: 0.8192 - val_loss: 4708721152.0000 - val_r2: 0.8002 - 2s/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4200830720.0000 - r2: 0.8209 - val_loss: 4488236544.0000 - val_r2: 0.8106 - 2s/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4162370816.0000 - r2: 0.8220 - val_loss: 4499477504.0000 - val_r2: 0.8052 - 2s/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4125347584.0000 - r2: 0.8240 - val_loss: 4408239104.0000 - val_r2: 0.8141 - 2s/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4104669440.0000 - r2: 0.8252 - val_loss: 4454311936.0000 - val_r2: 0.8131 - 2s/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4084249600.0000 - r2: 0.8257 - val_loss: 4360419840.0000 - val_r2: 0.8162 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4072099072.0000 - r2: 0.8266 - val_loss: 4466242560.0000 - val_r2: 0.8117 - 2s/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4060161280.0000 - r2: 0.8269 - val_loss: 4379017216.0000 - val_r2: 0.8150 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4055245312.0000 - r2: 0.8270 - val_loss: 4282937856.0000 - val_r2: 0.8176 - 2s/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4048946688.0000 - r2: 0.8271 - val_loss: 4266523904.0000 - val_r2: 0.8196 - 2s/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4044964352.0000 - r2: 0.8276 - val_loss: 4319704064.0000 - val_r2: 0.8170 - 2s/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4040329984.0000 - r2: 0.8278 - val_loss: 4254918144.0000 - val_r2: 0.8211 - 2s/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4029450240.0000 - r2: 0.8285 - val_loss: 4365715456.0000 - val_r2: 0.8157 - 2s/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4025250560.0000 - r2: 0.8282 - val_loss: 4183386112.0000 - val_r2: 0.8236 - 2s/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4026206720.0000 - r2: 0.8280 - val_loss: 4546568704.0000 - val_r2: 0.8090 - 2s/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4026156032.0000 - r2: 0.8285 - val_loss: 4214643968.0000 - val_r2: 0.8193 - 2s/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4038783488.0000 - r2: 0.8275 - val_loss: 4349834240.0000 - val_r2: 0.8161 - 2s/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4027250688.0000 - r2: 0.8283 - val_loss: 4327761920.0000 - val_r2: 0.8171 - 2s/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 3958166784.0000 - r2: 0.8310 - val_loss: 4153622272.0000 - val_r2: 0.8215 - 2s/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 3822266624.0000 - r2: 0.8368 - val_loss: 4041130752.0000 - val_r2: 0.8287 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 3697694208.0000 - r2: 0.8420 - val_loss: 4056354048.0000 - val_r2: 0.8273 - 2s/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 3595320832.0000 - r2: 0.8469 - val_loss: 3872435200.0000 - val_r2: 0.8355 - 2s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 3494916352.0000 - r2: 0.8509 - val_loss: 3760364800.0000 - val_r2: 0.8407 - 2s/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 3389810688.0000 - r2: 0.8556 - val_loss: 3658000128.0000 - val_r2: 0.8453 - 2s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 3329088512.0000 - r2: 0.8579 - val_loss: 3535012864.0000 - val_r2: 0.8482 - 2s/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 3247629312.0000 - r2: 0.8615 - val_loss: 3510855424.0000 - val_r2: 0.8518 - 2s/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 3170873088.0000 - r2: 0.8649 - val_loss: 3792802560.0000 - val_r2: 0.8389 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 3147610112.0000 - r2: 0.8658 - val_loss: 3650470400.0000 - val_r2: 0.8440 - 2s/epoch - 6ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 3122231808.0000 - r2: 0.8664 - val_loss: 3478057728.0000 - val_r2: 0.8522 - 2s/epoch - 7ms/step\n",
      "new min loss: len 9, ix 2\n",
      "session cleared!\n",
      "\n",
      "ix 3 i 1\n",
      "updated temp_vec [1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 3s - loss: 80541663232.0000 - r2: -2.4280e+00 - val_loss: 11735265280.0000 - val_r2: 0.5083 - 3s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 10750952448.0000 - r2: 0.5435 - val_loss: 10676870144.0000 - val_r2: 0.5506 - 2s/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 10008337408.0000 - r2: 0.5743 - val_loss: 10256952320.0000 - val_r2: 0.5680 - 2s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 9578649600.0000 - r2: 0.5934 - val_loss: 9841258496.0000 - val_r2: 0.5873 - 2s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 9201147904.0000 - r2: 0.6089 - val_loss: 9534200832.0000 - val_r2: 0.5957 - 2s/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 8793227264.0000 - r2: 0.6259 - val_loss: 9024408576.0000 - val_r2: 0.6231 - 2s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 8357597184.0000 - r2: 0.6458 - val_loss: 8753937408.0000 - val_r2: 0.6293 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7885827584.0000 - r2: 0.6648 - val_loss: 8149605376.0000 - val_r2: 0.6559 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 7415826432.0000 - r2: 0.6851 - val_loss: 7866862592.0000 - val_r2: 0.6679 - 2s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6968665088.0000 - r2: 0.7041 - val_loss: 7442454528.0000 - val_r2: 0.6875 - 2s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 6566584832.0000 - r2: 0.7204 - val_loss: 6979521024.0000 - val_r2: 0.7042 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6241295360.0000 - r2: 0.7340 - val_loss: 6743149568.0000 - val_r2: 0.7176 - 2s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5976578560.0000 - r2: 0.7462 - val_loss: 6405928448.0000 - val_r2: 0.7310 - 2s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 5757333504.0000 - r2: 0.7554 - val_loss: 6190540288.0000 - val_r2: 0.7387 - 2s/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5561171456.0000 - r2: 0.7638 - val_loss: 5909436416.0000 - val_r2: 0.7523 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 5386060288.0000 - r2: 0.7712 - val_loss: 5876636160.0000 - val_r2: 0.7528 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 5238131712.0000 - r2: 0.7774 - val_loss: 5790804992.0000 - val_r2: 0.7568 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 5093282816.0000 - r2: 0.7835 - val_loss: 5457660928.0000 - val_r2: 0.7673 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4979910144.0000 - r2: 0.7880 - val_loss: 5611124736.0000 - val_r2: 0.7638 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4866531840.0000 - r2: 0.7926 - val_loss: 5294551552.0000 - val_r2: 0.7771 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4771962368.0000 - r2: 0.7968 - val_loss: 5184052736.0000 - val_r2: 0.7827 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4693541888.0000 - r2: 0.7999 - val_loss: 5160546816.0000 - val_r2: 0.7825 - 2s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4634353664.0000 - r2: 0.8028 - val_loss: 4954309120.0000 - val_r2: 0.7921 - 2s/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4571914240.0000 - r2: 0.8056 - val_loss: 5142205952.0000 - val_r2: 0.7821 - 2s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4528340992.0000 - r2: 0.8071 - val_loss: 4891502080.0000 - val_r2: 0.7937 - 2s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4499385856.0000 - r2: 0.8081 - val_loss: 4824147456.0000 - val_r2: 0.7962 - 2s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4458118656.0000 - r2: 0.8101 - val_loss: 4929188864.0000 - val_r2: 0.7928 - 2s/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4431950848.0000 - r2: 0.8112 - val_loss: 4877921280.0000 - val_r2: 0.7939 - 2s/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4425017344.0000 - r2: 0.8111 - val_loss: 4935370240.0000 - val_r2: 0.7911 - 2s/epoch - 10ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 3s - loss: 4400263168.0000 - r2: 0.8125 - val_loss: 4794929664.0000 - val_r2: 0.7980 - 3s/epoch - 10ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 3s - loss: 4382961664.0000 - r2: 0.8130 - val_loss: 4767413760.0000 - val_r2: 0.7983 - 3s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 3s - loss: 4372424192.0000 - r2: 0.8139 - val_loss: 4818589184.0000 - val_r2: 0.7979 - 3s/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 3s - loss: 4367111680.0000 - r2: 0.8139 - val_loss: 5037935616.0000 - val_r2: 0.7876 - 3s/epoch - 10ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 3s - loss: 4361643008.0000 - r2: 0.8142 - val_loss: 4808264192.0000 - val_r2: 0.7976 - 3s/epoch - 12ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 3s - loss: 4349463552.0000 - r2: 0.8147 - val_loss: 4728970752.0000 - val_r2: 0.8008 - 3s/epoch - 11ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 3s - loss: 4352421888.0000 - r2: 0.8148 - val_loss: 4874020352.0000 - val_r2: 0.7914 - 3s/epoch - 11ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4345296384.0000 - r2: 0.8146 - val_loss: 4692307456.0000 - val_r2: 0.8030 - 2s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 3s - loss: 4332961280.0000 - r2: 0.8155 - val_loss: 4643922432.0000 - val_r2: 0.8046 - 3s/epoch - 10ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4345212416.0000 - r2: 0.8145 - val_loss: 4639512064.0000 - val_r2: 0.8044 - 2s/epoch - 10ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4327688192.0000 - r2: 0.8154 - val_loss: 4809363968.0000 - val_r2: 0.7981 - 2s/epoch - 9ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4327222784.0000 - r2: 0.8158 - val_loss: 4627110400.0000 - val_r2: 0.8036 - 2s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4323715584.0000 - r2: 0.8154 - val_loss: 4758331392.0000 - val_r2: 0.8005 - 2s/epoch - 10ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 3s - loss: 4319257600.0000 - r2: 0.8160 - val_loss: 4851697664.0000 - val_r2: 0.7963 - 3s/epoch - 10ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4320267264.0000 - r2: 0.8159 - val_loss: 4632128512.0000 - val_r2: 0.8044 - 2s/epoch - 9ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4325819904.0000 - r2: 0.8158 - val_loss: 4685087744.0000 - val_r2: 0.8027 - 2s/epoch - 9ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 3s - loss: 4311296512.0000 - r2: 0.8164 - val_loss: 4670247936.0000 - val_r2: 0.8025 - 3s/epoch - 10ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4309150208.0000 - r2: 0.8170 - val_loss: 4654593536.0000 - val_r2: 0.8003 - 2s/epoch - 9ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4301566464.0000 - r2: 0.8164 - val_loss: 4640985600.0000 - val_r2: 0.8037 - 2s/epoch - 10ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4321201664.0000 - r2: 0.8156 - val_loss: 4794821120.0000 - val_r2: 0.7975 - 2s/epoch - 9ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4307627008.0000 - r2: 0.8162 - val_loss: 4753640960.0000 - val_r2: 0.7988 - 2s/epoch - 10ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 6s - loss: 70454992896.0000 - r2: -1.9870e+00 - val_loss: 10045193216.0000 - val_r2: 0.5796 - 6s/epoch - 23ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 3s - loss: 8942090240.0000 - r2: 0.6198 - val_loss: 8709817344.0000 - val_r2: 0.6323 - 3s/epoch - 10ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 3s - loss: 8114574336.0000 - r2: 0.6542 - val_loss: 8272283136.0000 - val_r2: 0.6513 - 3s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7699982848.0000 - r2: 0.6725 - val_loss: 7836861440.0000 - val_r2: 0.6705 - 2s/epoch - 10ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 3s - loss: 7348218368.0000 - r2: 0.6877 - val_loss: 7564677120.0000 - val_r2: 0.6794 - 3s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 3s - loss: 6978975232.0000 - r2: 0.7029 - val_loss: 7180840448.0000 - val_r2: 0.6970 - 3s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 3s - loss: 6605377536.0000 - r2: 0.7194 - val_loss: 6804038656.0000 - val_r2: 0.7137 - 3s/epoch - 10ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 3s - loss: 6228009472.0000 - r2: 0.7345 - val_loss: 6461150208.0000 - val_r2: 0.7259 - 3s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 3s - loss: 5890794496.0000 - r2: 0.7497 - val_loss: 6225861120.0000 - val_r2: 0.7365 - 3s/epoch - 10ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5602419712.0000 - r2: 0.7608 - val_loss: 5967303168.0000 - val_r2: 0.7454 - 2s/epoch - 10ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 5378743808.0000 - r2: 0.7705 - val_loss: 5801987584.0000 - val_r2: 0.7566 - 2s/epoch - 10ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5204800000.0000 - r2: 0.7783 - val_loss: 6050684416.0000 - val_r2: 0.7453 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5052360192.0000 - r2: 0.7845 - val_loss: 5837550080.0000 - val_r2: 0.7548 - 2s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 4914213888.0000 - r2: 0.7906 - val_loss: 5822779392.0000 - val_r2: 0.7552 - 2s/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4803341312.0000 - r2: 0.7955 - val_loss: 5676314624.0000 - val_r2: 0.7593 - 2s/epoch - 10ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4693983232.0000 - r2: 0.8004 - val_loss: 5550682112.0000 - val_r2: 0.7559 - 2s/epoch - 10ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 3s - loss: 4597550080.0000 - r2: 0.8038 - val_loss: 5328402432.0000 - val_r2: 0.7748 - 3s/epoch - 10ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4520090624.0000 - r2: 0.8074 - val_loss: 5108496384.0000 - val_r2: 0.7830 - 2s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4455903232.0000 - r2: 0.8103 - val_loss: 5060128768.0000 - val_r2: 0.7862 - 2s/epoch - 10ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4399756800.0000 - r2: 0.8125 - val_loss: 5367785984.0000 - val_r2: 0.7750 - 2s/epoch - 10ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4346540544.0000 - r2: 0.8144 - val_loss: 4959117312.0000 - val_r2: 0.7899 - 2s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4314758144.0000 - r2: 0.8157 - val_loss: 5252436480.0000 - val_r2: 0.7776 - 2s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4294222592.0000 - r2: 0.8169 - val_loss: 5129316352.0000 - val_r2: 0.7813 - 2s/epoch - 10ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4266921728.0000 - r2: 0.8182 - val_loss: 5101763072.0000 - val_r2: 0.7833 - 2s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4254233600.0000 - r2: 0.8186 - val_loss: 5288134656.0000 - val_r2: 0.7748 - 2s/epoch - 6ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4258109440.0000 - r2: 0.8184 - val_loss: 4898845184.0000 - val_r2: 0.7937 - 2s/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4229680896.0000 - r2: 0.8197 - val_loss: 4955226624.0000 - val_r2: 0.7917 - 2s/epoch - 6ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4227723520.0000 - r2: 0.8197 - val_loss: 5084322816.0000 - val_r2: 0.7846 - 2s/epoch - 6ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4226651392.0000 - r2: 0.8197 - val_loss: 5111822336.0000 - val_r2: 0.7840 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4218503168.0000 - r2: 0.8203 - val_loss: 5017738752.0000 - val_r2: 0.7854 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 3s - loss: 4226813440.0000 - r2: 0.8199 - val_loss: 5201670144.0000 - val_r2: 0.7778 - 3s/epoch - 10ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 3s - loss: 4228139520.0000 - r2: 0.8197 - val_loss: 5232122368.0000 - val_r2: 0.7803 - 3s/epoch - 11ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 3s - loss: 4226864640.0000 - r2: 0.8199 - val_loss: 5105165312.0000 - val_r2: 0.7822 - 3s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 3s - loss: 4234191872.0000 - r2: 0.8192 - val_loss: 4959576576.0000 - val_r2: 0.7917 - 3s/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 3s - loss: 4221089280.0000 - r2: 0.8201 - val_loss: 5178758656.0000 - val_r2: 0.7817 - 3s/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 3s - loss: 4219584768.0000 - r2: 0.8197 - val_loss: 5017480704.0000 - val_r2: 0.7881 - 3s/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 3s - loss: 4221882880.0000 - r2: 0.8198 - val_loss: 5253142016.0000 - val_r2: 0.7750 - 3s/epoch - 11ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 3s - loss: 4215961088.0000 - r2: 0.8201 - val_loss: 5027213824.0000 - val_r2: 0.7876 - 3s/epoch - 11ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 3s - loss: 4225884672.0000 - r2: 0.8195 - val_loss: 5019784192.0000 - val_r2: 0.7881 - 3s/epoch - 11ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 3s - loss: 4219887616.0000 - r2: 0.8200 - val_loss: 5165300736.0000 - val_r2: 0.7830 - 3s/epoch - 11ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 3s - loss: 4219454976.0000 - r2: 0.8200 - val_loss: 5075402752.0000 - val_r2: 0.7842 - 3s/epoch - 11ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 3s - loss: 4215265024.0000 - r2: 0.8203 - val_loss: 5042650112.0000 - val_r2: 0.7870 - 3s/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 3s - loss: 4221463808.0000 - r2: 0.8199 - val_loss: 5156483584.0000 - val_r2: 0.7833 - 3s/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 3s - loss: 4218393600.0000 - r2: 0.8201 - val_loss: 4984733696.0000 - val_r2: 0.7891 - 3s/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 3s - loss: 4213340160.0000 - r2: 0.8206 - val_loss: 5063129600.0000 - val_r2: 0.7822 - 3s/epoch - 11ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 3s - loss: 4213734144.0000 - r2: 0.8200 - val_loss: 4937868800.0000 - val_r2: 0.7893 - 3s/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 3s - loss: 4222620160.0000 - r2: 0.8199 - val_loss: 5225478656.0000 - val_r2: 0.7785 - 3s/epoch - 11ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 3s - loss: 4218796544.0000 - r2: 0.8202 - val_loss: 5095293440.0000 - val_r2: 0.7837 - 3s/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 3s - loss: 4214870528.0000 - r2: 0.8202 - val_loss: 5139191296.0000 - val_r2: 0.7828 - 3s/epoch - 11ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4216091136.0000 - r2: 0.8197 - val_loss: 5020021248.0000 - val_r2: 0.7886 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 7s - loss: 75278974976.0000 - r2: -2.2522e+00 - val_loss: 10437273600.0000 - val_r2: 0.5633 - 7s/epoch - 29ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 3s - loss: 9323737088.0000 - r2: 0.6037 - val_loss: 8971197440.0000 - val_r2: 0.6181 - 3s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 3s - loss: 8453736960.0000 - r2: 0.6401 - val_loss: 8585815552.0000 - val_r2: 0.6342 - 3s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 3s - loss: 8097155072.0000 - r2: 0.6548 - val_loss: 8227363840.0000 - val_r2: 0.6533 - 3s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 3s - loss: 7835229184.0000 - r2: 0.6665 - val_loss: 7973347328.0000 - val_r2: 0.6600 - 3s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 3s - loss: 7574577152.0000 - r2: 0.6774 - val_loss: 7621582848.0000 - val_r2: 0.6802 - 3s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 3s - loss: 7292272128.0000 - r2: 0.6896 - val_loss: 7288675328.0000 - val_r2: 0.6952 - 3s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 3s - loss: 6979058176.0000 - r2: 0.7031 - val_loss: 7058051584.0000 - val_r2: 0.7031 - 3s/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 6634529792.0000 - r2: 0.7178 - val_loss: 6813435392.0000 - val_r2: 0.7102 - 2s/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6271779840.0000 - r2: 0.7335 - val_loss: 6452215296.0000 - val_r2: 0.7256 - 2s/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 3s - loss: 5895377408.0000 - r2: 0.7481 - val_loss: 6176794112.0000 - val_r2: 0.7377 - 3s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 3s - loss: 5549007360.0000 - r2: 0.7639 - val_loss: 5742365696.0000 - val_r2: 0.7560 - 3s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 3s - loss: 5234189824.0000 - r2: 0.7770 - val_loss: 5364691456.0000 - val_r2: 0.7735 - 3s/epoch - 11ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 3s - loss: 4956420608.0000 - r2: 0.7889 - val_loss: 5110192640.0000 - val_r2: 0.7857 - 3s/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 3s - loss: 4721033216.0000 - r2: 0.7989 - val_loss: 5051424256.0000 - val_r2: 0.7869 - 3s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 3s - loss: 4506652672.0000 - r2: 0.8078 - val_loss: 4876048384.0000 - val_r2: 0.7920 - 3s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 3s - loss: 4334358528.0000 - r2: 0.8159 - val_loss: 4846970368.0000 - val_r2: 0.7939 - 3s/epoch - 13ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 3s - loss: 4187237632.0000 - r2: 0.8216 - val_loss: 4629182976.0000 - val_r2: 0.8047 - 3s/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 3s - loss: 4068768000.0000 - r2: 0.8262 - val_loss: 4513196032.0000 - val_r2: 0.8097 - 3s/epoch - 13ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 3s - loss: 3980411648.0000 - r2: 0.8302 - val_loss: 4343436288.0000 - val_r2: 0.8168 - 3s/epoch - 13ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 3s - loss: 3908139264.0000 - r2: 0.8333 - val_loss: 4284203520.0000 - val_r2: 0.8189 - 3s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 3s - loss: 3872740352.0000 - r2: 0.8351 - val_loss: 4549230080.0000 - val_r2: 0.8069 - 3s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 3s - loss: 3843970304.0000 - r2: 0.8364 - val_loss: 4196137216.0000 - val_r2: 0.8223 - 3s/epoch - 13ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 3s - loss: 3814059776.0000 - r2: 0.8371 - val_loss: 4208377088.0000 - val_r2: 0.8214 - 3s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 4s - loss: 3807850496.0000 - r2: 0.8375 - val_loss: 4137486848.0000 - val_r2: 0.8265 - 4s/epoch - 15ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 3s - loss: 3782311936.0000 - r2: 0.8387 - val_loss: 4107533312.0000 - val_r2: 0.8248 - 3s/epoch - 12ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 3s - loss: 3777313792.0000 - r2: 0.8390 - val_loss: 3929352704.0000 - val_r2: 0.8325 - 3s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 3s - loss: 3766227456.0000 - r2: 0.8394 - val_loss: 4081093888.0000 - val_r2: 0.8275 - 3s/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 3s - loss: 3768159488.0000 - r2: 0.8393 - val_loss: 4273051648.0000 - val_r2: 0.8189 - 3s/epoch - 10ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 3741513216.0000 - r2: 0.8402 - val_loss: 4076036864.0000 - val_r2: 0.8284 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 3739844608.0000 - r2: 0.8403 - val_loss: 4130285568.0000 - val_r2: 0.8263 - 2s/epoch - 10ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 3738530816.0000 - r2: 0.8403 - val_loss: 4016915200.0000 - val_r2: 0.8298 - 2s/epoch - 10ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 3s - loss: 3726499072.0000 - r2: 0.8411 - val_loss: 4123347968.0000 - val_r2: 0.8258 - 3s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 3s - loss: 3733000448.0000 - r2: 0.8406 - val_loss: 4366816256.0000 - val_r2: 0.8142 - 3s/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 3s - loss: 3716235520.0000 - r2: 0.8415 - val_loss: 3939923968.0000 - val_r2: 0.8330 - 3s/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 3s - loss: 3713886208.0000 - r2: 0.8417 - val_loss: 4091717120.0000 - val_r2: 0.8280 - 3s/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 3s - loss: 3704291584.0000 - r2: 0.8422 - val_loss: 4024703232.0000 - val_r2: 0.8304 - 3s/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 3s - loss: 3697554944.0000 - r2: 0.8418 - val_loss: 3973238016.0000 - val_r2: 0.8321 - 3s/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 3s - loss: 3696361984.0000 - r2: 0.8420 - val_loss: 3974372352.0000 - val_r2: 0.8316 - 3s/epoch - 11ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 3s - loss: 3693766656.0000 - r2: 0.8424 - val_loss: 3874243584.0000 - val_r2: 0.8369 - 3s/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 3s - loss: 3691389696.0000 - r2: 0.8428 - val_loss: 4016734464.0000 - val_r2: 0.8295 - 3s/epoch - 11ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 3s - loss: 3673819136.0000 - r2: 0.8432 - val_loss: 3847911936.0000 - val_r2: 0.8377 - 3s/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 3s - loss: 3679721728.0000 - r2: 0.8431 - val_loss: 3976975872.0000 - val_r2: 0.8323 - 3s/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 3s - loss: 3670477824.0000 - r2: 0.8433 - val_loss: 4026987264.0000 - val_r2: 0.8304 - 3s/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 3s - loss: 3675436288.0000 - r2: 0.8434 - val_loss: 4003921920.0000 - val_r2: 0.8299 - 3s/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 3s - loss: 3670216960.0000 - r2: 0.8432 - val_loss: 4053915648.0000 - val_r2: 0.8297 - 3s/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 3s - loss: 3669971712.0000 - r2: 0.8432 - val_loss: 3932652800.0000 - val_r2: 0.8346 - 3s/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 3s - loss: 3670768384.0000 - r2: 0.8432 - val_loss: 3907921920.0000 - val_r2: 0.8345 - 3s/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 3s - loss: 3660100352.0000 - r2: 0.8436 - val_loss: 3954909440.0000 - val_r2: 0.8331 - 3s/epoch - 13ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 3s - loss: 3654113280.0000 - r2: 0.8445 - val_loss: 3903156736.0000 - val_r2: 0.8355 - 3s/epoch - 11ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 6 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 8s - loss: 79176359936.0000 - r2: -2.3829e+00 - val_loss: 10724567040.0000 - val_r2: 0.5474 - 8s/epoch - 31ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 4s - loss: 9509441536.0000 - r2: 0.5956 - val_loss: 9125067776.0000 - val_r2: 0.6173 - 4s/epoch - 14ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 3s - loss: 8558686720.0000 - r2: 0.6356 - val_loss: 8638312448.0000 - val_r2: 0.6357 - 3s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 8200486400.0000 - r2: 0.6514 - val_loss: 8337978368.0000 - val_r2: 0.6479 - 2s/epoch - 10ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 3s - loss: 7952011776.0000 - r2: 0.6614 - val_loss: 8139888640.0000 - val_r2: 0.6559 - 3s/epoch - 10ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 3s - loss: 7717857792.0000 - r2: 0.6714 - val_loss: 8000309760.0000 - val_r2: 0.6615 - 3s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 3s - loss: 7477265920.0000 - r2: 0.6819 - val_loss: 7588434944.0000 - val_r2: 0.6795 - 3s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 3s - loss: 7217642496.0000 - r2: 0.6921 - val_loss: 7481375232.0000 - val_r2: 0.6793 - 3s/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 3s - loss: 6923739648.0000 - r2: 0.7059 - val_loss: 7033920512.0000 - val_r2: 0.7024 - 3s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 3s - loss: 6619884032.0000 - r2: 0.7183 - val_loss: 6808576000.0000 - val_r2: 0.7114 - 3s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 3s - loss: 6294148096.0000 - r2: 0.7319 - val_loss: 6491394560.0000 - val_r2: 0.7262 - 3s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 3s - loss: 5977776128.0000 - r2: 0.7456 - val_loss: 6140189696.0000 - val_r2: 0.7412 - 3s/epoch - 13ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 3s - loss: 5682330624.0000 - r2: 0.7579 - val_loss: 6088042496.0000 - val_r2: 0.7432 - 3s/epoch - 13ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 3s - loss: 5432123392.0000 - r2: 0.7688 - val_loss: 5804847616.0000 - val_r2: 0.7537 - 3s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5208629760.0000 - r2: 0.7783 - val_loss: 5336676864.0000 - val_r2: 0.7728 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 5038581760.0000 - r2: 0.7851 - val_loss: 5263915520.0000 - val_r2: 0.7774 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4883962880.0000 - r2: 0.7920 - val_loss: 4980572160.0000 - val_r2: 0.7908 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4751757824.0000 - r2: 0.7974 - val_loss: 5163403776.0000 - val_r2: 0.7821 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4626933248.0000 - r2: 0.8029 - val_loss: 4857515520.0000 - val_r2: 0.7946 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4515995136.0000 - r2: 0.8075 - val_loss: 4747322368.0000 - val_r2: 0.8002 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4410435072.0000 - r2: 0.8121 - val_loss: 4692549120.0000 - val_r2: 0.8021 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4327231488.0000 - r2: 0.8155 - val_loss: 4466528768.0000 - val_r2: 0.8129 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4250979840.0000 - r2: 0.8184 - val_loss: 4536880640.0000 - val_r2: 0.8076 - 2s/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4175444992.0000 - r2: 0.8219 - val_loss: 4567448576.0000 - val_r2: 0.8058 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4117261824.0000 - r2: 0.8244 - val_loss: 4279248640.0000 - val_r2: 0.8191 - 2s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4069829376.0000 - r2: 0.8264 - val_loss: 4395547648.0000 - val_r2: 0.8145 - 2s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4016120320.0000 - r2: 0.8285 - val_loss: 4322819072.0000 - val_r2: 0.8154 - 2s/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 3988074752.0000 - r2: 0.8292 - val_loss: 4440896512.0000 - val_r2: 0.8135 - 2s/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 3958886656.0000 - r2: 0.8313 - val_loss: 4205120768.0000 - val_r2: 0.8201 - 2s/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 3928639232.0000 - r2: 0.8325 - val_loss: 4636838912.0000 - val_r2: 0.8042 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 3909817344.0000 - r2: 0.8331 - val_loss: 4046118912.0000 - val_r2: 0.8284 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 3890863104.0000 - r2: 0.8341 - val_loss: 4152352000.0000 - val_r2: 0.8223 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 3883509760.0000 - r2: 0.8338 - val_loss: 4089332224.0000 - val_r2: 0.8251 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 3867870976.0000 - r2: 0.8356 - val_loss: 4100106496.0000 - val_r2: 0.8266 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 3856329216.0000 - r2: 0.8354 - val_loss: 4089484800.0000 - val_r2: 0.8270 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 3849730560.0000 - r2: 0.8354 - val_loss: 4065190656.0000 - val_r2: 0.8289 - 2s/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 3838841088.0000 - r2: 0.8363 - val_loss: 4633157632.0000 - val_r2: 0.8046 - 2s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 3836055040.0000 - r2: 0.8365 - val_loss: 4255874816.0000 - val_r2: 0.8206 - 2s/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 3834978304.0000 - r2: 0.8364 - val_loss: 4208435712.0000 - val_r2: 0.8212 - 2s/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 3821734144.0000 - r2: 0.8370 - val_loss: 4013869312.0000 - val_r2: 0.8309 - 2s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 3816172032.0000 - r2: 0.8374 - val_loss: 4129834752.0000 - val_r2: 0.8258 - 2s/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 3820267008.0000 - r2: 0.8372 - val_loss: 4085580288.0000 - val_r2: 0.8270 - 2s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 3807877120.0000 - r2: 0.8375 - val_loss: 4219233792.0000 - val_r2: 0.8214 - 2s/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 3803830016.0000 - r2: 0.8374 - val_loss: 3988491776.0000 - val_r2: 0.8314 - 2s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 3802371584.0000 - r2: 0.8381 - val_loss: 3996977920.0000 - val_r2: 0.8318 - 2s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 3807005440.0000 - r2: 0.8376 - val_loss: 4139056128.0000 - val_r2: 0.8260 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 3794390016.0000 - r2: 0.8382 - val_loss: 4083803392.0000 - val_r2: 0.8276 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 3798018560.0000 - r2: 0.8381 - val_loss: 4131508736.0000 - val_r2: 0.8258 - 2s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 3791512064.0000 - r2: 0.8378 - val_loss: 4144164864.0000 - val_r2: 0.8246 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 3789680640.0000 - r2: 0.8384 - val_loss: 4033996032.0000 - val_r2: 0.8278 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 7 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 5s - loss: 73025560576.0000 - r2: -2.0880e+00 - val_loss: 10409491456.0000 - val_r2: 0.5641 - 5s/epoch - 19ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9354497024.0000 - r2: 0.6023 - val_loss: 9186583552.0000 - val_r2: 0.6096 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8575735296.0000 - r2: 0.6342 - val_loss: 8626342912.0000 - val_r2: 0.6377 - 2s/epoch - 9ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 8286854656.0000 - r2: 0.6469 - val_loss: 8383625216.0000 - val_r2: 0.6454 - 2s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 8094983168.0000 - r2: 0.6555 - val_loss: 8161261056.0000 - val_r2: 0.6586 - 2s/epoch - 9ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7917715456.0000 - r2: 0.6634 - val_loss: 8020946432.0000 - val_r2: 0.6638 - 2s/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7740974080.0000 - r2: 0.6703 - val_loss: 7941567488.0000 - val_r2: 0.6643 - 2s/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7550535168.0000 - r2: 0.6779 - val_loss: 7702578688.0000 - val_r2: 0.6750 - 2s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 7390620160.0000 - r2: 0.6855 - val_loss: 7633232384.0000 - val_r2: 0.6768 - 2s/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 7221188096.0000 - r2: 0.6929 - val_loss: 7460091904.0000 - val_r2: 0.6880 - 2s/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 7071242752.0000 - r2: 0.6989 - val_loss: 7188652544.0000 - val_r2: 0.6982 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6947756544.0000 - r2: 0.7037 - val_loss: 7210438144.0000 - val_r2: 0.6973 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 6868101120.0000 - r2: 0.7075 - val_loss: 6929219072.0000 - val_r2: 0.7076 - 2s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 6799894016.0000 - r2: 0.7102 - val_loss: 6944107008.0000 - val_r2: 0.7022 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 6755500032.0000 - r2: 0.7122 - val_loss: 6888517632.0000 - val_r2: 0.7086 - 2s/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 6731152384.0000 - r2: 0.7136 - val_loss: 6752821248.0000 - val_r2: 0.7171 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 6712870912.0000 - r2: 0.7141 - val_loss: 7102846464.0000 - val_r2: 0.7019 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 6691547136.0000 - r2: 0.7151 - val_loss: 6805137408.0000 - val_r2: 0.7128 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 6679077376.0000 - r2: 0.7158 - val_loss: 6787206144.0000 - val_r2: 0.7133 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 6684093952.0000 - r2: 0.7146 - val_loss: 6803898880.0000 - val_r2: 0.7149 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 6665608704.0000 - r2: 0.7164 - val_loss: 7124576768.0000 - val_r2: 0.6991 - 2s/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 6653308928.0000 - r2: 0.7165 - val_loss: 6752584192.0000 - val_r2: 0.7145 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 6636320768.0000 - r2: 0.7172 - val_loss: 6797315584.0000 - val_r2: 0.7142 - 2s/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 6632135168.0000 - r2: 0.7167 - val_loss: 6903757312.0000 - val_r2: 0.7087 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 6634396672.0000 - r2: 0.7163 - val_loss: 6733314560.0000 - val_r2: 0.7151 - 2s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 6628122624.0000 - r2: 0.7173 - val_loss: 6906144768.0000 - val_r2: 0.7069 - 2s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 6628822016.0000 - r2: 0.7173 - val_loss: 6918947328.0000 - val_r2: 0.7094 - 2s/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 6606008832.0000 - r2: 0.7184 - val_loss: 6563624448.0000 - val_r2: 0.7236 - 2s/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 6587489792.0000 - r2: 0.7201 - val_loss: 6670672384.0000 - val_r2: 0.7197 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 6597095936.0000 - r2: 0.7186 - val_loss: 6975216128.0000 - val_r2: 0.7043 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 6596405248.0000 - r2: 0.7186 - val_loss: 6707072000.0000 - val_r2: 0.7184 - 2s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 6583094272.0000 - r2: 0.7199 - val_loss: 6844728320.0000 - val_r2: 0.7122 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 6585878016.0000 - r2: 0.7189 - val_loss: 6913765888.0000 - val_r2: 0.7065 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 6571046400.0000 - r2: 0.7199 - val_loss: 6732895232.0000 - val_r2: 0.7184 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 6569558016.0000 - r2: 0.7196 - val_loss: 6724836864.0000 - val_r2: 0.7176 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 6553527808.0000 - r2: 0.7207 - val_loss: 6854339584.0000 - val_r2: 0.7097 - 2s/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 6556968960.0000 - r2: 0.7202 - val_loss: 6681563648.0000 - val_r2: 0.7173 - 2s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 6559061504.0000 - r2: 0.7201 - val_loss: 7011292160.0000 - val_r2: 0.7019 - 2s/epoch - 6ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 6546919424.0000 - r2: 0.7201 - val_loss: 6588571648.0000 - val_r2: 0.7235 - 2s/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 6542369280.0000 - r2: 0.7209 - val_loss: 6638896128.0000 - val_r2: 0.7187 - 2s/epoch - 6ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 6523838464.0000 - r2: 0.7226 - val_loss: 6615809024.0000 - val_r2: 0.7205 - 2s/epoch - 6ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 6517882880.0000 - r2: 0.7224 - val_loss: 6741834752.0000 - val_r2: 0.7128 - 2s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 6520277504.0000 - r2: 0.7217 - val_loss: 6782347776.0000 - val_r2: 0.7154 - 2s/epoch - 6ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 6510552576.0000 - r2: 0.7220 - val_loss: 6706971648.0000 - val_r2: 0.7171 - 2s/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 6515176448.0000 - r2: 0.7223 - val_loss: 6549940736.0000 - val_r2: 0.7236 - 2s/epoch - 6ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 6508867072.0000 - r2: 0.7231 - val_loss: 7064765952.0000 - val_r2: 0.7001 - 2s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 6520312832.0000 - r2: 0.7222 - val_loss: 6509611520.0000 - val_r2: 0.7275 - 2s/epoch - 6ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 6499525632.0000 - r2: 0.7229 - val_loss: 6789541376.0000 - val_r2: 0.7138 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 6503392256.0000 - r2: 0.7219 - val_loss: 6490211840.0000 - val_r2: 0.7244 - 2s/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 6494846464.0000 - r2: 0.7232 - val_loss: 6585797632.0000 - val_r2: 0.7230 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 4s - loss: 69640470528.0000 - r2: -1.9541e+00 - val_loss: 10202206208.0000 - val_r2: 0.5690 - 4s/epoch - 16ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9078207488.0000 - r2: 0.6144 - val_loss: 8796843008.0000 - val_r2: 0.6229 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8311735808.0000 - r2: 0.6463 - val_loss: 8515409920.0000 - val_r2: 0.6419 - 2s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7967390208.0000 - r2: 0.6611 - val_loss: 8030584832.0000 - val_r2: 0.6638 - 2s/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 7687518720.0000 - r2: 0.6730 - val_loss: 7804817920.0000 - val_r2: 0.6669 - 2s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7395385344.0000 - r2: 0.6851 - val_loss: 7583836160.0000 - val_r2: 0.6813 - 2s/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7092841984.0000 - r2: 0.6981 - val_loss: 7276151808.0000 - val_r2: 0.6947 - 2s/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 6776334336.0000 - r2: 0.7113 - val_loss: 6921866240.0000 - val_r2: 0.7086 - 2s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 6466490880.0000 - r2: 0.7250 - val_loss: 6600955392.0000 - val_r2: 0.7216 - 2s/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6164214272.0000 - r2: 0.7373 - val_loss: 6331944960.0000 - val_r2: 0.7328 - 2s/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 5915201536.0000 - r2: 0.7480 - val_loss: 6100453376.0000 - val_r2: 0.7433 - 2s/epoch - 8ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5684493824.0000 - r2: 0.7579 - val_loss: 5897432064.0000 - val_r2: 0.7528 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5494202368.0000 - r2: 0.7660 - val_loss: 5617479168.0000 - val_r2: 0.7631 - 2s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 5314975232.0000 - r2: 0.7733 - val_loss: 5591469568.0000 - val_r2: 0.7638 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5167231488.0000 - r2: 0.7801 - val_loss: 5366384128.0000 - val_r2: 0.7744 - 2s/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 5020343808.0000 - r2: 0.7862 - val_loss: 5296825856.0000 - val_r2: 0.7746 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4889702912.0000 - r2: 0.7916 - val_loss: 5184605696.0000 - val_r2: 0.7815 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4789471744.0000 - r2: 0.7955 - val_loss: 4979376128.0000 - val_r2: 0.7855 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4690745856.0000 - r2: 0.7997 - val_loss: 4959038976.0000 - val_r2: 0.7906 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4627419136.0000 - r2: 0.8025 - val_loss: 4992369152.0000 - val_r2: 0.7893 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4562484736.0000 - r2: 0.8052 - val_loss: 4754545152.0000 - val_r2: 0.7960 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4506345472.0000 - r2: 0.8080 - val_loss: 4706118656.0000 - val_r2: 0.7999 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4467450880.0000 - r2: 0.8094 - val_loss: 4700760064.0000 - val_r2: 0.8018 - 2s/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4443228160.0000 - r2: 0.8106 - val_loss: 4670978048.0000 - val_r2: 0.8028 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 3s - loss: 4421279232.0000 - r2: 0.8113 - val_loss: 4647627264.0000 - val_r2: 0.8028 - 3s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4391342592.0000 - r2: 0.8129 - val_loss: 4843449856.0000 - val_r2: 0.7951 - 2s/epoch - 10ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4377894400.0000 - r2: 0.8134 - val_loss: 4766342656.0000 - val_r2: 0.7999 - 2s/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4368403456.0000 - r2: 0.8134 - val_loss: 4594213376.0000 - val_r2: 0.8066 - 2s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4351472128.0000 - r2: 0.8144 - val_loss: 4607721472.0000 - val_r2: 0.8056 - 2s/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4341275136.0000 - r2: 0.8146 - val_loss: 4836570112.0000 - val_r2: 0.7959 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4340632064.0000 - r2: 0.8149 - val_loss: 5019337216.0000 - val_r2: 0.7862 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4337342464.0000 - r2: 0.8147 - val_loss: 4538273280.0000 - val_r2: 0.8084 - 2s/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4325539328.0000 - r2: 0.8155 - val_loss: 4537781248.0000 - val_r2: 0.8074 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4321446400.0000 - r2: 0.8157 - val_loss: 4596442112.0000 - val_r2: 0.8054 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4317785088.0000 - r2: 0.8154 - val_loss: 4534438400.0000 - val_r2: 0.8073 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4315886592.0000 - r2: 0.8155 - val_loss: 4767132672.0000 - val_r2: 0.7995 - 2s/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4323378176.0000 - r2: 0.8156 - val_loss: 4511989248.0000 - val_r2: 0.8077 - 2s/epoch - 8ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4303304192.0000 - r2: 0.8163 - val_loss: 4785691648.0000 - val_r2: 0.7973 - 2s/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4307586048.0000 - r2: 0.8161 - val_loss: 4524624896.0000 - val_r2: 0.8087 - 2s/epoch - 10ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4311407616.0000 - r2: 0.8160 - val_loss: 4621466112.0000 - val_r2: 0.8051 - 2s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4295592448.0000 - r2: 0.8166 - val_loss: 4555543552.0000 - val_r2: 0.8079 - 2s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4301984256.0000 - r2: 0.8163 - val_loss: 4653914112.0000 - val_r2: 0.8031 - 2s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 4289592064.0000 - r2: 0.8169 - val_loss: 4525125120.0000 - val_r2: 0.8093 - 2s/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4286132224.0000 - r2: 0.8173 - val_loss: 4452325376.0000 - val_r2: 0.8114 - 2s/epoch - 9ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4290405632.0000 - r2: 0.8172 - val_loss: 4594632192.0000 - val_r2: 0.8044 - 2s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 4296806400.0000 - r2: 0.8169 - val_loss: 4435342336.0000 - val_r2: 0.8122 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4291814912.0000 - r2: 0.8171 - val_loss: 4660758016.0000 - val_r2: 0.8031 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4288224256.0000 - r2: 0.8172 - val_loss: 4519443968.0000 - val_r2: 0.8090 - 2s/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4280361216.0000 - r2: 0.8175 - val_loss: 4550287872.0000 - val_r2: 0.8079 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4286399232.0000 - r2: 0.8168 - val_loss: 4468993024.0000 - val_r2: 0.8123 - 2s/epoch - 9ms/step\n",
      "session cleared!\n",
      "\n",
      "1153.5946817398071 seconds elapsed\n",
      "\n",
      "vec [1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "ix 0 i 1\n",
      "updated temp_vec [0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 4s - loss: 68785774592.0000 - r2: -1.9624e+00 - val_loss: 9839817728.0000 - val_r2: 0.5864 - 4s/epoch - 16ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 8700904448.0000 - r2: 0.6296 - val_loss: 8483782656.0000 - val_r2: 0.6451 - 2s/epoch - 9ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 7874932736.0000 - r2: 0.6650 - val_loss: 7909292544.0000 - val_r2: 0.6606 - 2s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7405230592.0000 - r2: 0.6844 - val_loss: 7547045376.0000 - val_r2: 0.6835 - 2s/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 6978780672.0000 - r2: 0.7030 - val_loss: 6980230144.0000 - val_r2: 0.7033 - 2s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 6545062400.0000 - r2: 0.7216 - val_loss: 6705975296.0000 - val_r2: 0.7177 - 2s/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 6112261120.0000 - r2: 0.7393 - val_loss: 6329141248.0000 - val_r2: 0.7349 - 2s/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 3s - loss: 5722168832.0000 - r2: 0.7562 - val_loss: 6061186560.0000 - val_r2: 0.7443 - 3s/epoch - 10ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 5407330816.0000 - r2: 0.7699 - val_loss: 5664359424.0000 - val_r2: 0.7614 - 2s/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5162357760.0000 - r2: 0.7802 - val_loss: 5429393408.0000 - val_r2: 0.7717 - 2s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 4966293504.0000 - r2: 0.7887 - val_loss: 5399344640.0000 - val_r2: 0.7716 - 2s/epoch - 10ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 4781444096.0000 - r2: 0.7966 - val_loss: 5337696768.0000 - val_r2: 0.7736 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 4644717056.0000 - r2: 0.8023 - val_loss: 5080108544.0000 - val_r2: 0.7867 - 2s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 4515499520.0000 - r2: 0.8073 - val_loss: 4747019264.0000 - val_r2: 0.7995 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4411546112.0000 - r2: 0.8119 - val_loss: 4810320896.0000 - val_r2: 0.7981 - 2s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4310163456.0000 - r2: 0.8164 - val_loss: 4801390080.0000 - val_r2: 0.7970 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4229589504.0000 - r2: 0.8196 - val_loss: 4691877376.0000 - val_r2: 0.8031 - 2s/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4174905344.0000 - r2: 0.8221 - val_loss: 4605158912.0000 - val_r2: 0.8034 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4131004160.0000 - r2: 0.8238 - val_loss: 4569219584.0000 - val_r2: 0.8073 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4095844608.0000 - r2: 0.8250 - val_loss: 4390373376.0000 - val_r2: 0.8148 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4077883904.0000 - r2: 0.8260 - val_loss: 4448027136.0000 - val_r2: 0.8131 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4070009600.0000 - r2: 0.8266 - val_loss: 4387662848.0000 - val_r2: 0.8145 - 2s/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4048090112.0000 - r2: 0.8276 - val_loss: 4333059584.0000 - val_r2: 0.8168 - 2s/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4035703296.0000 - r2: 0.8279 - val_loss: 4445971968.0000 - val_r2: 0.8126 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4047105536.0000 - r2: 0.8277 - val_loss: 4411298304.0000 - val_r2: 0.8126 - 2s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4042162688.0000 - r2: 0.8277 - val_loss: 4679214592.0000 - val_r2: 0.8025 - 2s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4030220800.0000 - r2: 0.8277 - val_loss: 4325705216.0000 - val_r2: 0.8184 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4035628032.0000 - r2: 0.8281 - val_loss: 4375997952.0000 - val_r2: 0.8145 - 2s/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4038270976.0000 - r2: 0.8277 - val_loss: 4576932864.0000 - val_r2: 0.8073 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4035610880.0000 - r2: 0.8281 - val_loss: 4391697408.0000 - val_r2: 0.8118 - 2s/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4024226048.0000 - r2: 0.8288 - val_loss: 4446225408.0000 - val_r2: 0.8134 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4033325568.0000 - r2: 0.8283 - val_loss: 4403981312.0000 - val_r2: 0.8124 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4028832256.0000 - r2: 0.8282 - val_loss: 4309446656.0000 - val_r2: 0.8166 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4026501888.0000 - r2: 0.8281 - val_loss: 4317441024.0000 - val_r2: 0.8149 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4042561536.0000 - r2: 0.8274 - val_loss: 4335205888.0000 - val_r2: 0.8174 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4034099712.0000 - r2: 0.8282 - val_loss: 4433911296.0000 - val_r2: 0.8122 - 2s/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4035101696.0000 - r2: 0.8281 - val_loss: 4510588416.0000 - val_r2: 0.8097 - 2s/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4032827648.0000 - r2: 0.8282 - val_loss: 4391455232.0000 - val_r2: 0.8146 - 2s/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4036186624.0000 - r2: 0.8277 - val_loss: 4361523712.0000 - val_r2: 0.8148 - 2s/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4022901760.0000 - r2: 0.8281 - val_loss: 4548142080.0000 - val_r2: 0.8074 - 2s/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4046948096.0000 - r2: 0.8268 - val_loss: 4221492480.0000 - val_r2: 0.8220 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 1s - loss: 4033714176.0000 - r2: 0.8281 - val_loss: 4391083008.0000 - val_r2: 0.8145 - 1s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 1s - loss: 4032750848.0000 - r2: 0.8279 - val_loss: 4296114688.0000 - val_r2: 0.8136 - 1s/epoch - 6ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4035479552.0000 - r2: 0.8276 - val_loss: 4451106816.0000 - val_r2: 0.8122 - 2s/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4036552448.0000 - r2: 0.8280 - val_loss: 4323845632.0000 - val_r2: 0.8164 - 2s/epoch - 6ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 4038191360.0000 - r2: 0.8279 - val_loss: 4387002368.0000 - val_r2: 0.8146 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4038264576.0000 - r2: 0.8276 - val_loss: 4205661184.0000 - val_r2: 0.8181 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4032315392.0000 - r2: 0.8281 - val_loss: 4340606464.0000 - val_r2: 0.8168 - 2s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4019254528.0000 - r2: 0.8284 - val_loss: 4382752256.0000 - val_r2: 0.8145 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4028643840.0000 - r2: 0.8280 - val_loss: 4535141888.0000 - val_r2: 0.8091 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 1 i 1\n",
      "updated temp_vec [1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 4s - loss: 86102335488.0000 - r2: -2.6778e+00 - val_loss: 21399726080.0000 - val_r2: 0.1000 - 4s/epoch - 15ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 19803027456.0000 - r2: 0.1593 - val_loss: 20162150400.0000 - val_r2: 0.1440 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 18933467136.0000 - r2: 0.1953 - val_loss: 19644522496.0000 - val_r2: 0.1703 - 2s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 18444709888.0000 - r2: 0.2158 - val_loss: 19194011648.0000 - val_r2: 0.1912 - 2s/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 18016806912.0000 - r2: 0.2337 - val_loss: 18746855424.0000 - val_r2: 0.2119 - 2s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 17620307968.0000 - r2: 0.2508 - val_loss: 18312574976.0000 - val_r2: 0.2254 - 2s/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 17212624896.0000 - r2: 0.2687 - val_loss: 17851492352.0000 - val_r2: 0.2419 - 2s/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 16843083776.0000 - r2: 0.2833 - val_loss: 17826680832.0000 - val_r2: 0.2489 - 2s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 16490340352.0000 - r2: 0.2985 - val_loss: 17261826048.0000 - val_r2: 0.2722 - 2s/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 16161841152.0000 - r2: 0.3114 - val_loss: 17111037952.0000 - val_r2: 0.2754 - 2s/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 15881439232.0000 - r2: 0.3248 - val_loss: 16932262912.0000 - val_r2: 0.2858 - 2s/epoch - 8ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 15646298112.0000 - r2: 0.3347 - val_loss: 16364588032.0000 - val_r2: 0.3064 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 15422094336.0000 - r2: 0.3434 - val_loss: 16302454784.0000 - val_r2: 0.3120 - 2s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 15252776960.0000 - r2: 0.3500 - val_loss: 16080855040.0000 - val_r2: 0.3241 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 15139685376.0000 - r2: 0.3561 - val_loss: 15912201216.0000 - val_r2: 0.3264 - 2s/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 15051810816.0000 - r2: 0.3582 - val_loss: 15994482688.0000 - val_r2: 0.3219 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 14999392256.0000 - r2: 0.3617 - val_loss: 16153142272.0000 - val_r2: 0.3192 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 14966459392.0000 - r2: 0.3615 - val_loss: 15975385088.0000 - val_r2: 0.3262 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 14959033344.0000 - r2: 0.3619 - val_loss: 15999526912.0000 - val_r2: 0.3233 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 14941071360.0000 - r2: 0.3638 - val_loss: 16034182144.0000 - val_r2: 0.3221 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 14934760448.0000 - r2: 0.3646 - val_loss: 15960769536.0000 - val_r2: 0.3275 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 14931899392.0000 - r2: 0.3652 - val_loss: 16095664128.0000 - val_r2: 0.3199 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 14927420416.0000 - r2: 0.3639 - val_loss: 15971223552.0000 - val_r2: 0.3270 - 2s/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 14916105216.0000 - r2: 0.3645 - val_loss: 16008544256.0000 - val_r2: 0.3249 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 14922528768.0000 - r2: 0.3650 - val_loss: 16227041280.0000 - val_r2: 0.3088 - 2s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 14921939968.0000 - r2: 0.3643 - val_loss: 16029234176.0000 - val_r2: 0.3233 - 2s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 14926446592.0000 - r2: 0.3652 - val_loss: 15958281216.0000 - val_r2: 0.3249 - 2s/epoch - 8ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 14920647680.0000 - r2: 0.3646 - val_loss: 15978896384.0000 - val_r2: 0.3199 - 2s/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 14923740160.0000 - r2: 0.3652 - val_loss: 16107372544.0000 - val_r2: 0.3202 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 14920860672.0000 - r2: 0.3648 - val_loss: 16309612544.0000 - val_r2: 0.3125 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 14918304768.0000 - r2: 0.3635 - val_loss: 16078324736.0000 - val_r2: 0.3189 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 14914880512.0000 - r2: 0.3639 - val_loss: 16099757056.0000 - val_r2: 0.3185 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 14931147776.0000 - r2: 0.3644 - val_loss: 16130395136.0000 - val_r2: 0.3137 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 14920925184.0000 - r2: 0.3643 - val_loss: 15986324480.0000 - val_r2: 0.3270 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 14917527552.0000 - r2: 0.3635 - val_loss: 16038618112.0000 - val_r2: 0.3245 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 14923154432.0000 - r2: 0.3640 - val_loss: 15953384448.0000 - val_r2: 0.3259 - 2s/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 14940892160.0000 - r2: 0.3638 - val_loss: 15972341760.0000 - val_r2: 0.3248 - 2s/epoch - 8ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 14919428096.0000 - r2: 0.3657 - val_loss: 16295168000.0000 - val_r2: 0.3092 - 2s/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 14921948160.0000 - r2: 0.3640 - val_loss: 16028624896.0000 - val_r2: 0.3283 - 2s/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 14919361536.0000 - r2: 0.3651 - val_loss: 16063771648.0000 - val_r2: 0.3215 - 2s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 14914263040.0000 - r2: 0.3648 - val_loss: 16131984384.0000 - val_r2: 0.3186 - 2s/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 14918117376.0000 - r2: 0.3651 - val_loss: 16022811648.0000 - val_r2: 0.3217 - 2s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 14929152000.0000 - r2: 0.3636 - val_loss: 15970690048.0000 - val_r2: 0.3257 - 2s/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 14921102336.0000 - r2: 0.3644 - val_loss: 15978925056.0000 - val_r2: 0.3177 - 2s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 14939327488.0000 - r2: 0.3649 - val_loss: 15844008960.0000 - val_r2: 0.3298 - 2s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 14932992000.0000 - r2: 0.3647 - val_loss: 16176263168.0000 - val_r2: 0.3190 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 14912081920.0000 - r2: 0.3639 - val_loss: 16313512960.0000 - val_r2: 0.3103 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 14919552000.0000 - r2: 0.3653 - val_loss: 16125894656.0000 - val_r2: 0.3152 - 2s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 14908500992.0000 - r2: 0.3651 - val_loss: 16063119360.0000 - val_r2: 0.3232 - 2s/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 14930147328.0000 - r2: 0.3641 - val_loss: 15853118464.0000 - val_r2: 0.3325 - 2s/epoch - 7ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 0\n",
      "ix 3 i 1\n",
      "updated temp_vec [1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 4s - loss: 74540310528.0000 - r2: -2.1647e+00 - val_loss: 11511211008.0000 - val_r2: 0.5163 - 4s/epoch - 16ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 10520294400.0000 - r2: 0.5529 - val_loss: 10487916544.0000 - val_r2: 0.5576 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 9782007808.0000 - r2: 0.5846 - val_loss: 10045921280.0000 - val_r2: 0.5765 - 2s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 9289721856.0000 - r2: 0.6060 - val_loss: 9561530368.0000 - val_r2: 0.5977 - 2s/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 8810956800.0000 - r2: 0.6250 - val_loss: 9090135040.0000 - val_r2: 0.6181 - 2s/epoch - 8ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 8312635392.0000 - r2: 0.6467 - val_loss: 8661579776.0000 - val_r2: 0.6363 - 2s/epoch - 8ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7777836544.0000 - r2: 0.6696 - val_loss: 8124525056.0000 - val_r2: 0.6560 - 2s/epoch - 8ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7244780544.0000 - r2: 0.6924 - val_loss: 7573415936.0000 - val_r2: 0.6825 - 2s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 6767924224.0000 - r2: 0.7122 - val_loss: 7342056960.0000 - val_r2: 0.6910 - 2s/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6377603584.0000 - r2: 0.7289 - val_loss: 6856029184.0000 - val_r2: 0.7095 - 2s/epoch - 8ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 6056292352.0000 - r2: 0.7426 - val_loss: 6678568960.0000 - val_r2: 0.7187 - 2s/epoch - 8ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5833381376.0000 - r2: 0.7517 - val_loss: 6330098176.0000 - val_r2: 0.7349 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5618149376.0000 - r2: 0.7609 - val_loss: 5946606080.0000 - val_r2: 0.7495 - 2s/epoch - 8ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 5457792000.0000 - r2: 0.7671 - val_loss: 6157260800.0000 - val_r2: 0.7409 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5297319936.0000 - r2: 0.7743 - val_loss: 5741212672.0000 - val_r2: 0.7576 - 2s/epoch - 8ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 5162462208.0000 - r2: 0.7800 - val_loss: 5456054784.0000 - val_r2: 0.7682 - 2s/epoch - 8ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 5053330432.0000 - r2: 0.7851 - val_loss: 5470088704.0000 - val_r2: 0.7700 - 2s/epoch - 8ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4960692224.0000 - r2: 0.7887 - val_loss: 5378924032.0000 - val_r2: 0.7758 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4890667008.0000 - r2: 0.7918 - val_loss: 5334051328.0000 - val_r2: 0.7751 - 2s/epoch - 8ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4827068416.0000 - r2: 0.7944 - val_loss: 5091297792.0000 - val_r2: 0.7855 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4777583616.0000 - r2: 0.7963 - val_loss: 5216843776.0000 - val_r2: 0.7796 - 2s/epoch - 8ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4740449280.0000 - r2: 0.7980 - val_loss: 4975813632.0000 - val_r2: 0.7899 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4713634304.0000 - r2: 0.7989 - val_loss: 5147862528.0000 - val_r2: 0.7840 - 2s/epoch - 8ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4694169600.0000 - r2: 0.8001 - val_loss: 5193033216.0000 - val_r2: 0.7809 - 2s/epoch - 8ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4676017664.0000 - r2: 0.8005 - val_loss: 5094967808.0000 - val_r2: 0.7842 - 2s/epoch - 8ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4676017664.0000 - r2: 0.8010 - val_loss: 5063848960.0000 - val_r2: 0.7853 - 2s/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4662775808.0000 - r2: 0.8011 - val_loss: 5030822912.0000 - val_r2: 0.7879 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4672451584.0000 - r2: 0.8006 - val_loss: 4938898944.0000 - val_r2: 0.7910 - 2s/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4655962112.0000 - r2: 0.8011 - val_loss: 5180786176.0000 - val_r2: 0.7811 - 2s/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4663412224.0000 - r2: 0.8015 - val_loss: 4974051840.0000 - val_r2: 0.7885 - 2s/epoch - 8ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4664246272.0000 - r2: 0.8009 - val_loss: 4941280256.0000 - val_r2: 0.7916 - 2s/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4649424896.0000 - r2: 0.8022 - val_loss: 4932816896.0000 - val_r2: 0.7918 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4643106816.0000 - r2: 0.8022 - val_loss: 5141168640.0000 - val_r2: 0.7795 - 2s/epoch - 8ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4652732416.0000 - r2: 0.8018 - val_loss: 4940163584.0000 - val_r2: 0.7926 - 2s/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4652267008.0000 - r2: 0.8018 - val_loss: 5292490240.0000 - val_r2: 0.7734 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4657758720.0000 - r2: 0.8017 - val_loss: 5053720576.0000 - val_r2: 0.7870 - 2s/epoch - 8ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4663236096.0000 - r2: 0.8015 - val_loss: 4971337728.0000 - val_r2: 0.7906 - 2s/epoch - 8ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4646977024.0000 - r2: 0.8015 - val_loss: 5090134016.0000 - val_r2: 0.7869 - 2s/epoch - 8ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4659663872.0000 - r2: 0.8015 - val_loss: 5066969600.0000 - val_r2: 0.7877 - 2s/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4651122176.0000 - r2: 0.8018 - val_loss: 4996446720.0000 - val_r2: 0.7891 - 2s/epoch - 8ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4639070208.0000 - r2: 0.8025 - val_loss: 5123075072.0000 - val_r2: 0.7826 - 2s/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4655373312.0000 - r2: 0.8018 - val_loss: 5004960768.0000 - val_r2: 0.7883 - 2s/epoch - 8ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 4642636800.0000 - r2: 0.8023 - val_loss: 4997144576.0000 - val_r2: 0.7893 - 2s/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4647933440.0000 - r2: 0.8017 - val_loss: 5121963520.0000 - val_r2: 0.7847 - 2s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4647905280.0000 - r2: 0.8019 - val_loss: 4971812864.0000 - val_r2: 0.7907 - 2s/epoch - 8ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 4648593408.0000 - r2: 0.8019 - val_loss: 5125467136.0000 - val_r2: 0.7829 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4650168832.0000 - r2: 0.8015 - val_loss: 5067152384.0000 - val_r2: 0.7855 - 2s/epoch - 8ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4661851648.0000 - r2: 0.8015 - val_loss: 5000349696.0000 - val_r2: 0.7895 - 2s/epoch - 8ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4646208000.0000 - r2: 0.8018 - val_loss: 5078110208.0000 - val_r2: 0.7848 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4658101760.0000 - r2: 0.8017 - val_loss: 4988742144.0000 - val_r2: 0.7903 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 4s - loss: 70226968576.0000 - r2: -1.9923e+00 - val_loss: 10029475840.0000 - val_r2: 0.5799 - 4s/epoch - 15ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 8931938304.0000 - r2: 0.6200 - val_loss: 8679271424.0000 - val_r2: 0.6357 - 2s/epoch - 8ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8118309376.0000 - r2: 0.6540 - val_loss: 8157398016.0000 - val_r2: 0.6562 - 2s/epoch - 8ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7703947776.0000 - r2: 0.6725 - val_loss: 7881257984.0000 - val_r2: 0.6648 - 2s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 7356706304.0000 - r2: 0.6876 - val_loss: 7470654464.0000 - val_r2: 0.6870 - 2s/epoch - 9ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 6996845568.0000 - r2: 0.7026 - val_loss: 7210987008.0000 - val_r2: 0.6955 - 2s/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 6620906496.0000 - r2: 0.7179 - val_loss: 6840461824.0000 - val_r2: 0.7105 - 2s/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 6253277184.0000 - r2: 0.7332 - val_loss: 6428572672.0000 - val_r2: 0.7295 - 2s/epoch - 8ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 5910037504.0000 - r2: 0.7484 - val_loss: 6325223424.0000 - val_r2: 0.7335 - 2s/epoch - 8ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5628908544.0000 - r2: 0.7604 - val_loss: 6161173504.0000 - val_r2: 0.7398 - 2s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 5408010752.0000 - r2: 0.7696 - val_loss: 5905031168.0000 - val_r2: 0.7512 - 2s/epoch - 9ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 5236033024.0000 - r2: 0.7772 - val_loss: 5882135040.0000 - val_r2: 0.7515 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5087835648.0000 - r2: 0.7832 - val_loss: 5795523072.0000 - val_r2: 0.7554 - 2s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 4961708032.0000 - r2: 0.7883 - val_loss: 5604978688.0000 - val_r2: 0.7636 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4854588928.0000 - r2: 0.7933 - val_loss: 5740607488.0000 - val_r2: 0.7582 - 2s/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4756277760.0000 - r2: 0.7978 - val_loss: 5299271680.0000 - val_r2: 0.7764 - 2s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4663151616.0000 - r2: 0.8013 - val_loss: 5488863232.0000 - val_r2: 0.7669 - 2s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4581494272.0000 - r2: 0.8050 - val_loss: 5243164160.0000 - val_r2: 0.7782 - 2s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4515410944.0000 - r2: 0.8074 - val_loss: 5242424832.0000 - val_r2: 0.7798 - 2s/epoch - 9ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4451545600.0000 - r2: 0.8104 - val_loss: 5405533184.0000 - val_r2: 0.7725 - 2s/epoch - 8ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4415995904.0000 - r2: 0.8119 - val_loss: 5271965184.0000 - val_r2: 0.7790 - 2s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4378670592.0000 - r2: 0.8131 - val_loss: 5237084160.0000 - val_r2: 0.7792 - 2s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4347450368.0000 - r2: 0.8147 - val_loss: 5235997696.0000 - val_r2: 0.7755 - 2s/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4332801024.0000 - r2: 0.8154 - val_loss: 5042537472.0000 - val_r2: 0.7846 - 2s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4313769984.0000 - r2: 0.8161 - val_loss: 5033824768.0000 - val_r2: 0.7862 - 2s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4314636800.0000 - r2: 0.8158 - val_loss: 4930602496.0000 - val_r2: 0.7912 - 2s/epoch - 8ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4304010752.0000 - r2: 0.8162 - val_loss: 5016687104.0000 - val_r2: 0.7875 - 2s/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4304133120.0000 - r2: 0.8161 - val_loss: 4953519104.0000 - val_r2: 0.7909 - 2s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4297636352.0000 - r2: 0.8170 - val_loss: 5004686848.0000 - val_r2: 0.7888 - 2s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4311208960.0000 - r2: 0.8157 - val_loss: 5166491136.0000 - val_r2: 0.7824 - 2s/epoch - 9ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4298076160.0000 - r2: 0.8168 - val_loss: 4929241600.0000 - val_r2: 0.7910 - 2s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4313906688.0000 - r2: 0.8160 - val_loss: 5113879552.0000 - val_r2: 0.7826 - 2s/epoch - 8ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4296612864.0000 - r2: 0.8168 - val_loss: 4928923648.0000 - val_r2: 0.7908 - 2s/epoch - 9ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4297659392.0000 - r2: 0.8165 - val_loss: 4931843584.0000 - val_r2: 0.7927 - 2s/epoch - 9ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4294224896.0000 - r2: 0.8165 - val_loss: 4919771136.0000 - val_r2: 0.7922 - 2s/epoch - 10ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 3s - loss: 4296345088.0000 - r2: 0.8167 - val_loss: 5044975104.0000 - val_r2: 0.7860 - 3s/epoch - 10ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4299524096.0000 - r2: 0.8167 - val_loss: 5006922240.0000 - val_r2: 0.7869 - 2s/epoch - 10ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4293048832.0000 - r2: 0.8169 - val_loss: 5216124416.0000 - val_r2: 0.7774 - 2s/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4309746176.0000 - r2: 0.8163 - val_loss: 4977944576.0000 - val_r2: 0.7905 - 2s/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4293394688.0000 - r2: 0.8169 - val_loss: 5012770816.0000 - val_r2: 0.7889 - 2s/epoch - 9ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4296885760.0000 - r2: 0.8165 - val_loss: 5043362304.0000 - val_r2: 0.7859 - 2s/epoch - 8ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4297189888.0000 - r2: 0.8170 - val_loss: 4880065536.0000 - val_r2: 0.7938 - 2s/epoch - 9ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 4292141056.0000 - r2: 0.8167 - val_loss: 4925750272.0000 - val_r2: 0.7897 - 2s/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4310714880.0000 - r2: 0.8165 - val_loss: 4901444096.0000 - val_r2: 0.7927 - 2s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4292655104.0000 - r2: 0.8166 - val_loss: 4975847424.0000 - val_r2: 0.7880 - 2s/epoch - 9ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 4302514688.0000 - r2: 0.8165 - val_loss: 5076057088.0000 - val_r2: 0.7851 - 2s/epoch - 8ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4294979072.0000 - r2: 0.8165 - val_loss: 4931428352.0000 - val_r2: 0.7903 - 2s/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4287262720.0000 - r2: 0.8174 - val_loss: 4979611136.0000 - val_r2: 0.7900 - 2s/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4300695552.0000 - r2: 0.8165 - val_loss: 5130002432.0000 - val_r2: 0.7812 - 2s/epoch - 9ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4292645888.0000 - r2: 0.8170 - val_loss: 5229794304.0000 - val_r2: 0.7799 - 2s/epoch - 8ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 5s - loss: 80620339200.0000 - r2: -2.3836e+00 - val_loss: 10885280768.0000 - val_r2: 0.5469 - 5s/epoch - 18ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9608365056.0000 - r2: 0.5923 - val_loss: 9161404416.0000 - val_r2: 0.6164 - 2s/epoch - 9ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8612025344.0000 - r2: 0.6327 - val_loss: 8711471104.0000 - val_r2: 0.6321 - 2s/epoch - 9ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 8243639808.0000 - r2: 0.6489 - val_loss: 8353401856.0000 - val_r2: 0.6479 - 2s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 8002674688.0000 - r2: 0.6588 - val_loss: 8218224128.0000 - val_r2: 0.6563 - 2s/epoch - 9ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7775410176.0000 - r2: 0.6692 - val_loss: 7935250432.0000 - val_r2: 0.6553 - 2s/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7539417088.0000 - r2: 0.6795 - val_loss: 7662383104.0000 - val_r2: 0.6757 - 2s/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7291724800.0000 - r2: 0.6899 - val_loss: 7392795648.0000 - val_r2: 0.6874 - 2s/epoch - 9ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 7006298624.0000 - r2: 0.7020 - val_loss: 7145703424.0000 - val_r2: 0.6985 - 2s/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6698179072.0000 - r2: 0.7151 - val_loss: 6877946880.0000 - val_r2: 0.7108 - 2s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 6366705664.0000 - r2: 0.7287 - val_loss: 6530050560.0000 - val_r2: 0.7248 - 2s/epoch - 8ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6038828032.0000 - r2: 0.7426 - val_loss: 6214887424.0000 - val_r2: 0.7366 - 2s/epoch - 9ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 5717168128.0000 - r2: 0.7564 - val_loss: 5931679744.0000 - val_r2: 0.7494 - 2s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 5412521472.0000 - r2: 0.7698 - val_loss: 5659663360.0000 - val_r2: 0.7610 - 2s/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5157511680.0000 - r2: 0.7803 - val_loss: 5363677184.0000 - val_r2: 0.7748 - 2s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4935328768.0000 - r2: 0.7898 - val_loss: 5190993408.0000 - val_r2: 0.7812 - 2s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4737597952.0000 - r2: 0.7982 - val_loss: 5007907328.0000 - val_r2: 0.7886 - 2s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 4585629696.0000 - r2: 0.8045 - val_loss: 4924516864.0000 - val_r2: 0.7918 - 2s/epoch - 8ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4433600512.0000 - r2: 0.8109 - val_loss: 4694083072.0000 - val_r2: 0.8022 - 2s/epoch - 9ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 4327226368.0000 - r2: 0.8159 - val_loss: 4519327232.0000 - val_r2: 0.8085 - 2s/epoch - 9ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4232745728.0000 - r2: 0.8195 - val_loss: 4862692864.0000 - val_r2: 0.7958 - 2s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 4167859456.0000 - r2: 0.8219 - val_loss: 4513955328.0000 - val_r2: 0.8083 - 2s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4131201792.0000 - r2: 0.8240 - val_loss: 4319882240.0000 - val_r2: 0.8175 - 2s/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 4087849984.0000 - r2: 0.8262 - val_loss: 4451505152.0000 - val_r2: 0.8108 - 2s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4079010816.0000 - r2: 0.8262 - val_loss: 4546376704.0000 - val_r2: 0.8082 - 2s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4063392256.0000 - r2: 0.8262 - val_loss: 4306279424.0000 - val_r2: 0.8193 - 2s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4052230400.0000 - r2: 0.8270 - val_loss: 4282071040.0000 - val_r2: 0.8193 - 2s/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4056001024.0000 - r2: 0.8270 - val_loss: 4396463616.0000 - val_r2: 0.8150 - 2s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4054693120.0000 - r2: 0.8274 - val_loss: 4418978304.0000 - val_r2: 0.8135 - 2s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4046126592.0000 - r2: 0.8276 - val_loss: 4401196544.0000 - val_r2: 0.8135 - 2s/epoch - 10ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4048973056.0000 - r2: 0.8275 - val_loss: 4568754176.0000 - val_r2: 0.8048 - 2s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4044116992.0000 - r2: 0.8271 - val_loss: 4394494464.0000 - val_r2: 0.8107 - 2s/epoch - 9ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4044511488.0000 - r2: 0.8281 - val_loss: 4286879744.0000 - val_r2: 0.8176 - 2s/epoch - 9ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4050807296.0000 - r2: 0.8271 - val_loss: 4272244736.0000 - val_r2: 0.8194 - 2s/epoch - 9ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4045437184.0000 - r2: 0.8273 - val_loss: 4309208576.0000 - val_r2: 0.8192 - 2s/epoch - 8ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4051256064.0000 - r2: 0.8273 - val_loss: 4273520640.0000 - val_r2: 0.8154 - 2s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4046383616.0000 - r2: 0.8271 - val_loss: 4431859712.0000 - val_r2: 0.8113 - 2s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4051294464.0000 - r2: 0.8270 - val_loss: 4327799296.0000 - val_r2: 0.8173 - 2s/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4043819264.0000 - r2: 0.8276 - val_loss: 4402592256.0000 - val_r2: 0.8150 - 2s/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4049318656.0000 - r2: 0.8272 - val_loss: 4266774528.0000 - val_r2: 0.8198 - 2s/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 3s - loss: 4045157888.0000 - r2: 0.8277 - val_loss: 4363126784.0000 - val_r2: 0.8148 - 3s/epoch - 10ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4038930176.0000 - r2: 0.8280 - val_loss: 4234772736.0000 - val_r2: 0.8227 - 2s/epoch - 10ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 4050696704.0000 - r2: 0.8268 - val_loss: 4296412160.0000 - val_r2: 0.8191 - 2s/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 4049664512.0000 - r2: 0.8274 - val_loss: 4289203200.0000 - val_r2: 0.8198 - 2s/epoch - 8ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 3s - loss: 4042731776.0000 - r2: 0.8276 - val_loss: 4303275520.0000 - val_r2: 0.8169 - 3s/epoch - 10ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 3s - loss: 4044839168.0000 - r2: 0.8274 - val_loss: 4299656192.0000 - val_r2: 0.8190 - 3s/epoch - 10ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 3s - loss: 4049598464.0000 - r2: 0.8271 - val_loss: 4193671424.0000 - val_r2: 0.8214 - 3s/epoch - 10ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4048839936.0000 - r2: 0.8273 - val_loss: 4255864832.0000 - val_r2: 0.8212 - 2s/epoch - 10ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 3s - loss: 4050761984.0000 - r2: 0.8272 - val_loss: 4363005440.0000 - val_r2: 0.8158 - 3s/epoch - 10ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4047846144.0000 - r2: 0.8274 - val_loss: 4329530368.0000 - val_r2: 0.8166 - 2s/epoch - 10ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 6 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 6s - loss: 66852552704.0000 - r2: -1.8072e+00 - val_loss: 9806189568.0000 - val_r2: 0.5850 - 6s/epoch - 23ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 8818950144.0000 - r2: 0.6248 - val_loss: 8566428160.0000 - val_r2: 0.6381 - 2s/epoch - 10ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8068279296.0000 - r2: 0.6562 - val_loss: 8230011392.0000 - val_r2: 0.6540 - 2s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 7664964096.0000 - r2: 0.6739 - val_loss: 7885252608.0000 - val_r2: 0.6671 - 2s/epoch - 8ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 3s - loss: 7292692480.0000 - r2: 0.6897 - val_loss: 7405289472.0000 - val_r2: 0.6850 - 3s/epoch - 10ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 3s - loss: 6898735104.0000 - r2: 0.7062 - val_loss: 6891658240.0000 - val_r2: 0.7076 - 3s/epoch - 10ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 6489356288.0000 - r2: 0.7230 - val_loss: 6607235584.0000 - val_r2: 0.7202 - 2s/epoch - 10ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 6078223872.0000 - r2: 0.7411 - val_loss: 6304805376.0000 - val_r2: 0.7339 - 2s/epoch - 10ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 5706153472.0000 - r2: 0.7572 - val_loss: 5878149632.0000 - val_r2: 0.7509 - 2s/epoch - 10ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 5403947520.0000 - r2: 0.7696 - val_loss: 5589250048.0000 - val_r2: 0.7645 - 2s/epoch - 10ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 3s - loss: 5166736384.0000 - r2: 0.7800 - val_loss: 5355986944.0000 - val_r2: 0.7701 - 3s/epoch - 10ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 4979481088.0000 - r2: 0.7876 - val_loss: 5235154432.0000 - val_r2: 0.7787 - 2s/epoch - 10ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 4820253696.0000 - r2: 0.7947 - val_loss: 5038263296.0000 - val_r2: 0.7889 - 2s/epoch - 10ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 3s - loss: 4687245312.0000 - r2: 0.8001 - val_loss: 4873278464.0000 - val_r2: 0.7955 - 3s/epoch - 10ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 4559839744.0000 - r2: 0.8059 - val_loss: 4666280448.0000 - val_r2: 0.8012 - 2s/epoch - 10ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 4470834176.0000 - r2: 0.8096 - val_loss: 4772494336.0000 - val_r2: 0.7985 - 2s/epoch - 10ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 4385797632.0000 - r2: 0.8129 - val_loss: 4618441216.0000 - val_r2: 0.8055 - 2s/epoch - 10ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 3s - loss: 4334208512.0000 - r2: 0.8156 - val_loss: 4665310720.0000 - val_r2: 0.8034 - 3s/epoch - 10ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 4277373184.0000 - r2: 0.8181 - val_loss: 4603102720.0000 - val_r2: 0.8061 - 2s/epoch - 10ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 3s - loss: 4244391936.0000 - r2: 0.8188 - val_loss: 4702587392.0000 - val_r2: 0.8007 - 3s/epoch - 11ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 4218824960.0000 - r2: 0.8200 - val_loss: 4363692544.0000 - val_r2: 0.8164 - 2s/epoch - 10ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 3s - loss: 4195135488.0000 - r2: 0.8215 - val_loss: 4365387264.0000 - val_r2: 0.8151 - 3s/epoch - 10ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 4187192832.0000 - r2: 0.8217 - val_loss: 4500966400.0000 - val_r2: 0.8079 - 2s/epoch - 10ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 3s - loss: 4177469440.0000 - r2: 0.8221 - val_loss: 4521137664.0000 - val_r2: 0.8095 - 3s/epoch - 10ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 4169498624.0000 - r2: 0.8223 - val_loss: 4403878912.0000 - val_r2: 0.8143 - 2s/epoch - 10ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 4174934272.0000 - r2: 0.8220 - val_loss: 4371041792.0000 - val_r2: 0.8159 - 2s/epoch - 10ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 4167750656.0000 - r2: 0.8221 - val_loss: 4682968576.0000 - val_r2: 0.8025 - 2s/epoch - 10ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 4175531008.0000 - r2: 0.8218 - val_loss: 4548599296.0000 - val_r2: 0.8085 - 2s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 4164525056.0000 - r2: 0.8221 - val_loss: 4431767552.0000 - val_r2: 0.8122 - 2s/epoch - 6ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 2s - loss: 4176356352.0000 - r2: 0.8225 - val_loss: 4385610752.0000 - val_r2: 0.8148 - 2s/epoch - 6ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 2s - loss: 4169398528.0000 - r2: 0.8221 - val_loss: 4449662464.0000 - val_r2: 0.8122 - 2s/epoch - 6ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 4174334720.0000 - r2: 0.8220 - val_loss: 4352563200.0000 - val_r2: 0.8168 - 2s/epoch - 6ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 4167438080.0000 - r2: 0.8223 - val_loss: 4420497408.0000 - val_r2: 0.8146 - 2s/epoch - 6ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 4183773696.0000 - r2: 0.8214 - val_loss: 4413907456.0000 - val_r2: 0.8127 - 2s/epoch - 6ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 4173187328.0000 - r2: 0.8216 - val_loss: 4361496064.0000 - val_r2: 0.8148 - 2s/epoch - 6ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 4174295296.0000 - r2: 0.8219 - val_loss: 4726611456.0000 - val_r2: 0.7994 - 2s/epoch - 6ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 4165350656.0000 - r2: 0.8220 - val_loss: 4381711872.0000 - val_r2: 0.8160 - 2s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 2s - loss: 4168075520.0000 - r2: 0.8219 - val_loss: 4516498944.0000 - val_r2: 0.8096 - 2s/epoch - 6ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 4164467456.0000 - r2: 0.8221 - val_loss: 4407994880.0000 - val_r2: 0.8129 - 2s/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 4167451392.0000 - r2: 0.8223 - val_loss: 4704341504.0000 - val_r2: 0.8012 - 2s/epoch - 6ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 4174503168.0000 - r2: 0.8223 - val_loss: 4403129856.0000 - val_r2: 0.8136 - 2s/epoch - 6ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 4173264128.0000 - r2: 0.8219 - val_loss: 4423400960.0000 - val_r2: 0.8134 - 2s/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 4166245888.0000 - r2: 0.8226 - val_loss: 4636321792.0000 - val_r2: 0.8047 - 2s/epoch - 8ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 3s - loss: 4168404480.0000 - r2: 0.8224 - val_loss: 4423054848.0000 - val_r2: 0.8126 - 3s/epoch - 10ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 4159941632.0000 - r2: 0.8226 - val_loss: 4459798016.0000 - val_r2: 0.8038 - 2s/epoch - 10ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 4162142976.0000 - r2: 0.8226 - val_loss: 4397949440.0000 - val_r2: 0.8149 - 2s/epoch - 9ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 4172788736.0000 - r2: 0.8221 - val_loss: 4355482112.0000 - val_r2: 0.8174 - 2s/epoch - 10ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 4164575744.0000 - r2: 0.8221 - val_loss: 4287794944.0000 - val_r2: 0.8188 - 2s/epoch - 10ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 4168124928.0000 - r2: 0.8221 - val_loss: 4423505920.0000 - val_r2: 0.8131 - 2s/epoch - 10ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 4165188608.0000 - r2: 0.8222 - val_loss: 4358433792.0000 - val_r2: 0.8165 - 2s/epoch - 9ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 7 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 6s - loss: 68185804800.0000 - r2: -1.8771e+00 - val_loss: 10101292032.0000 - val_r2: 0.5767 - 6s/epoch - 23ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9100988416.0000 - r2: 0.6128 - val_loss: 8929492992.0000 - val_r2: 0.6244 - 2s/epoch - 10ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8437756416.0000 - r2: 0.6412 - val_loss: 8512466944.0000 - val_r2: 0.6399 - 2s/epoch - 10ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 8162930688.0000 - r2: 0.6522 - val_loss: 8281661952.0000 - val_r2: 0.6481 - 2s/epoch - 10ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 7941106688.0000 - r2: 0.6616 - val_loss: 8012701184.0000 - val_r2: 0.6596 - 2s/epoch - 10ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7752958976.0000 - r2: 0.6692 - val_loss: 7774498304.0000 - val_r2: 0.6725 - 2s/epoch - 10ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7540237824.0000 - r2: 0.6795 - val_loss: 7739873792.0000 - val_r2: 0.6737 - 2s/epoch - 10ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7355514368.0000 - r2: 0.6873 - val_loss: 7457977344.0000 - val_r2: 0.6861 - 2s/epoch - 10ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 7182434304.0000 - r2: 0.6938 - val_loss: 7236645888.0000 - val_r2: 0.6951 - 2s/epoch - 10ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 3s - loss: 7038780416.0000 - r2: 0.7002 - val_loss: 7156662272.0000 - val_r2: 0.6979 - 3s/epoch - 10ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 6931414528.0000 - r2: 0.7045 - val_loss: 7086409728.0000 - val_r2: 0.7011 - 2s/epoch - 9ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6868042752.0000 - r2: 0.7070 - val_loss: 7002250752.0000 - val_r2: 0.7047 - 2s/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 6826242560.0000 - r2: 0.7088 - val_loss: 6886368256.0000 - val_r2: 0.7100 - 2s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 6807261184.0000 - r2: 0.7101 - val_loss: 7009402368.0000 - val_r2: 0.7047 - 2s/epoch - 10ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 3s - loss: 6797995008.0000 - r2: 0.7105 - val_loss: 7155344896.0000 - val_r2: 0.6999 - 3s/epoch - 10ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 6788816896.0000 - r2: 0.7107 - val_loss: 7076969984.0000 - val_r2: 0.7015 - 2s/epoch - 10ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 6782099456.0000 - r2: 0.7109 - val_loss: 6956247552.0000 - val_r2: 0.7065 - 2s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 6782198784.0000 - r2: 0.7112 - val_loss: 7157698048.0000 - val_r2: 0.6971 - 2s/epoch - 10ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 6777577472.0000 - r2: 0.7108 - val_loss: 6960716800.0000 - val_r2: 0.7040 - 2s/epoch - 10ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 6778456064.0000 - r2: 0.7114 - val_loss: 6922969088.0000 - val_r2: 0.7089 - 2s/epoch - 10ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 6776346624.0000 - r2: 0.7114 - val_loss: 7064999936.0000 - val_r2: 0.7031 - 2s/epoch - 10ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 6779713024.0000 - r2: 0.7111 - val_loss: 7167799808.0000 - val_r2: 0.6986 - 2s/epoch - 10ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 2s - loss: 6782487552.0000 - r2: 0.7107 - val_loss: 6935355392.0000 - val_r2: 0.7082 - 2s/epoch - 10ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 2s - loss: 6787061248.0000 - r2: 0.7111 - val_loss: 6896619008.0000 - val_r2: 0.7116 - 2s/epoch - 10ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 2s - loss: 6776159744.0000 - r2: 0.7106 - val_loss: 7069640704.0000 - val_r2: 0.7022 - 2s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 2s - loss: 6778397696.0000 - r2: 0.7105 - val_loss: 6852423680.0000 - val_r2: 0.7111 - 2s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 2s - loss: 6780720128.0000 - r2: 0.7114 - val_loss: 6985074176.0000 - val_r2: 0.7026 - 2s/epoch - 10ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 2s - loss: 6775506432.0000 - r2: 0.7106 - val_loss: 6753583104.0000 - val_r2: 0.7140 - 2s/epoch - 10ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 2s - loss: 6784729088.0000 - r2: 0.7111 - val_loss: 6876062208.0000 - val_r2: 0.7119 - 2s/epoch - 10ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 3s - loss: 6781564416.0000 - r2: 0.7110 - val_loss: 6915708416.0000 - val_r2: 0.7090 - 3s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 3s - loss: 6785310720.0000 - r2: 0.7110 - val_loss: 6901117952.0000 - val_r2: 0.7043 - 3s/epoch - 10ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 2s - loss: 6781362688.0000 - r2: 0.7119 - val_loss: 6843732480.0000 - val_r2: 0.7089 - 2s/epoch - 10ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 2s - loss: 6774129152.0000 - r2: 0.7107 - val_loss: 6946654208.0000 - val_r2: 0.7091 - 2s/epoch - 10ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 2s - loss: 6783801344.0000 - r2: 0.7111 - val_loss: 6911100928.0000 - val_r2: 0.7086 - 2s/epoch - 10ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 2s - loss: 6779807232.0000 - r2: 0.7108 - val_loss: 7018134528.0000 - val_r2: 0.7006 - 2s/epoch - 9ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 2s - loss: 6776141824.0000 - r2: 0.7107 - val_loss: 6874371584.0000 - val_r2: 0.7098 - 2s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 2s - loss: 6775040512.0000 - r2: 0.7111 - val_loss: 6814341120.0000 - val_r2: 0.7110 - 2s/epoch - 10ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 3s - loss: 6775773696.0000 - r2: 0.7107 - val_loss: 6826177536.0000 - val_r2: 0.7088 - 3s/epoch - 10ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 2s - loss: 6783874048.0000 - r2: 0.7098 - val_loss: 6869038080.0000 - val_r2: 0.7120 - 2s/epoch - 10ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 2s - loss: 6775751168.0000 - r2: 0.7113 - val_loss: 6830276608.0000 - val_r2: 0.7132 - 2s/epoch - 10ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 2s - loss: 6773330432.0000 - r2: 0.7120 - val_loss: 6759271936.0000 - val_r2: 0.7152 - 2s/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 2s - loss: 6775267840.0000 - r2: 0.7109 - val_loss: 6929467392.0000 - val_r2: 0.7032 - 2s/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 2s - loss: 6776669696.0000 - r2: 0.7108 - val_loss: 6925919232.0000 - val_r2: 0.7095 - 2s/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 2s - loss: 6777387520.0000 - r2: 0.7106 - val_loss: 6812371456.0000 - val_r2: 0.7118 - 2s/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 2s - loss: 6768695808.0000 - r2: 0.7117 - val_loss: 6911448576.0000 - val_r2: 0.7101 - 2s/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 2s - loss: 6769966080.0000 - r2: 0.7108 - val_loss: 7019965440.0000 - val_r2: 0.7022 - 2s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 2s - loss: 6766393344.0000 - r2: 0.7117 - val_loss: 6759911936.0000 - val_r2: 0.7146 - 2s/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 2s - loss: 6776561664.0000 - r2: 0.7118 - val_loss: 6800160256.0000 - val_r2: 0.7122 - 2s/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 2s - loss: 6763941888.0000 - r2: 0.7118 - val_loss: 6982676992.0000 - val_r2: 0.6999 - 2s/epoch - 8ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 2s - loss: 6769786880.0000 - r2: 0.7116 - val_loss: 6906946560.0000 - val_r2: 0.7089 - 2s/epoch - 7ms/step\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 41), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 17), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Epoch 1/50\n",
      "251/251 - 3s - loss: 80665092096.0000 - r2: -2.4534e+00 - val_loss: 10725353472.0000 - val_r2: 0.5499 - 3s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "251/251 - 2s - loss: 9551756288.0000 - r2: 0.5948 - val_loss: 9212503040.0000 - val_r2: 0.6131 - 2s/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "251/251 - 2s - loss: 8631305216.0000 - r2: 0.6336 - val_loss: 8661899264.0000 - val_r2: 0.6324 - 2s/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "251/251 - 2s - loss: 8293208576.0000 - r2: 0.6470 - val_loss: 8441613824.0000 - val_r2: 0.6463 - 2s/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "251/251 - 2s - loss: 8074191872.0000 - r2: 0.6566 - val_loss: 8174600192.0000 - val_r2: 0.6564 - 2s/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "251/251 - 2s - loss: 7875687936.0000 - r2: 0.6641 - val_loss: 7965177856.0000 - val_r2: 0.6648 - 2s/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "251/251 - 2s - loss: 7667191808.0000 - r2: 0.6744 - val_loss: 7774774272.0000 - val_r2: 0.6716 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "251/251 - 2s - loss: 7441463296.0000 - r2: 0.6835 - val_loss: 7519314432.0000 - val_r2: 0.6807 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "251/251 - 2s - loss: 7210925568.0000 - r2: 0.6928 - val_loss: 7330919424.0000 - val_r2: 0.6924 - 2s/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "251/251 - 2s - loss: 6961442304.0000 - r2: 0.7029 - val_loss: 7091375616.0000 - val_r2: 0.6997 - 2s/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "251/251 - 2s - loss: 6710310400.0000 - r2: 0.7143 - val_loss: 7002445824.0000 - val_r2: 0.7060 - 2s/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "251/251 - 2s - loss: 6456221184.0000 - r2: 0.7247 - val_loss: 6586064384.0000 - val_r2: 0.7215 - 2s/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "251/251 - 2s - loss: 6225007104.0000 - r2: 0.7351 - val_loss: 6349451264.0000 - val_r2: 0.7295 - 2s/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "251/251 - 2s - loss: 6007693824.0000 - r2: 0.7444 - val_loss: 6264693760.0000 - val_r2: 0.7324 - 2s/epoch - 8ms/step\n",
      "Epoch 15/50\n",
      "251/251 - 2s - loss: 5841905664.0000 - r2: 0.7516 - val_loss: 6166470144.0000 - val_r2: 0.7377 - 2s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "251/251 - 2s - loss: 5677805568.0000 - r2: 0.7579 - val_loss: 5991416832.0000 - val_r2: 0.7447 - 2s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "251/251 - 2s - loss: 5543218176.0000 - r2: 0.7635 - val_loss: 5853892608.0000 - val_r2: 0.7522 - 2s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "251/251 - 2s - loss: 5419225088.0000 - r2: 0.7696 - val_loss: 5678306816.0000 - val_r2: 0.7584 - 2s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "251/251 - 2s - loss: 5306109440.0000 - r2: 0.7745 - val_loss: 5840234496.0000 - val_r2: 0.7541 - 2s/epoch - 10ms/step\n",
      "Epoch 20/50\n",
      "251/251 - 2s - loss: 5195698176.0000 - r2: 0.7782 - val_loss: 5574502912.0000 - val_r2: 0.7651 - 2s/epoch - 10ms/step\n",
      "Epoch 21/50\n",
      "251/251 - 2s - loss: 5113092608.0000 - r2: 0.7815 - val_loss: 5383865856.0000 - val_r2: 0.7716 - 2s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "251/251 - 2s - loss: 5033038336.0000 - r2: 0.7854 - val_loss: 5343070208.0000 - val_r2: 0.7741 - 2s/epoch - 8ms/step\n",
      "Epoch 23/50\n",
      "251/251 - 3s - loss: 4958845440.0000 - r2: 0.7892 - val_loss: 5179813888.0000 - val_r2: 0.7806 - 3s/epoch - 10ms/step\n",
      "Epoch 24/50\n",
      "251/251 - 3s - loss: 4896156672.0000 - r2: 0.7909 - val_loss: 5039368192.0000 - val_r2: 0.7866 - 3s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "251/251 - 3s - loss: 4851125760.0000 - r2: 0.7930 - val_loss: 5155617280.0000 - val_r2: 0.7831 - 3s/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "251/251 - 3s - loss: 4805400064.0000 - r2: 0.7947 - val_loss: 5047177728.0000 - val_r2: 0.7866 - 3s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "251/251 - 3s - loss: 4768000512.0000 - r2: 0.7967 - val_loss: 4970598912.0000 - val_r2: 0.7905 - 3s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "251/251 - 3s - loss: 4736842240.0000 - r2: 0.7981 - val_loss: 4940043264.0000 - val_r2: 0.7924 - 3s/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "251/251 - 3s - loss: 4712818688.0000 - r2: 0.7994 - val_loss: 5111924736.0000 - val_r2: 0.7847 - 3s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "251/251 - 3s - loss: 4694346240.0000 - r2: 0.8002 - val_loss: 4862543360.0000 - val_r2: 0.7953 - 3s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "251/251 - 3s - loss: 4672238592.0000 - r2: 0.8006 - val_loss: 4911054336.0000 - val_r2: 0.7919 - 3s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "251/251 - 3s - loss: 4664390656.0000 - r2: 0.8010 - val_loss: 4907404800.0000 - val_r2: 0.7928 - 3s/epoch - 13ms/step\n",
      "Epoch 33/50\n",
      "251/251 - 3s - loss: 4656537088.0000 - r2: 0.8014 - val_loss: 4880980480.0000 - val_r2: 0.7941 - 3s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "251/251 - 3s - loss: 4643557888.0000 - r2: 0.8016 - val_loss: 4880158720.0000 - val_r2: 0.7944 - 3s/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "251/251 - 3s - loss: 4639871488.0000 - r2: 0.8019 - val_loss: 4898126848.0000 - val_r2: 0.7945 - 3s/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "251/251 - 3s - loss: 4638951936.0000 - r2: 0.8020 - val_loss: 4880784384.0000 - val_r2: 0.7943 - 3s/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "251/251 - 3s - loss: 4621831168.0000 - r2: 0.8024 - val_loss: 4792479232.0000 - val_r2: 0.7980 - 3s/epoch - 11ms/step\n",
      "Epoch 38/50\n",
      "251/251 - 3s - loss: 4622145536.0000 - r2: 0.8024 - val_loss: 4879309824.0000 - val_r2: 0.7920 - 3s/epoch - 11ms/step\n",
      "Epoch 39/50\n",
      "251/251 - 3s - loss: 4618699264.0000 - r2: 0.8027 - val_loss: 4983171584.0000 - val_r2: 0.7891 - 3s/epoch - 12ms/step\n",
      "Epoch 40/50\n",
      "251/251 - 3s - loss: 4615791616.0000 - r2: 0.8033 - val_loss: 4798297600.0000 - val_r2: 0.7954 - 3s/epoch - 13ms/step\n",
      "Epoch 41/50\n",
      "251/251 - 3s - loss: 4613424640.0000 - r2: 0.8032 - val_loss: 4827879424.0000 - val_r2: 0.7952 - 3s/epoch - 12ms/step\n",
      "Epoch 42/50\n",
      "251/251 - 3s - loss: 4617830400.0000 - r2: 0.8029 - val_loss: 4928033792.0000 - val_r2: 0.7913 - 3s/epoch - 11ms/step\n",
      "Epoch 43/50\n",
      "251/251 - 3s - loss: 4619041792.0000 - r2: 0.8024 - val_loss: 4995707392.0000 - val_r2: 0.7891 - 3s/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "251/251 - 3s - loss: 4615045120.0000 - r2: 0.8028 - val_loss: 4881572352.0000 - val_r2: 0.7944 - 3s/epoch - 11ms/step\n",
      "Epoch 45/50\n",
      "251/251 - 3s - loss: 4615432704.0000 - r2: 0.8034 - val_loss: 4853277696.0000 - val_r2: 0.7918 - 3s/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "251/251 - 3s - loss: 4622966784.0000 - r2: 0.8022 - val_loss: 4788743168.0000 - val_r2: 0.7956 - 3s/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "251/251 - 3s - loss: 4619960832.0000 - r2: 0.8026 - val_loss: 5056658432.0000 - val_r2: 0.7865 - 3s/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "251/251 - 3s - loss: 4617555968.0000 - r2: 0.8029 - val_loss: 4853598208.0000 - val_r2: 0.7924 - 3s/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "251/251 - 3s - loss: 4618601472.0000 - r2: 0.8030 - val_loss: 5133220352.0000 - val_r2: 0.7822 - 3s/epoch - 12ms/step\n",
      "Epoch 50/50\n",
      "251/251 - 3s - loss: 4620977664.0000 - r2: 0.8029 - val_loss: 4957196800.0000 - val_r2: 0.7901 - 3s/epoch - 12ms/step\n",
      "session cleared!\n",
      "\n",
      "2190.458012342453 seconds elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## RFE starts here \n",
    "while sum(vec) > 0 and best_loss > new_best_loss:\n",
    "    \n",
    "    print('vec', vec)\n",
    "\n",
    "    best_loss = new_best_loss\n",
    "    new_min_loss_flag = False\n",
    "    \n",
    "    losses_from_same_vec = []\n",
    "    \n",
    "    for ix, i in enumerate(vec):\n",
    "        \n",
    "        print('ix', ix, 'i', i)\n",
    "        \n",
    "        if i == 0:\n",
    "            continue # if the feature is off, no need to do anything, go to next position\n",
    "        else:\n",
    "            temp_vec = vec[:]\n",
    "            temp_vec[ix] = 0 # turn off the feature\n",
    "            print('updated temp_vec', temp_vec)\n",
    "            \n",
    "            loss = train_model(temp_vec, best_neuron, best_divisor, best_learning)\n",
    "            losses_from_same_vec.append(loss)\n",
    "            \n",
    "            if loss < new_best_loss:\n",
    "                new_best_loss = loss\n",
    "                which_iter = 'len ' + str(sum(vec)) + ', ix ' + str(ix)\n",
    "                print('new min loss:', which_iter)\n",
    "                new_min_loss_flag = True\n",
    "                min_loss_vec = temp_vec[:]\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            print('session cleared!\\n')\n",
    "                \n",
    "    \n",
    "    all_losses.append(losses_from_same_vec)\n",
    "    \n",
    "    # After going through the vec once, update vec if new min loss    \n",
    "    if new_min_loss_flag:\n",
    "        vec = min_loss_vec\n",
    "    \n",
    "    # else case means no new min loss, the latter while loop condition will cause it to terminate \n",
    "    print(time.time() - start, 'seconds elapsed')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "4IMfS_GSGzzK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3831674112.0, 3492169984.0, 3478057728.0, 4627110400.0, 4898845184.0, 3847911936.0, 3988491776.0, 6490211840.0, 4435342336.0], [4205661184.0, 15844008960.0, 4932816896.0, 4880065536.0, 4193671424.0, 4287794944.0, 6753583104.0, 4788743168.0]]\n"
     ]
    }
   ],
   "source": [
    "print(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qIrWqCzRb0Da"
   },
   "outputs": [],
   "source": [
    "# best feature subset obtained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAgMU7JX5dqA"
   },
   "source": [
    "### Part e\n",
    "\n",
    "RFE on the ‘old test set’ eliminated features degree_centrality and month. It also showed that dist_to_dhoby and dist_to_nearest_stn are crucial (removing\n",
    "them leads to higher test loss). Compare these features to those in Q3d and\n",
    "discuss whether concept drift has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Ythi7VwG5dyo"
   },
   "outputs": [],
   "source": [
    "# compare to those in Q3d\n",
    "\n",
    "# discuss whether concept drift occurred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wu7Tqq5kNG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Possible discussion pointers for conclusion:\n",
    "\n",
    "Besides the discussion pointers mentioned in Part A,\n",
    "\n",
    "*   In Q1, we compared a linear regression model to an equivalent neural network architecture and also saw how adding a hidden layer changes model performance. In Q2, we saw how adding an Embedding layer introduces more learnable parameters to the neural network. What other benefits do neural networks have over other machine learning approaches? In cases where neural networks perform better, is it possible to modify ‘traditional’ machine learning algorithms to close up the gap?\n",
    "*   In Q2, we tried out another approach of model tuning. KerasTuner offers many other algorithms – how do Bayesian optimisation or HyperBand work? Are they necessarily better than random search? Also, is random search better than grid search?\n",
    "*   In Q3, we witnessed what happened to machine learning models if they are not updated with the latest datasets and looked at whether covariate shift, label shift or concept drift has occurred. Which of these have led to model degradation? Was the change in LTV ratio the cause of it (if so, how did it affect the model performance)?\n",
    "*   You do not have to answer all the above discussion pointers. You can choose to deep dive into one of it and write a paragraph or two to summarise your thoughts + reflect on what you have learnt from this part of the assignment.\n",
    "*   Feel free to include your own pointers, as long as they are within the scope of Part B. For instance, we see that the RMSE is still rather high – what else can we do to reduce it? There are many possible extensions to the current model especially because we are limited to using vanilla neural networks for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMf_EVoO6tKM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
